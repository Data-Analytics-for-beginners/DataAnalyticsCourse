


# Сучасна обробка природної мови: токенізація, ембедінги і класифікація тексту
https://medium.com/data-science-collective/modern-nlp-tokenization-embedding-and-text-classification-448826f489bf

## Вступ

Обробка природної мови (NLP) значно еволюціонувала за останні роки. Найкращим доказом цього є великі мовні моделі (LLM). Завдяки сучасним методам стало можливим вести діалог з комп'ютером так само, як із людиною.

Я пам'ятаю старі пакети для NLP, такі як nltk і spacy, і сумую за їхньою простотою. Ці пакети допомагали мені в багатьох проектах, де я міг знаходити закономірності в текстах, виявляти ключові слова і навіть аналізувати контекст за допомогою n-грам (послідовності з n слів, що повторюються в тексті). Не кажучи вже про чудові хмари слів, які були такими популярними певний час.

Але ті часи минули. Старі пакети все ще мають своє місце в аналізі текстів, але коли йдеться про LLM, існують сучасні методи NLP, які ми розглянемо в цій лекції.

## Сучасні концепції NLP

У цій лекції ми розглянемо:

1. Як LLM обробляють текст за допомогою токенізації
2. Що таке ембедінги
3. Як працює механізм уваги (Attention) в трансформерах
4. Як дотренувати попередньо навчену LLM (BERT) для задачі класифікації тексту

## Як LLM обробляють текст за допомогою токенізації

Чим більше я дізнаюся про комп'ютери та мови програмування, тим більше бачу, як багато технологій було розроблено шляхом відображення людської діяльності. Наприклад, процес прийняття рішень, який відбувається в нашій голові, надихнув алгоритм дерева рішень.

Те саме стосується і LLM та того, як вони обробляють мову. Подумаймо, як ми вчимося говорити:
1. Ми чуємо звуки
2. Ми починаємо асоціювати звуки (слова) з об'єктами
3. Ми починаємо асоціювати звуки з діями, і вони починають щось означати
4. Ми повторюємо звуки, і тоді ми говоримо

Кожен новий звук, який ми пов'язуємо зі значенням, включається до нашого словника. Пізніше, коли нам потрібно спілкуватися, ми звертаємося до свого словника і використовуємо наші слова. Після багатьох років тренування ми розвиваємо дуже складну мережу фраз і значень, використовуючи наш словник.

Тепер, переносячи цей процес на комп'ютери і те, як вони обробляють текст, науковці, які розробили LLM, успішно відобразили цей процес:

1. Комп'ютери отримують текст
2. Розділяють його на токени – найменші частини фрази зі значенням. Зазвичай це слово або частина слова
3. Далі комп'ютерам потрібно зрозуміти ці токени. А що вони обробляють найкраще? Числа! Тому кожен токен буде перетворено на вектор чисел
4. Ці вектори чисел будуть використані для наступного етапу – ембедінгів, що дозволяє комп'ютерам розуміти взаємозв'язки між словами

Розглянемо це на практиці з деяким кодом:

```python
# Імпорт
from transformers import BertTokenizer

# Завантаження попередньо навченого токенізатора BERT
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# Приклад тексту для токенізації
text = "Large Language Models are revolutionizing NLP."

# Крок 1: Токенізація тексту
tokens = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
# Перегляд
tokens
```

Результат виглядає так:

```
{'input_ids': tensor([[  101,  2312,  2653,  4275,  2024,  4329,  6026, 17953,  2361,  1012,
           102]]), 
'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 
'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}
```

Зверніть увагу:
- Кожне слово було перетворено на ідентифікатор
- Слово "revolutionizing" ймовірно було розбито на два підслова
- ID 101 пов'язаний з токеном [CLS]. Вектор цього токена вважається таким, що фіксує загальне значення всього речення. Це ніби штамп, що вказує LLM на значення цього фрагменту
- ID 102 пов'язаний з токеном [SEP] для розділення речень

## Що таке ембедінги

Комп'ютер тепер має свій словник. Він знає токени, і вони щось означають самі по собі. Наступним кроком у процесі "навчання говорити" комп'ютера є осмислення всієї цієї бази даних (словника) та створення взаємозв'язків між словами та їхніми значеннями.

Існують два цікаві способи мислення про ембедінги:

1. Уявіть карту. Якщо я просто покажу її вам і скажу, що Нью-Йорк і Лос-Анджелес є на ній, це не дуже допоможе, бо ви не знаєте, де вони розташовані та як далеко один від одного. Але якщо я дам вам координати, ви побачите, що вони знаходяться на протилежних сторонах однієї країни. Так само ембедінг створює такі координати для кожного токена, щоб LLM легше розуміла, наскільки близькі чи далекі їхні значення.

2. Можна думати про вектор ембедінгу як про "профіль значення" слова. Кожне число у векторі показує, наскільки слово "оцінюється" за певним аспектом значення. Уявіть, що у вас є 3-вимірний вектор ембедінгу. Вектор для "собака" може бути [0.8, 0.2, 0.1], де: 0.8 означає високу оцінку в категорії "тварина"; 0.2 означає низьку оцінку в категорії "їжа"; 0.1 означає дуже низьку оцінку в категорії "дія".

Розглянемо код для створення ембедінгів:

```python
# Імпорт необхідних бібліотек
from transformers import AutoTokenizer, AutoModel
import torch

# Завантаження попередньо навченого токенізатора і моделі (BERT у цьому випадку)
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModel.from_pretrained("bert-base-uncased")

# Приклад тексту для токенізації
text = "Large Language Models are revolutionizing NLP."

# Крок 1: Токенізація тексту
tokens = tokenizer(text, return_tensors="pt", padding=True, truncation=True)

# Крок 2: Генерація ембедінгів за допомогою моделі
with torch.no_grad():  # Обчислення градієнта не потрібно для виведення
    output = model(**tokens)

# Крок 3: Вилучення ембедінгів (токен CLS представляє ембедінг речення)
embedding = output.last_hidden_state[:, 0, :]

# Це те, чим став текст. Тензор з числами для розуміння LLM
embedding
```

Результат ембедінгу виглядає так:

```
tensor([[-3.4580e-01, -1.7613e-01,  9.1574e-02, -3.1258e-01, -4.3672e-01,
         -4.6326e-01,  3.5836e-01,  6.3136e-01, -1.6741e-01, -6.0901e-01,
          1.5760e-02,  2.0739e-01, -3.9844e-01,  1.2286e-01,  2.5884e-01,
          2.2635e-01, -2.9151e-01,  5.7592e-01,  2.7204e-01,  7.5218e-02,
         -3.6411e-01, -6.5309e-01,  1.4161e-01, -5.1403e-02,  6.1204e-02,
         -1.0301e-01,  8.3905e-02, -3.2397e-01,  4.2635e-02, -2.5525e-01,
          ...
         -1.9017e-01, -1.5907e-01,  4.7907e-02,  5.5601e-01,  7.5720e-02,
         -4.8495e-01,  1.6084e-02,  3.4603e-01]])
```

Маючи цей тензор, LLM тепер може помістити це речення в контекст з іншим текстом, що полегшує побудову складного текстового виводу.

## Чому ембедінги є вирішальними для завдань NLP

Як раніше зазначалося, комп'ютери працюють з числами, а не словами. Ембедінги заповнюють цей розрив, перетворюючи слова на числові вектори.

- Ембедінги розроблені таким чином, що подібні значення мають подібні вектори – тобто вони розташовані близько один до одного у векторному просторі
- Це дозволяє моделям NLP розуміти семантичні відносини, такі як синоніми, антоніми та пов'язані концепції
- Сучасні ембедінги, особливо ті, що походять з моделей трансформерів (таких як BERT – Bidirectional Encoder Representations from Transformers), йдуть ще далі, вловлюючи контекстуальне значення

Ембедінги перетворюють текст зі слів на змістовні числові представлення для машин, вловлюючи семантичні відносини, зменшуючи розмірність та забезпечуючи краще узагальнення. Вони є основою сучасного NLP, дозволяючи моделям розуміти та обробляти людську мову з безпрецедентною точністю.

## Як працює механізм уваги в трансформерах

У відомій статті "Attention is All You Need" автори визначили увагу (attention) як функцію, яка зіставляє запит (query) та набір пар ключ-значення (key-value) з виходом, де всі вони є векторами. Вихід обчислюється як зважена сума значень, де вага, присвоєна кожному значенню, обчислюється функцією сумісності запиту з відповідним ключем.

Простіше кажучи, коли ви шукаєте щось і запитуєте LLM (query), модель була навчена на купі маркованої інформації (keys and values), і ви хочете отримати відповідь (output).

Механізм уваги визначає, які мітки найбільш релевантні вашому запитанню, і дає відповідь, комбінуючи інформацію з цих релевантних міток.

## Як дотренувати попередньо навчену LLM (BERT) для задачі класифікації тексту

Ми можемо використовувати попередньо навчені моделі BERT для класифікації тексту, наприклад, відгуків про фільми. Розглянемо, як це робиться.

Спочатку встановимо необхідні бібліотеки та завантажимо відкритий набір даних IMDB:

```python
pip install datasets --quiet

# Імпорт необхідних бібліотек
import torch
from torch.utils.data import DataLoader
from transformers import BertTokenizer, BertForSequenceClassification
from torch.optim import AdamW 
from datasets import load_dataset
import numpy as np
from transformers import DataCollatorWithPadding, TrainingArguments, Trainer

# Перевірка доступності GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
```

Якщо ваш текст англійською, токенізатор bert-base-uncased є звичайною відправною точкою. Для інших мов вам знадобиться багатомовний BERT (bert-base-multilingual-cased) або BERT, специфічний для мови.

Якщо ваш текст із спеціалізованої галузі (наприклад, медичної, юридичної, наукової), розгляньте можливість використання моделі BERT, яка була попередньо навчена в цій галузі.

```python
# Завантаження попередньо навченого токенізатора BERT
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# Завантаження набору даних для класифікації настроїв (відгуки IMDB)
dataset = load_dataset("imdb")
```

Далі ми повинні токенізувати наші дані. Створимо просту функцію для цього, розбиваючи текст на фрагменти максимум 128 символів. Коли текст довший, фрагмент обрізається. Коли коротший, додаються заповнювачі (padding).

```python
# Функція токенізації
def tokenize_function(examples):
    tokenized = tokenizer(examples["text"], 
                          padding="max_length",
                          truncation=True, 
                          max_length=128)

    return {key: torch.tensor(val) for key, val in tokenized.items()} #Конвертація в тензори тут.

# Токенізація набору даних
tokenized_datasets = dataset.map(tokenize_function, batched=True)
```

Для швидкості ми візьмемо лише кілька спостережень з наших даних:

```python
# Конвертація в формат PyTorch набору даних
train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(20))  # Використання підмножини для швидкого навчання
test_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(10))
```

Ось приклад тексту з набору даних:

```
train_dataset['text'][1]
```

Результат:
```
This movie is a great. The plot is very true to the book which is a classic 
written by Mark Twain. The movie starts of with a scene where Hank sings a 
song with a bunch of kids called "when you stub your toe on the moon" 
It reminds me of Sinatra's song High Hopes, it is fun and inspirational. 
The Music is great throughout and my favorite song is sung by the King, Hank
 (bing Crosby) and Sir "Saggy" Sagamore. OVerall a great family movie or even 
a great Date movie. This is a movie you can watch over and over again.
 The princess played by Rhonda Fleming is gorgeous. I love this movie!!
 If you liked Danny Kaye in the Court Jester then you will definitely 
like this movie.
```

Наступний код "заморожує" більшість попередньо навченої моделі, щоб зберегти її загальні мовні знання та зберегти обчислювальні ресурси. Потім він "розморожує" конкретну частину (шар "pooler"), щоб дозволити моделі адаптуватися до вашого конкретного завдання.

```python
# Завантаження попередньо навченої моделі BERT для класифікації (2 класи: Позитивний/Негативний)
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2).to(device)

# заморозка всіх параметрів базової моделі
for name, param in model.base_model.named_parameters():
    param.requires_grad = False

# розморозка шарів об'єднання базової моделі
for name, param in model.base_model.named_parameters():
    if "pooler" in name:
        param.requires_grad = True
```

Встановимо гіперпараметри:

```python
# гіперпараметри
lr = 2e-4
batch_size = 8
num_epochs = 10

training_args = TrainingArguments(
    output_dir="bert-classifier_movie",
    learning_rate=lr,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=num_epochs,
    logging_strategy="epoch",
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
)

# Екземпляр Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer
)

# Навчання моделі
trainer.train()
```

І нарешті, отримуємо результати:

```python
# застосування моделі до набору даних валідації
predictions = trainer.predict(test_dataset)

# Перевірка, чи прогноз збігається з тестовою міткою
predictions.label_ids == test_dataset['label']

# --- Результати ---
# array([ True,  True,  True,  True,  True,  True,  True,  True,  True, True])
```

## Підсумок

У цій лекції ми розглянули багато важливих аспектів сучасної обробки природної мови.

Сучасний NLP перебуває на піку популярності. З цією технологією робиться багато чого, і новини про неї постійно з'являються.

У цій лекції я хотів дати вам зрозуміле пояснення того, як LLM приймають вхідні дані, такі як текстові запити, і повертають такі дивовижні результати, допомагаючи нам у багатьох щоденних завданнях.

Ембедінги перетворюють текст зі слів на змістовні числові представлення для машин, токенізація розбиває текст на значущі одиниці, а механізм уваги дозволяє моделі зосередитися на найважливіших частинах входу. Все це дозволяє нам дотренувати великі моделі для вирішення конкретних завдань із високою точністю.



Посилання:
1) How Text Embeddings help suggest similar words, https://pathway.com/blog/how-text-embeddings-help-suggest-similar-words?source=post_page-----448826f489bf---------------------------------------
2) A Visual Guide to Using BERT for the First Time, https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/?source=post_page-----448826f489bf---------------------------------------
3) Tokenization in large language models, explained, https://medium.com/data-science-collective/modern-nlp-tokenization-embedding-and-text-classification-448826f489bf
4) Fine-Tuning BERT for Text Classification, https://medium.com/data-science-collective/modern-nlp-tokenization-embedding-and-text-classification-448826f489bf
5) 



-----------------------------------------------------------------------------------

- Named Entity Recognition with Python and Hugging Face Transformers, https://ai.plainenglish.io/named-entity-recognition-with-python-and-hugging-face-transformers-d50e88e06633
- 
