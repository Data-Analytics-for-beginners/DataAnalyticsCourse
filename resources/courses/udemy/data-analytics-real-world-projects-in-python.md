

# Data Analytics Real-World Projects in Python

https://ua.udemy.com/course/data-analytics-projects-python/

# Розділ лекції: Основи аналітики даних

## 1.1 Що таке аналітика даних?

Аналітика даних - це процес застосування аналітичних технік до даних для отримання змістовних висновків. Ці висновки також називають **інсайтами** або **висновками**, які підтримують краще прийняття рішень та допомагають організаціям і компаніям.

### Три ключові компоненти аналітики даних:

1. **Дані** - вихідна інформація
2. **Аналітичні техніки** - методи обробки 
3. **Висновки** - інсайти для прийняття рішень

## 1.2 Що таке дані?

**Дані** - це сира інформація, яку можна збирати з різних джерел залежно від проекту, над яким ви працюєте.

### Приклад збору даних в Amazon:

Якщо ви аналітик даних в Amazon і маєте завдання зібрати транзакційні дані користувачів, ви можете збирати такі атрибути:

- Що купив кожен клієнт
- Скільки витратив покупець
- Які пристрої використовують користувачі Amazon
- Локація покупців

### Джерела даних:
- Корпоративні бази даних
- Excel файли
- CSV файли
- Фреймворки для великих даних

## 1.3 Аналітичні техніки

Після отримання даних необхідно застосувати аналітичні техніки для отримання змістовних висновків. До таких технік належать:

- Різні статистичні методи
- Побудова графіків та діаграм
- Техніки дослідницького аналізу даних
- Очищення даних

## 1.4 Висновки та інсайти

### Приклад висновків з аналізу даних Amazon:

1. **iPhone є найпопулярнішим товаром** на Amazon
2. **Знижки останньої хвилини** допомагають збільшити продажі
3. **Рекомендовані товари**, які часто купують разом

## 1.5 Повсякденний приклад: приготування піци

Для кращого розуміння концепції розглянемо аналогію з приготуванням піци:

### Інгредієнти (сирі дані):
- Борошно
- Сир
- Овочі
- Соуси та топінги

### Процес приготування (аналітичні техніки):
- Приготування тіста
- Випікання піци

### Висновки (інсайти):
Після куштування піци ви можете зробити висновок: "наступного разу додам більше сиру"

## 1.6 Реальний приклад: авіакомпанія

### Сценарій: 
Ви - аналітик даних в авіакомпанії

### Дані:
- Міста відправлення та призначення
- Ціни квитків
- Дати подорожей
- Приблизно 10 мільйонів спостережень

### Завдання для аналізу:
1. **Сезонний попит на рейси** - коли люди подорожують найчастіше?
2. **Оцінка задоволеності пасажирів** - наскільки клієнти задоволені сервісом?
3. **Найекономічніші авіалінії** - які компанії пропонують найкращі ціни?

### Результат:
Перетворення сирих даних про польоти в розумні бізнес-рішення, які допомагають покращити роботу авіакомпанії.

## Висновок

Аналітика даних - це процес перетворення сирих даних у практичні інсайти, що допомагають організаціям приймати кращі рішення та покращувати свою діяльність. Основна суть полягає в тому, щоб використовувати дані для підтримки розумного прийняття рішень у бізнесі.

---------------------------------------------------------------------------

# Розділ лекції: Відмінності між аналітикою даних та наукою про дані

## 1.7 Основні відмінності між Data Analytics та Data Science

### Ключова різниця:
**Наука про дані = Аналітика даних + Прогнозне моделювання**

### Аналітика даних:
1. **Збір даних** - отримання сирої інформації
2. **Очищення даних** - підготовка до аналізу
3. **Аналіз даних** - дослідження закономірностей
4. **Отримання інсайтів** - висновки для прийняття рішень

### Наука про дані:
Все, що є в аналітиці даних **ПЛЮС**:
- **Прогнозне моделювання** - використання алгоритмів для передбачення майбутніх подій
- **Машинне навчання**
- **Глибоке навчання**
- **Обробка природної мови (NLP)**
- **Алгоритми часових рядів**

## 1.8 Повсякденний приклад: туристична компанія

### Сценарій: 
Аналітик в компанії MakeMyTrip або Goibibo збирає дані про ваші подорожі

### Аналітика даних може відповісти на питання:
- **Куди ви їздили** в останній поїздці?
- **Скільки витратили** на останніх 10 подорожах?
- **Що пройшло добре** під час подорожей?
- **Які напрямки були найприємнішими?**

### Наука про дані може передбачити:
- **Який буде найкращий напрямок** для вашого наступного відпустки?
- **Коли найкраще їхати** в цей напрямок?
- **Скільки коштуватиме** ця подорож?

### Різниця в підході:
- **Аналітика даних**: розуміння минулого (що і чому сталося)
- **Наука про дані**: передбачення майбутнього на основі аналізу минулих даних

## 1.9 Реальний приклад: YouTube

### Дані для аналізу:
- Назва відео
- ID відео
- Співвідношення лайків
- Рівень залученості
- Довжина заголовка
- Чи стало відео вірусним

*Уявімо, що у нас є 1 мільйон таких записів*

### Аналітика даних дозволяє визначити:
- **Яке відео має найвище співвідношення лайків?**
- **У якого відео найвищий рівень залученості?**

### Наука про дані може передбачити:
- **Чи стане нове відео, яке я завантажу, вірусним?**
- Використовуючи минулі дані та алгоритми машинного навчання для прогнозування

## 1.10 Підсумок відмінностей

| Аспект | Аналітика даних | Наука про дані |
|--------|----------------|----------------|
| **Фокус** | Аналіз минулих даних | Аналіз + прогнозування |
| **Питання** | Що і чому сталося? | Що станеться в майбутньому? |
| **Методи** | Статистичний аналіз, візуалізація | ML, AI алгоритми, прогнозні моделі |
| **Результат** | Інсайти та висновки | Передбачення та рекомендації |
| **Часова орієнтація** | Минуле та теперішнє | Майбутнє |

## Висновок

Аналітика даних зосереджена на розумінні того, що сталося в минулому та чому це сталося. Наука про дані використовує цю інформацію для створення моделей, які можуть передбачати майбутні події та тенденції, застосовуючи складні алгоритми машинного навчання та штучного інтелекту.

-------------------------------------------------------------------------------------------------------




flowchart TD
    A[Understanding Use-case] --> B[Run ETL Pipeline]
    B --> C[EDA]
    B --> D[Extract Data]
    B --> E[Transform Data into Acceptable Format]
    B --> F[Load Data]
    C --> G[Conclusion]
    C --> H[Data Viz.]
    G --> I[Dashboarding]
    H --> I
    
    %% ETL Process details
    E -.-> J[Remove Duplicate Rows<br/>Remove In-relevant Rows/Cols<br/>Fix Errors<br/>Fix Missing Values & Data Types<br/>Deal with Outliers]
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style G fill:#fff3e0
    style I fill:#fce4ec



--------------------------------------------------------------------------------------------------------------------------------------------------------

# Розділ лекції: Життєвий цикл проекту з аналітики даних

## 1.11 Життєвий цикл проекту з аналітики даних

Для вирішення будь-якого реального проекту з аналітики даних необхідно дотримуватися певного життєвого циклу:

### Основні етапи:
1. **Розуміння use case** (бізнес-розуміння)
2. **Запуск ETL pipeline**
3. **Виконання EDA** (дослідницького аналізу даних)
4. **Формулювання висновків** з EDA
5. **Створення дашбордів** для проекту

## 1.12 Приклад застосування

### Сценарій: Аналітик даних в Amazon

**Дані про продажі:**
- Продукт, купований користувачем
- Ім'я користувача
- Ціна, сплачена користувачем
- Країна покупки

**Приклад датасету:**
```
Продукт    | Користувач | Ціна (₹) | Країна
iPhone     | John       | 80,000   | US
MacBook    | Rahul      | 120,000  | India
Phone      | Saurabh    | 25,000   | India
```

*Уявімо, що у нас є 1 мільйон таких записів*

### Можливі інсайти після аналізу:
- **iPhone та MacBook** є найпопулярнішими товарами на Amazon
- **Люди з Азії та Європи** зазвичай купують преміум продукти
- **Підлітки** рідше купують дорогі товари

## 1.13 Етап 1: Розуміння use case (бізнес-розуміння)

### Ключові учасники проекту:

#### Клієнт
- **Хто це**: власник бізнесу з проблемою
- **Приклад**: власник роздрібного магазину
- **Проблема**: різке падіння продажів минулого місяця
- **Потреба**: вирішити проблему зниження продажів

#### Product Owner (Власник продукту)
- **Роль**: розуміє потреби клієнта
- **Функції**:
  - Перетворює потреби клієнта в цілі та пріоритети
  - Визначає завдання для аналітичної команди
  - Приклад рішень: "Спочатку проаналізуємо дані продажів, потім відгуки клієнтів"
- **Відповідальність**: надання правильного продукту або рішення клієнту

#### Business Analyst (Бізнес-аналітик)
- **Роль**: міст між бізнесом та технічними командами
- **Функції**:
  - Спілкується з клієнтом та власником продукту
  - Перетворює бізнес-потреби в технічні завдання
  - Готує завдання для data команди (аналітики, інженери, data scientist)

## 1.14 Процес збору вимог

### Етапи взаємодії:
1. **Клієнт** → розмова з **Product Owner** + **Business Analyst**
2. **Збір вимог** для проекту
3. **Передача завдань** команді аналітики даних

### Ключові питання аналітика даних:
- **Звідки збирати дані?**
  - Сторонні API?
  - Внутрішні системи компанії?
  - Бази даних компанії?
  - Веб-скрапінг з сайтів?
  - Big Data джерела?
  - Інші джерела?

## 1.15 Основна мета першого етапу

**Головне завдання**: визначити джерела даних для проекту

**Принцип**: "Без даних немає ні data science, ні data analytics"

**Результат**: чітке розуміння:
- Що потрібно проаналізувати
- Звідки взяти дані
- Які технічні завдання необхідно виконати

## Висновок

Перший етап життєвого циклу проекту з аналітики даних - це фундамент успішного проекту. Правильне розуміння бізнес-потреб та ідентифікація джерел даних критично важливі для подальшого успіху всього проекту.

-----------------------------------------------------------------------------------------------------------------------------------------------------

# Розділ лекції: ETL Pipeline в аналітиці даних

## 1.16 Що таке ETL Pipeline?

**ETL** - це другий етап життєвого циклу проекту з аналітики даних, який виконується командою аналітиків та big data інженерів.

### Розшифровка ETL:
- **E** - **Extract** (Витягування) - отримання сирих даних
- **T** - **Transform** (Трансформація) - очищення та підготовка даних
- **L** - **Load** (Завантаження) - збереження підготовлених даних

## 1.17 Етап 1: Extract (Витягування даних)

### Джерела даних:

#### Структуровані формати:
- **CSV** (Comma Separated Values) - дані, розділені комою
- **TSV** (Tab Separated Values) - дані, розділені табуляцією
- **JSON** - колекція пар "ключ-значення"

#### Бази даних:
**SQL бази даних** (табличні):
- MySQL, PostgreSQL, SQL Server
- Дані зберігаються у формі таблиць (рядки та стовпці)
- Доступ через SQL запити

**NoSQL бази даних** (документо-орієнтовані):
- MongoDB, Cassandra
- Дані зберігаються у формі документів

#### Big Data платформи:
- Hive, Hadoop, Spark
- Використовують HiveQL (подібна до SQL)

### Приклади форматів даних:

**CSV формат:**
```
Name,Age,City
John,25,New York
Mary,30,London
```

**TSV формат:**
```
Name    Age    City
John    25     New York
Mary    30     London
```

**JSON формат:**
```json
[
  {"Name": "John", "Age": 25, "City": "New York"},
  {"Name": "Mary", "Age": 30, "City": "London"}
]
```

## 1.18 Розподіл джерел даних

### Внутрішні джерела (60-80% часу):
- Корпоративні бази даних
- Внутрішні системи компанії
- Data warehouses та data lakes

### Зовнішні джерела (20-40% часу):
- API третіх сторін (комерційні, платні)
- Web scraping з веб-сайтів
- Відкриті дані

## 1.19 Етап 2: Transform (Трансформація даних)

**Найважливіший етап**: займає **2/3 часу всього проекту**

### Типові проблеми сирих даних:
- Помилки та опечатки
- Пропущені значення
- Дублікати рядків та стовпців
- Нерелевантні дані
- Неправильні типи даних
- Викиди (outliers)

### Синоніми трансформації:
- Data Cleaning (очищення даних)
- Data Pre-processing (попередня обробка)
- Data Wrangling (обробка даних)
- Data Preparation (підготовка даних)

**Результат**: чисті, підготовлені, featurized дані

## 1.20 Етап 3: Load (Завантаження даних)

Вибір способу зберігання залежить від обсягу даних:

### Великі дані (петабайти, терабайти):
- **Big Data платформи**: Hive, Hadoop, Spark clusters
- **Відповідальний**: Big Data Engineer

### Середні дані (10-100 ГБ):
- **Бази даних**: SQL або NoSQL
- Залежить від специфіки проекту

### Малі дані:
- **Файли**: CSV, TSV, JSON, Excel
- Простіше зберігання та доступ

## 1.21 Схема ETL процесу

```
Сирі дані → Data Cleaning → Чисті дані → Зберігання → Подальший аналіз
```

### Деталізація трансформації:
1. Виправлення помилок
2. Обробка пропущених значень
3. Видалення дублікатів
4. Фільтрація нерелевантних даних
5. Приведення до правильних типів даних
6. Обробка викидів

## 1.22 Роль Big Data Engineer

**Відповідальність**: ефективне зберігання підготовлених даних

**Вибір технології залежить від**:
- Обсягу даних
- Швидкості доступу
- Типу аналізу
- Бюджету проекту

## Висновок

ETL Pipeline - це критично важливий етап, який забезпечує якість даних для подальшого аналізу. Без якісного ETL неможливо отримати достовірні інсайти та зробити правильні бізнес-рішення. Найбільше уваги потребує етап трансформації, оскільки якість очищення даних безпосередньо впливає на результати всього проекту.

-----------------------------------------------------------------------------------------------------------------------
