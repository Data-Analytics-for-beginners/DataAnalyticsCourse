

# Data Analytics Real-World Projects in Python

https://ua.udemy.com/course/data-analytics-projects-python/

# Розділ лекції: Основи аналітики даних

## 1.1 Що таке аналітика даних?

Аналітика даних - це процес застосування аналітичних технік до даних для отримання змістовних висновків. Ці висновки також називають **інсайтами** або **висновками**, які підтримують краще прийняття рішень та допомагають організаціям і компаніям.

### Три ключові компоненти аналітики даних:

1. **Дані** - вихідна інформація
2. **Аналітичні техніки** - методи обробки 
3. **Висновки** - інсайти для прийняття рішень

## 1.2 Що таке дані?

**Дані** - це сира інформація, яку можна збирати з різних джерел залежно від проекту, над яким ви працюєте.

### Приклад збору даних в Amazon:

Якщо ви аналітик даних в Amazon і маєте завдання зібрати транзакційні дані користувачів, ви можете збирати такі атрибути:

- Що купив кожен клієнт
- Скільки витратив покупець
- Які пристрої використовують користувачі Amazon
- Локація покупців

### Джерела даних:
- Корпоративні бази даних
- Excel файли
- CSV файли
- Фреймворки для великих даних

## 1.3 Аналітичні техніки

Після отримання даних необхідно застосувати аналітичні техніки для отримання змістовних висновків. До таких технік належать:

- Різні статистичні методи
- Побудова графіків та діаграм
- Техніки дослідницького аналізу даних
- Очищення даних

## 1.4 Висновки та інсайти

### Приклад висновків з аналізу даних Amazon:

1. **iPhone є найпопулярнішим товаром** на Amazon
2. **Знижки останньої хвилини** допомагають збільшити продажі
3. **Рекомендовані товари**, які часто купують разом

## 1.5 Повсякденний приклад: приготування піци

Для кращого розуміння концепції розглянемо аналогію з приготуванням піци:

### Інгредієнти (сирі дані):
- Борошно
- Сир
- Овочі
- Соуси та топінги

### Процес приготування (аналітичні техніки):
- Приготування тіста
- Випікання піци

### Висновки (інсайти):
Після куштування піци ви можете зробити висновок: "наступного разу додам більше сиру"

## 1.6 Реальний приклад: авіакомпанія

### Сценарій: 
Ви - аналітик даних в авіакомпанії

### Дані:
- Міста відправлення та призначення
- Ціни квитків
- Дати подорожей
- Приблизно 10 мільйонів спостережень

### Завдання для аналізу:
1. **Сезонний попит на рейси** - коли люди подорожують найчастіше?
2. **Оцінка задоволеності пасажирів** - наскільки клієнти задоволені сервісом?
3. **Найекономічніші авіалінії** - які компанії пропонують найкращі ціни?

### Результат:
Перетворення сирих даних про польоти в розумні бізнес-рішення, які допомагають покращити роботу авіакомпанії.

## Висновок

Аналітика даних - це процес перетворення сирих даних у практичні інсайти, що допомагають організаціям приймати кращі рішення та покращувати свою діяльність. Основна суть полягає в тому, щоб використовувати дані для підтримки розумного прийняття рішень у бізнесі.

---------------------------------------------------------------------------

# Розділ лекції: Відмінності між аналітикою даних та наукою про дані

## 1.7 Основні відмінності між Data Analytics та Data Science

### Ключова різниця:
**Наука про дані = Аналітика даних + Прогнозне моделювання**

### Аналітика даних:
1. **Збір даних** - отримання сирої інформації
2. **Очищення даних** - підготовка до аналізу
3. **Аналіз даних** - дослідження закономірностей
4. **Отримання інсайтів** - висновки для прийняття рішень

### Наука про дані:
Все, що є в аналітиці даних **ПЛЮС**:
- **Прогнозне моделювання** - використання алгоритмів для передбачення майбутніх подій
- **Машинне навчання**
- **Глибоке навчання**
- **Обробка природної мови (NLP)**
- **Алгоритми часових рядів**

## 1.8 Повсякденний приклад: туристична компанія

### Сценарій: 
Аналітик в компанії MakeMyTrip або Goibibo збирає дані про ваші подорожі

### Аналітика даних може відповісти на питання:
- **Куди ви їздили** в останній поїздці?
- **Скільки витратили** на останніх 10 подорожах?
- **Що пройшло добре** під час подорожей?
- **Які напрямки були найприємнішими?**

### Наука про дані може передбачити:
- **Який буде найкращий напрямок** для вашого наступного відпустки?
- **Коли найкраще їхати** в цей напрямок?
- **Скільки коштуватиме** ця подорож?

### Різниця в підході:
- **Аналітика даних**: розуміння минулого (що і чому сталося)
- **Наука про дані**: передбачення майбутнього на основі аналізу минулих даних

## 1.9 Реальний приклад: YouTube

### Дані для аналізу:
- Назва відео
- ID відео
- Співвідношення лайків
- Рівень залученості
- Довжина заголовка
- Чи стало відео вірусним

*Уявімо, що у нас є 1 мільйон таких записів*

### Аналітика даних дозволяє визначити:
- **Яке відео має найвище співвідношення лайків?**
- **У якого відео найвищий рівень залученості?**

### Наука про дані може передбачити:
- **Чи стане нове відео, яке я завантажу, вірусним?**
- Використовуючи минулі дані та алгоритми машинного навчання для прогнозування

## 1.10 Підсумок відмінностей

| Аспект | Аналітика даних | Наука про дані |
|--------|----------------|----------------|
| **Фокус** | Аналіз минулих даних | Аналіз + прогнозування |
| **Питання** | Що і чому сталося? | Що станеться в майбутньому? |
| **Методи** | Статистичний аналіз, візуалізація | ML, AI алгоритми, прогнозні моделі |
| **Результат** | Інсайти та висновки | Передбачення та рекомендації |
| **Часова орієнтація** | Минуле та теперішнє | Майбутнє |

## Висновок

Аналітика даних зосереджена на розумінні того, що сталося в минулому та чому це сталося. Наука про дані використовує цю інформацію для створення моделей, які можуть передбачати майбутні події та тенденції, застосовуючи складні алгоритми машинного навчання та штучного інтелекту.

-------------------------------------------------------------------------------------------------------




flowchart TD
    A[Understanding Use-case] --> B[Run ETL Pipeline]
    B --> C[EDA]
    B --> D[Extract Data]
    B --> E[Transform Data into Acceptable Format]
    B --> F[Load Data]
    C --> G[Conclusion]
    C --> H[Data Viz.]
    G --> I[Dashboarding]
    H --> I
    
    %% ETL Process details
    E -.-> J[Remove Duplicate Rows<br/>Remove In-relevant Rows/Cols<br/>Fix Errors<br/>Fix Missing Values & Data Types<br/>Deal with Outliers]
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style G fill:#fff3e0
    style I fill:#fce4ec



--------------------------------------------------------------------------------------------------------------------------------------------------------

# Розділ лекції: Життєвий цикл проекту з аналітики даних

## 1.11 Життєвий цикл проекту з аналітики даних

Для вирішення будь-якого реального проекту з аналітики даних необхідно дотримуватися певного життєвого циклу:

### Основні етапи:
1. **Розуміння use case** (бізнес-розуміння)
2. **Запуск ETL pipeline**
3. **Виконання EDA** (дослідницького аналізу даних)
4. **Формулювання висновків** з EDA
5. **Створення дашбордів** для проекту

## 1.12 Приклад застосування

### Сценарій: Аналітик даних в Amazon

**Дані про продажі:**
- Продукт, купований користувачем
- Ім'я користувача
- Ціна, сплачена користувачем
- Країна покупки

**Приклад датасету:**
```
Продукт    | Користувач | Ціна (₹) | Країна
iPhone     | John       | 80,000   | US
MacBook    | Rahul      | 120,000  | India
Phone      | Saurabh    | 25,000   | India
```

*Уявімо, що у нас є 1 мільйон таких записів*

### Можливі інсайти після аналізу:
- **iPhone та MacBook** є найпопулярнішими товарами на Amazon
- **Люди з Азії та Європи** зазвичай купують преміум продукти
- **Підлітки** рідше купують дорогі товари

## 1.13 Етап 1: Розуміння use case (бізнес-розуміння)

### Ключові учасники проекту:

#### Клієнт
- **Хто це**: власник бізнесу з проблемою
- **Приклад**: власник роздрібного магазину
- **Проблема**: різке падіння продажів минулого місяця
- **Потреба**: вирішити проблему зниження продажів

#### Product Owner (Власник продукту)
- **Роль**: розуміє потреби клієнта
- **Функції**:
  - Перетворює потреби клієнта в цілі та пріоритети
  - Визначає завдання для аналітичної команди
  - Приклад рішень: "Спочатку проаналізуємо дані продажів, потім відгуки клієнтів"
- **Відповідальність**: надання правильного продукту або рішення клієнту

#### Business Analyst (Бізнес-аналітик)
- **Роль**: міст між бізнесом та технічними командами
- **Функції**:
  - Спілкується з клієнтом та власником продукту
  - Перетворює бізнес-потреби в технічні завдання
  - Готує завдання для data команди (аналітики, інженери, data scientist)

## 1.14 Процес збору вимог

### Етапи взаємодії:
1. **Клієнт** → розмова з **Product Owner** + **Business Analyst**
2. **Збір вимог** для проекту
3. **Передача завдань** команді аналітики даних

### Ключові питання аналітика даних:
- **Звідки збирати дані?**
  - Сторонні API?
  - Внутрішні системи компанії?
  - Бази даних компанії?
  - Веб-скрапінг з сайтів?
  - Big Data джерела?
  - Інші джерела?

## 1.15 Основна мета першого етапу

**Головне завдання**: визначити джерела даних для проекту

**Принцип**: "Без даних немає ні data science, ні data analytics"

**Результат**: чітке розуміння:
- Що потрібно проаналізувати
- Звідки взяти дані
- Які технічні завдання необхідно виконати

## Висновок

Перший етап життєвого циклу проекту з аналітики даних - це фундамент успішного проекту. Правильне розуміння бізнес-потреб та ідентифікація джерел даних критично важливі для подальшого успіху всього проекту.

-----------------------------------------------------------------------------------------------------------------------------------------------------

# Розділ лекції: ETL Pipeline в аналітиці даних

## 1.16 Що таке ETL Pipeline?

**ETL** - це другий етап життєвого циклу проекту з аналітики даних, який виконується командою аналітиків та big data інженерів.

### Розшифровка ETL:
- **E** - **Extract** (Витягування) - отримання сирих даних
- **T** - **Transform** (Трансформація) - очищення та підготовка даних
- **L** - **Load** (Завантаження) - збереження підготовлених даних

## 1.17 Етап 1: Extract (Витягування даних)

### Джерела даних:

#### Структуровані формати:
- **CSV** (Comma Separated Values) - дані, розділені комою
- **TSV** (Tab Separated Values) - дані, розділені табуляцією
- **JSON** - колекція пар "ключ-значення"

#### Бази даних:
**SQL бази даних** (табличні):
- MySQL, PostgreSQL, SQL Server
- Дані зберігаються у формі таблиць (рядки та стовпці)
- Доступ через SQL запити

**NoSQL бази даних** (документо-орієнтовані):
- MongoDB, Cassandra
- Дані зберігаються у формі документів

#### Big Data платформи:
- Hive, Hadoop, Spark
- Використовують HiveQL (подібна до SQL)

### Приклади форматів даних:

**CSV формат:**
```
Name,Age,City
John,25,New York
Mary,30,London
```

**TSV формат:**
```
Name    Age    City
John    25     New York
Mary    30     London
```

**JSON формат:**
```json
[
  {"Name": "John", "Age": 25, "City": "New York"},
  {"Name": "Mary", "Age": 30, "City": "London"}
]
```

## 1.18 Розподіл джерел даних

### Внутрішні джерела (60-80% часу):
- Корпоративні бази даних
- Внутрішні системи компанії
- Data warehouses та data lakes

### Зовнішні джерела (20-40% часу):
- API третіх сторін (комерційні, платні)
- Web scraping з веб-сайтів
- Відкриті дані

## 1.19 Етап 2: Transform (Трансформація даних)

**Найважливіший етап**: займає **2/3 часу всього проекту**

### Типові проблеми сирих даних:
- Помилки та опечатки
- Пропущені значення
- Дублікати рядків та стовпців
- Нерелевантні дані
- Неправильні типи даних
- Викиди (outliers)

### Синоніми трансформації:
- Data Cleaning (очищення даних)
- Data Pre-processing (попередня обробка)
- Data Wrangling (обробка даних)
- Data Preparation (підготовка даних)

**Результат**: чисті, підготовлені, featurized дані

## 1.20 Етап 3: Load (Завантаження даних)

Вибір способу зберігання залежить від обсягу даних:

### Великі дані (петабайти, терабайти):
- **Big Data платформи**: Hive, Hadoop, Spark clusters
- **Відповідальний**: Big Data Engineer

### Середні дані (10-100 ГБ):
- **Бази даних**: SQL або NoSQL
- Залежить від специфіки проекту

### Малі дані:
- **Файли**: CSV, TSV, JSON, Excel
- Простіше зберігання та доступ

## 1.21 Схема ETL процесу

```
Сирі дані → Data Cleaning → Чисті дані → Зберігання → Подальший аналіз
```

### Деталізація трансформації:
1. Виправлення помилок
2. Обробка пропущених значень
3. Видалення дублікатів
4. Фільтрація нерелевантних даних
5. Приведення до правильних типів даних
6. Обробка викидів

## 1.22 Роль Big Data Engineer

**Відповідальність**: ефективне зберігання підготовлених даних

**Вибір технології залежить від**:
- Обсягу даних
- Швидкості доступу
- Типу аналізу
- Бюджету проекту

## Висновок

ETL Pipeline - це критично важливий етап, який забезпечує якість даних для подальшого аналізу. Без якісного ETL неможливо отримати достовірні інсайти та зробити правильні бізнес-рішення. Найбільше уваги потребує етап трансформації, оскільки якість очищення даних безпосередньо впливає на результати всього проекту.

-----------------------------------------------------------------------------------------------------------------------

# Розділ лекції: Візуалізація даних та створення дашбордів

## 1.23 Від EDA до візуалізації

Після виконання дослідницького аналізу даних (EDA) ми отримуємо висновки та інсайти з даних. Однак великі обсяги текстової інформації важко сприймати та аналізувати.

### Проблема текстових даних:

**Приклад результатів EDA** (найпопулярніші продукти):
```
iPhone: 50,000 проданих одиниць
OnePlus: 35,000 проданих одиниць
MacBook: 60,000 проданих одиниць
Samsung: 42,000 проданих одиниць
```

Коли у нас мільйон записів, така текстова інформація стає незрозумілою для швидкого аналізу.

## 1.24 Рішення: Візуалізація даних

### Що таке візуалізація даних?
**Візуалізація даних** - це процес створення графіків та діаграм для наочного представлення результатів аналізу.

### Приклад: стовпчикова діаграма

```
Продажі продуктів
│
│     ┌───┐
│ ┌───┤   │
│ │   │   │   ┌───┐
│ │   │   │   │   │   ┌───┐
└─┴───┴───┴───┴───┴───┴───┴─
iPhone OnePlus MacBook Samsung
```

**Результат**: одним поглядом можна побачити, що MacBook є лідером продажів.

## 1.25 Інструменти для візуалізації в Python

### Основні бібліотеки:

#### 1. **Pandas**
- Вбудовані можливості візуалізації
- Швидкі і прості графіки

#### 2. **Matplotlib** 
- Базовий модуль для візуалізації
- Фундамент для інших бібліотек
- Статичні графіки

#### 3. **Seaborn**
- Красивіші статистичні графіки
- Побудований на Matplotlib

#### 4. **Plotly**
- Інтерактивні та динамічні графіки
- Можливість взаємодії з користувачем

## 1.26 Важливість інференцій

### Ключовий принцип:
**Якщо ви не можете зробити висновки з ваших графіків, то ваш аналіз марний.**

### Необхідність комунікації:
- **Ділитеся результатами** з колегами та командою
- **Пояснюйте технічні речі** простою мовою
- **Робіть висновки зрозумілими** для нетехнічної аудиторії

### Критерії успішної візуалізації:
1. Графік легко читається
2. Висновки очевидні з першого погляду
3. Дані підтверджують або спростовують гіпотези
4. Результати можна пояснити бізнесу

## 1.27 Дашборди

### Що таке дашборд?
**Дашборд** - це інтерактивна панель, що об'єднує всі аналітичні результати на одному екрані.

### Приклад: дашборд фондового ринку

**Функціональність:**
- Вибір компанії (Amazon, Microsoft, Apple, Google)
- Автоматичне оновлення всіх графіків
- Різні види аналізу в одному місці:
  - Динаміка цін
  - Обсяги торгівлі  
  - Технічні індикатори
  - Порівняльний аналіз

### Переваги дашбордів:

#### Для аналітиків:
- Централізація всіх результатів
- Швидкий доступ до різних метрик
- Можливість інтерактивного дослідження

#### Для бізнесу:
- Миттєвий доступ до ключових показників
- Не потрібні технічні знання
- Можливість самостійного аналізу

#### Для команди:
- Спільне використання результатів
- Однакове розуміння даних
- Ефективне прийняття рішень

## 1.28 Процес створення дашбордів

### Етапи розробки:
1. **Визначення ключових метрик** - що важливо показати?
2. **Вибір типів візуалізації** - які графіки найкраще відображають дані?
3. **Створення інтерактивних елементів** - фільтри, селектори, кнопки
4. **Тестування зручності** - чи зрозуміло користувачеві?
5. **Розгортання та підтримка** - забезпечення актуальності даних

### Технічна реалізація:
- **Python**: Dash, Streamlit, Flask
- **BI-інструменти**: Power BI, Tableau, Looker
- **Веб-технології**: D3.js, React + Charts

## 1.29 Поради щодо ефективної візуалізації

### Принципи хорошої візуалізації:
1. **Простота** - не перевантажуйте графік
2. **Релевантність** - показуйте тільки важливі дані
3. **Читабельність** - підписи, легенди, кольори
4. **Консистентність** - єдиний стиль у всьому дашборді

### Поширені помилки:
- Забагато інформації на одному графіку
- Неправильний вибір типу діаграми
- Відсутність контексту або пояснень
- Ігнорування потреб кінцевих користувачів

## Висновок

Візуалізація даних та створення дашбордів - це завершальний етап аналітичного проекту, який перетворює технічні результати в зрозумілі бізнес-інсайти. Успішна візуалізація не тільки показує дані, а й розповідає історію, яка допомагає приймати обґрунтовані рішення.

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


# Розділ лекції: Аналіз текстових даних YouTube

## 2.1 Вступ до проекту аналізу даних YouTube

YouTube є другим за відвідуваністю сайтом у світі після Google. Щодня на платформі переглядається **1 мільярд годин** контенту, що генерує величезні обсяги даних для аналізу.

### Масштаб даних YouTube:
- Понад 2 мільярди активних користувачів щомісяця
- 500 годин відео завантажуються щохвилини
- Мільярди коментарів, лайків та дизлайків щодня

## 2.2 Типи аналізу даних YouTube

### 1. Аналіз настроїв (Sentiment Analysis)

**Мета**: визначити позитивні, негативні або нейтральні настрої в коментарях користувачів.

**Застосування**:
- Оцінка реакції аудиторії на відео
- Розуміння сприйняття бренду або продукту
- Виявлення потенційних проблем або трендів

**Приклад результату**:
```
Позитивні коментарі: 65%
Нейтральні коментарі: 25%  
Негативні коментарі: 10%
```

### 2. Аналіз використання емодзі

**Мета**: дослідження популярності та контексту використання емодзі в коментарях.

**Цікаві інсайти**:
- Які емодзі найчастіше використовуються
- Зв'язок між емодзі та тематикою відео
- Культурні відмінності у використанні емодзі
- Вплив емодзі на engagement

**Візуалізація**: хмара емодзі або топ-список найпопулярніших символів.

### 3. Аналіз співвідношення дизлайків та переглядів

**Мета**: вивчення кореляції між кількістю переглядів та дизлайків.

**Ключові питання**:
- Як зростання переглядів впливає на кількість дизлайків?
- Чи є оптимальне співвідношення для успішного відео?
- Які фактори впливають на це співвідношення?

**Візуалізація**: регресійний графік, що показує тренд зростання дизлайків відносно переглядів.

### 4. Аналіз трендових відео

**Мета**: виявлення закономірностей у популярному контенті.

**Дослідження включає**:
- Аналіз заголовків трендових відео
- Ключові слова та фрази
- Оптимальна довжина заголовків
- Часові патерни популярності

## 2.3 Методологія аналізу текстових даних

### Крок 1: Збір даних
- YouTube API для отримання метаданих
- Парсинг коментарів та описів
- Збір інформації про engagement метрики

### Крок 2: Попередня обробка тексту
- Очищення від HTML тегів та спеціальних символів
- Нормалізація тексту (lowercase, видалення зайвих пробілів)
- Токенізація та лематизація
- Видалення стоп-слів

### Крок 3: Аналіз та візуалізація
- Застосування NLP алгоритмів
- Створення візуалізацій
- Інтерпретація результатів

## 2.4 Інструменти для аналізу

### Python бібліотеки:
- **pandas** - обробка даних
- **numpy** - численні обчислення  
- **matplotlib/seaborn** - візуалізація
- **nltk/spacy** - обробка природної мови
- **textblob** - аналіз настроїв
- **wordcloud** - хмари слів

### API та інструменти:
- YouTube Data API v3
- Google Cloud Natural Language API
- Jupyter Notebooks для інтерактивного аналізу

## 2.5 Практичні застосування результатів

### Для контент-мейкерів:
- Оптимізація заголовків та описів відео
- Розуміння preferences аудиторії
- Покращення engagement метрик

### Для маркетологів:
- Аналіз ефективності рекламних кампаній
- Моніторинг репутації бренду
- Виявлення впливових трендів

### Для дослідників:
- Вивчення соціальних трендів
- Аналіз громадської думки
- Дослідження цифрової поведінки

## 2.6 Етичні міркування

### Важливі принципи:
- **Конфіденційність**: анонімізація персональних даних
- **Згода**: дотримання умов використання платформи
- **Об'єктивність**: уникнення упередженості в інтерпретації
- **Прозорість**: відкритість щодо методології аналізу

## Висновок

Аналіз текстових даних YouTube надає потужні можливості для розуміння цифрової поведінки мільйонів користувачів. Поєднання технік обробки природної мови з візуалізацією даних дозволяє отримувати цінні інсайти для бізнесу, досліджень та створення контенту. Ключем до успіху є правильна методологія збору даних, якісна попередня обробка та етичний підхід до аналізу.

---------------------------------------------------------------------------------------------------------

# Лекція: ETL Pipeline для аналізу YouTube даних

## Вступ

У попередній сесії ми розглянули постановку задачі та ознайомилися з даними. Тепер поглиблено вивчимо ETL pipeline (Extract, Transform, Load) - процес витягування, трансформації та завантаження даних.

## Життєвий цикл проєкту з аналізу даних

### Етапи проєкту:
1. **Розуміння задачі** - аналіз вимог та даних
2. **ETL Pipeline**:
   - **Extract (Витягування)** - збір сирих даних
   - **Transform (Трансформація)** - очищення та підготовка
   - **Load (Завантаження)** - збереження готових даних
3. **EDA** (Exploratory Data Analysis) - дослідницький аналіз
4. **Візуалізація** - створення графіків та діаграм
5. **Дашборд** - фінальна презентація результатів

## Необхідні Python бібліотеки

### Основні модулі для роботи з даними:

```python
import pandas as pd          # Робота з даними та їх маніпуляція
import numpy as np          # Чисельні обчислення та масиви
import seaborn as sns       # Швидка візуалізація даних
import matplotlib.pyplot as plt  # Базова візуалізація
import plotly              # Інтерактивні графіки
```

### Призначення бібліотек:

- **Pandas** - "бог" роботи з даними: читання, модифікація, маніпуляція
- **NumPy** - чисельні операції: середні значення, стандартне відхилення, персентилі, одновимірні масиви
- **Matplotlib** - базовий пакет для візуалізації
- **Seaborn** - швидкі та зручні графіки
- **Plotly** - інтерактивні та динамічні візуалізації

## Практична робота: Завантаження даних

### 1. Створення Jupyter Notebook

```python
# Створіть файл з назвою: YouTube_data_analysis.ipynb
```

### 2. Імпорт необхідних бібліотек

```python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
```

### 3. Читання CSV файлу

```python
# Базовий синтаксис читання CSV
comments = pd.read_csv('шлях_до_файлу/us_comments.csv')
```

### 4. Вирішення проблем з файловими шляхами

**Проблема**: Windows використовує зворотні слеші `\`, а Mac/Linux - прямі `/`

**Рішення**: Використовуйте raw string з префіксом `r`:

```python
comments = pd.read_csv(r'C:\Users\Desktop\us_comments.csv')
```

### 5. Обробка пошкоджених рядків

**Проблема**: Parser error через некоректні рядки

**Рішення**: Додайте параметр для пропуску проблемних рядків:

```python
comments = pd.read_csv(r'шлях_до_файлу/us_comments.csv', 
                      on_bad_lines='skip')
```

## Початковий аналіз даних

### Перегляд структури даних

```python
# Перші 5 рядків
comments.head(5)

# Перевірка типу даних
type(comments)  # pandas.core.frame.DataFrame
```

### Структури даних у Pandas:
- **Series** - одновимірна структура (аналог масиву)
- **DataFrame** - двовимірна структура (таблиця з рядками та стовпчиками)

## Очищення даних (Data Cleaning)

### 1. Виявлення пропущених значень

```python
# Перевірка на наявність пропусків
missing_check = comments.isnull()  # або comments.isna()

# Підрахунок пропусків по кожному стовпчику
missing_count = comments.isnull().sum()
```

**Результат**: Boolean DataFrame, де:
- `False` - значення присутнє
- `True` - значення відсутнє

### 2. Видалення пропущених значень

```python
# Видалення всіх рядків з пропусками
comments.dropna(inplace=True)

# Перевірка результату
comments.isnull().sum()  # Має показати 0 для всіх стовпчиків
```

**Параметр `inplace=True`** означає, що зміни застосовуються до самого DataFrame без створення копії.

## Результати першого етапу

Після виконання цих кроків ми маємо:
- ✅ Завантажені сирі дані з CSV файлу
- ✅ Очищені дані від пропущених значень
- ✅ Готовий DataFrame для подальшого аналізу

## Наступні кроки

У наступних сесіях ми будемо:
1. Проводити детальний дослідницький аналіз (EDA)
2. Створювати візуалізації
3. Будувати дашборд для презентації результатів

## Корисні поради

- Завжди перевіряйте дані після кожного етапу очищення
- Використовуйте `inplace=True` обережно - це незворотна операція
- Зберігайте резервні копії оригінальних даних
- Документуйте всі кроки очищення для відтворюваності результатів

---

*Примітка: Якщо у вас виникають питання, будь ласка, задавайте їх у розділі Q&A.*

-------------------------------------------------------------------------------------------------------------

