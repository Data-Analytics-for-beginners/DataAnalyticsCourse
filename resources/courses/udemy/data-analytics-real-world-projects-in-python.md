

# Data Analytics Real-World Projects in Python

https://ua.udemy.com/course/data-analytics-projects-python/



----------------------------------------------------------

# 1 Вступ до курсу: 5 реальних проєктів аналізу даних

## Чому важливі практичні навички?

Якщо ваша мета стати data analyst або data scientist, необхідно демонструвати практичні навички через розв'язання реальних проєктів. Менеджери та рекрутери компаній не зосереджуються на теоретичних знаннях - їх цікавить здатність розв'язувати справжні бізнес-задачі.

## Про інструктора

**Шан Сінгх** - ваш викладач протягом курсу з понад 8-річним досвідом викладання на Udemy у сферах:
- Аналіз даних
- Data Science
- Штучний інтелект

За роки роботи навчив мільйони студентів по всьому світу.

## Огляд п'яти проєктів курсу

### Проєкт 1: YouTube - Аналіз відеоплатформи

**Контекст:** YouTube - друга за відвідуваністю платформа у світі з величезними обсягами даних.

**Завдання data analyst:**
- **Sentiment Analysis** - аналіз тональності коментарів користувачів
- **Word Cloud Analysis** - візуалізація найчастіших слів
- **Emoji Analysis** - виявлення популярних емодзі з створенням стовпчикових діаграм

**Бізнес-цінність:** Розуміння поведінки та настроїв аудиторії для оптимізації контенту

### Проєкт 2: S&P 500 - Фінансовий аналіз фондового ринку США

**Предмет аналізу:** Дані американського фондового ринку

**Ключові завдання:**
- Розрахунок **щоденної прибутковості** (daily returns)
- Порівняння **цін закриття** акцій
- Застосування **ковзних середніх** (moving averages) для виявлення трендів
- Створення **інтерактивних дашбордів**

**Практична цінність:** Навички фінансового аналізу для роботи в банках та інвестиційних компаніях

### Проєкт 3: Zomato - Геопросторовий аналіз доставки їжі

**Тип аналізу:** Map-based аналіз даних доставки їжі

**Технології візуалізації:**
- **Heat Maps** - теплові карти активності
- **Географічні scatter plots** - точкові карти розташування
- **Cluster Maps** - кластерний аналіз регіонів

**Результат:** Вилучення інсайтів з просторових патернів для оптимізації бізнесу

### Проєкт 4: E-commerce Sales - Аналіз продажів

**Орієнтація:** Методи аналізу топ-компаній (Amazon, Flipkart, Myntra)

**Фокус аналізу:**
- Паттерни продажів
- Поведінка клієнтів  
- Сезонні тренди
- KPI метрики ефективності

**Значення для кар'єри:** Рекрутери високо цінують досвід роботи з e-commerce даними

### Проєкт 5: IPL Cricket - Спортивна аналітика

**Об'єкт дослідження:** Indian Premier League - провідна світова крикетна ліга

**Аналітичні завдання:**
- **Performance Trends** - тренди ефективності гравців
- **Player Statistics** - статистика індивідуальних гравців
- **Team Patterns** - командні паттерни та стратегії
- **Візуалізація** спортивних даних

**Унікальність:** Міжнародні гравці з різних континентів створюють багатовимірні дані

## Кар'єрні можливості

### Цільові позиції після завершення курсу:

**Топові продуктові компанії:**
- Amazon
- Google
- Netflix
- Microsoft

**Ролі:**
- **Data Analyst** - аналітик даних
- **Data Scientist** - дослідник даних
- **Business Analyst** - бізнес-аналітик
- **Financial Analyst** - фінансовий аналітик

## Структура навчання

### Підхід до кожного проєкту:
1. **Постановка задачі** - розуміння бізнес-контексту
2. **Збір та підготовка даних** - ETL процеси
3. **Дослідницький аналіз** - EDA (Exploratory Data Analysis)
4. **Візуалізація** - створення графіків та дашбордів
5. **Інсайти та рекомендації** - бізнес-висновки

### Технічний стек:
- **Python** - основна мова програмування
- **Pandas** - маніпулювання даними
- **Matplotlib/Seaborn** - статична візуалізація
- **Plotly** - інтерактивні графіки
- **Jupyter Notebook** - середовище розробки

## Портфоліо та резюме

### Як презентувати проєкти:

**На платформі GitHub:**
- Структуровані репозиторії для кожного проєкту
- README файли з описом завдань та результатів
- Jupyter notebooks з детальним аналізом

**У резюме:**
- Конкретні досягнення з кожного проєкту
- Використані технології та методи
- Бізнес-інсайти та їх вплив

**На співбесідах:**
- Демонстрація візуалізацій та дашбордів
- Пояснення методології та підходу
- Обговорення виявлених паттернів

## Переваги практичного підходу

### Чому роботодавці цінують проєктний досвід:

1. **Реальні кейси** - досвід з справжніми даними та викликами
2. **Комплексні навички** - від збору даних до презентації результатів  
3. **Бізнес-мислення** - розуміння практичного застосування аналітики
4. **Технічна універсальність** - робота з різними типами та джерелами даних

### Конкурентні переваги:
- Демонстрація здатності завершувати проєкти повністю
- Показ творчого підходу до розв'язання проблем
- Підтвердження готовності до реальної роботи
- Навички презентації результатів стейкхолдерам

Цей курс забезпечує не лише технічні знання, але й практичний досвід, необхідний для успішної кар'єри в аналітиці даних.


------------------------------------------------------------------------------------------------------------------

# 2 Оголошення: Підтримка студентів через Q&A секцію курсу

## Наша ініціатива підтримки

Ми запроваджуємо спеціальну ініціативу для вирішення всіх ваших запитань через Q&A секцію курсу.

## Гарантії швидкості відповіді

### Стандартний час відповіді:
- **2-3 години** - для більшості запитань
- **95% запитань** вирішуємо протягом робочого дня
- **Максимум 8-10 годин** у складних випадках

## Типи запитань, з якими ми допомагаємо

### Технічні проблеми:
- **Проблеми з кодом** - будь-який рядок коду
- **Помилки виконання** - надішліть скріншот помилки
- **Налагодження програм** - покрокова допомога
- **Встановлення програмного забезпечення**

### Кар'єрні консультації:
- **Перехід у Data Science** - планування кар'єрного шляху
- **Штучний інтелект** - можливості та перспективи
- **Портфоліо** - поради щодо презентації проєктів
- **Підготовка до співбесід** - практичні рекомендації

### Навчальні питання:
- **Пояснення концепцій** - додаткові роз'яснення матеріалу
- **Практичні завдання** - допомога з виконанням
- **Альтернативні рішення** - різні підходи до задач

## Як ефективно використовувати Q&A

### При технічних проблемах:
1. **Надішліть скріншот** помилки або коду
2. **Опишіть контекст** - що ви намагались зробити
3. **Вкажіть операційну систему** та версії програм
4. **Додайте код**, який викликає проблему

### При кар'єрних питаннях:
1. **Опишіть поточну ситуацію** - освіта, досвід
2. **Вкажіть цілі** - куди хочете потрапити
3. **Часовий горизонт** - коли плануєте перехід
4. **Специфічні сфери інтересів** - напрямки в DS/AI

## Доказ ефективності

**Сотні студентів** вже скористались нашою Q&A підтримкою та отримали:
- Швидкі вирішення технічних проблем
- Персоналізовані кар'єрні поради
- Додаткові навчальні матеріали
- Мотивацію для продовження навчання

## Рекомендації по використанню

### Максимізуйте користь:
- **Не соромтесь питати** - немає "дурних" запитань
- **Деталізуйте проблему** - чим більше інформації, тим краща допомога
- **Слідкуйте за відповідями** інших студентів - можете знайти рішення своїх проблем
- **Діліться досвідом** - допомагайте іншим студентам

### Типи корисного контенту в Q&A:
- Поширені помилки та їх рішення
- Альтернативні способи виконання завдань
- Рекомендації додаткових ресурсів
- Реальні кейси з практики

## Переваги активної участі в Q&A

### Для навчання:
- **Поглиблення розуміння** через питання інших
- **Різні перспективи** на одну задачу
- **Реальні проблеми** та їх вирішення
- **Спільнота однодумців**

### Для кар'єри:
- **Персоналізовані поради** від досвідчених фахівців
- **Мережування** з іншими студентами
- **Інсайдерська інформація** про індустрію
- **Мотивація** через успіхи інших

## Етикет Q&A секції

### Рекомендації:
- **Формулюйте запитання чітко** та конкретно
- **Шукайте спочатку** - можливо, питання вже обговорювалось
- **Дякуйте за допомогу** - підтримуйте дружню атмосферу
- **Відмічайте корисні відповіді** для інших студентів

### Чого уникати:
- Дублювання однакових запитань
- Неконструктивна критика
- Запити на виконання домашніх завдань цілком
- Питання, не пов'язані з курсом

## Заохочення до активності

Використовуйте цю можливість **максимально** - це безкоштовна персональна підтримка, яка рідко зустрічається в онлайн курсах. Ваша активність у Q&A секції може стати ключовим фактором успішного завершення курсу та кар'єрного розвитку.

**Пам'ятайте:** кожне запитання робить вас ближче до мети стати успішним data analyst або data scientist.

--------------------------------------------------------


# 3 Передумови курсу: Основи Python програмування

## Важливе попередження

Перед початком цього курсу необхідно переконатися, що ви добре знаєте основи Python, оскільки всі кейси та проєкти будуть виконуватися виключно на Python.

## Для кого обов'язковий підготовчий курс

### Якщо ви НЕ знаєте Python:
- Люди з нетехнічних сфер
- Особи без досвіду програмування
- Новачки в IT сфері
- Ті, хто раніше не працював з Python

### Якщо ви ВЖЕ знаєте Python:
- Можете пропустити підготовчий курс
- Одразу переходьте до основних проєктів
- Підготовчий курс буде неінформативним для вас

## Безкоштовний підготовчий курс Python

### Як отримати доступ:
1. Натисніть кнопку **"Enroll Now"**
2. Зареєструйтесь **безкоштовно**
3. Отримайте повний доступ до матеріалів

### Характеристики курсу:
- **Тривалість:** Приблизно 90 хвилин
- **Вартість:** Повністю безкоштовний
- **Формат:** Практично-орієнтований
- **Рівень:** Для абсолютних початківців

## Зміст підготовчого курсу

### Базові концепції Python:

#### 1. Основи синтаксису
- **Keywords** - ключові слова мови
- **Identifiers** - ідентифікатори змінних та функцій
- **Comments** - коментарі в коді
- **Statements** - інструкції та команди

#### 2. Змінні та типи даних
- **Оголошення змінних** - як створювати змінні
- **Типи даних** - числа, рядки, булеві значення
- **Присвоєння значень** - операції з змінними
- **Перетворення типів** - casting між типами

#### 3. Ключові структури даних
- **List (Списки)** - впорядковані колекції даних
- **Set (Множини)** - унікальні елементи без дублікатів  
- **Dictionary (Словники)** - пари ключ-значення
- **Tuple (Кортежі)** - незмінні послідовності

## Чому ці основи критично важливі

### Для аналізу даних потрібні:

**Списки (Lists):**
```python
# Зберігання множинних значень
sales_data = [100, 250, 175, 300, 220]
user_names = ['Alice', 'Bob', 'Charlie']
```

**Словники (Dictionaries):**
```python
# Структурування даних ключ-значення
user_data = {
    'name': 'John',
    'age': 25,
    'country': 'Ukraine'
}
```

**Множини (Sets):**
```python
# Унікальні значення та операції над множинами
unique_categories = {'Music', 'Gaming', 'Education'}
```

### Практичне застосування в проєктах:

1. **YouTube аналіз** - словники для зберігання метаданих відео
2. **Фінансові дані** - списки цін акцій та часових міток
3. **Геодані** - структури для координат та адрес
4. **E-commerce** - словники товарів та характеристик
5. **Спортивна статистика** - множини команд та гравців

## Рекомендації щодо вивчення

### Для новачків:
- **Обов'язково пройдіть** підготовчий курс
- **Практикуйтесь** з кожною концепцією
- **Не поспішайте** - забезпечте міцне розуміння основ
- **Записуйте приклади** для майбутнього використання

### Для досвідчених:
- **Швидко оцініть** свій рівень знань
- **Пропустіть підготовку**, якщо впевнені в основах
- **Зосередьтесь** на специфічних бібліотеках для аналізу даних
- **Одразу переходьте** до практичних проєктів

## Самооцінка готовності

### Перевірте себе - чи знаєте ви як:
- [ ] Створювати та модифікувати списки
- [ ] Працювати зі словниками (додавати, видаляти, отримувати значення)
- [ ] Використовувати множини для унікальних операцій
- [ ] Писати прості цикли `for` та `while`
- [ ] Створювати умовні конструкції `if-elif-else`
- [ ] Визначати та викликати функції
- [ ] Розуміти поняття змінних та типів даних

### Якщо на більшість пунктів відповіли "НІ":
Підготовчий курс **обов'язковий** для вашого успіху в основному курсі.

### Якщо на більшість пунктів відповіли "ТАК":
Можете сміливо переходити до проєктів аналізу даних.

## Наступні кроки

1. **Чесно оцініть** свій поточний рівень
2. **Пройдіть підготовку** за потреби
3. **Практикуйтесь** з базовими концепціями
4. **Переходьте** до захоплюючих проєктів аналізу даних

**Пам'ятайте:** міцна основа в Python - запорука успіху в аналізі даних та data science!


--------------------------------------------------------------------------------------------------


# 4 Встановлення Anaconda Navigator для Python аналізу даних

## Необхідне програмне забезпечення

Для проходження курсу з аналізу даних на Python потрібно встановити лише одну програму - **Anaconda Navigator**.

## Що таке Anaconda Navigator?

**Anaconda Navigator** - це дистрибутив Python, який дозволяє:
- Програмувати на Python та R
- Автоматично управляти пакетами та залежностями
- Включає попередньо встановлений Python
- Надає доступ до Jupyter Notebook та інших інструментів data science

## Крок 1: Завантаження Anaconda

### Пошук і доступ до сайту:
1. Відкрийте Google
2. Введіть запит: **"download Anaconda Navigator"**
3. Натисніть на **перше посилання** у результатах пошуку

### Вибір версії для завантаження:
1. **Прокрутіть вниз** до розділу завантажень
2. **Оберіть операційну систему:**
   - Windows
   - Mac
   - Linux

3. **Визначте розрядність процесора:**
   - 64-bit (рекомендовано для сучасних систем)
   - 32-bit (для старіших комп'ютерів)

### Приклад для Windows:
- Операційна система: **Windows**
- Процесор: **64-bit**
- Натисніть відповідну кнопку завантаження

## Крок 2: Встановлення Anaconda

### Процес інсталяції:
1. **Знайдіть завантажений файл** (.exe) у папці Downloads
2. **Двічі клікніть** на файл для запуску установки
3. **Оберіть шлях встановлення** або залиште за замовчуванням
4. **Натисніть "Next"** для продовження
5. **Прийміть умови ліцензії** Anaconda
6. **Натисніть "Next"** знову
7. **Дочекайтесь завершення** процесу встановлення
8. **Натисніть "Finish"** для завершення

### Важливі налаштування:
- Залиште всі опції за замовчуванням
- Дозвольте додавання Anaconda до системного PATH
- Рекомендується встановити як системний Python

## Крок 3: Запуск Jupyter Notebook

### Відкриття через меню Windows:
1. **Натисніть клавішу Windows** на клавіатурі
2. **Введіть:** "Jupyter Notebook"
3. **Натисніть Enter** або клікніть на результат

### Альтернативний спосіб:
- Відкрийте Anaconda Navigator з меню Пуск
- Знайдіть Jupyter Notebook та натисніть "Launch"

## Крок 4: Створення нового Python проєкту

### У інтерфейсі Jupyter Notebook:
1. **Натисніть кнопку "New"** у правому верхньому куті
2. **Оберіть "Python 3"** зі спадного меню
3. **Відкриється новий notebook** для програмування

### Інтерфейс Jupyter Notebook:
- **Клітинки коду** - для написання Python коду
- **Кнопка Run** - для виконання коду
- **Меню File** - для збереження та експорту
- **Kernel** - для управління Python процесом

## Переваги використання Anaconda

### Для початківців:
- **Все в одному** - не потрібно окремо встановлювати Python
- **Попередньо налаштовано** - всі необхідні пакети включені
- **Jupyter Notebook** - зручне середовище для навчання
- **Простота управління** - графічний інтерфейс

### Для аналізу даних:
- **Pandas** - для роботи з даними
- **NumPy** - для числових обчислень
- **Matplotlib/Seaborn** - для візуалізації
- **Scikit-learn** - для машинного навчання

## Перевірка правильності встановлення

### Тестовий код для перевірки:
```python
# Перевірка версії Python
import sys
print(f"Python версія: {sys.version}")

# Перевірка основних пакетів
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

print("Всі основні пакети успішно імпортовані!")
```

### Якщо виникли проблеми:
- Переконайтеся, що Anaconda встановлено правильно
- Перезавантажте комп'ютер
- Спробуйте запустити Anaconda Navigator як адміністратор

## Альтернативи запуску

### Через командний рядок:
1. Відкрийте Command Prompt (cmd)
2. Введіть: `jupyter notebook`
3. Натисніть Enter

### Через Anaconda Prompt:
1. Знайдіть "Anaconda Prompt" у меню Пуск
2. Введіть: `jupyter notebook`
3. Відкриється браузер з інтерфейсом

## Структура робочого простору

### Рекомендована організація файлів:
```
DataAnalysisProjects/
├── Project1_YouTube/
│   ├── data/
│   ├── notebooks/
│   └── results/
├── Project2_StockMarket/
├── Project3_Zomato/
├── Project4_Sales/
└── Project5_IPL/
```

### Корисні поради:
- Створіть окрему папку для кожного проєкту
- Регулярно зберігайте роботу (Ctrl+S)
- Використовуйте описові назви для notebooks
- Додавайте коментарі до коду

## Готовність до курсу

Після успішного встановлення Anaconda та запуску Jupyter Notebook ви готові до:
- Виконання всіх проєктів курсу
- Роботи з реальними наборами даних
- Створення професійних візуалізацій
- Розробки аналітичних звітів

**Вітаємо!** Тепер ви готові розпочати захоплюючу подорож у світ аналізу даних з Python.


--------------------------------------------------------------------------------------------------

# 5 Огляд Jupyter Notebook: Інтерактивне середовище для аналізу даних

## Що таке Jupyter Notebook?

**Jupyter Notebook** (раніше IPython Notebook) - це веб-додаток, який виглядає як веб-сторінка, але працює локально у вашому браузері. На відміну від статичних веб-сторінок, Jupyter дозволяє інтерактивно змінювати код та миттєво бачити результати.

## Три основні компоненти Jupyter Notebook

### 1. Живий код (Live Code)

```python
# Приклад інтерактивного коду
x = 10
print(x)
```

**Особливості:**
- Код виконується в реальному часі
- Результати відображаються безпосередньо під клітинкою
- Можна змінювати код та переприйомати для оновлення результатів
- Комбінація клавіш: **Ctrl + Enter** для виконання

### 2. Візуалізація даних

```python
import matplotlib.pyplot as plt
import numpy as np

# Створення даних
x = np.linspace(0, 10, 100)
y = np.sin(x)

# Візуалізація з можливістю зміни параметрів
plt.scatter(x, y, color='red')  # Можна змінити колір на 'blue', 'green' тощо
plt.show()
```

**Переваги візуалізації:**
- Миттєве оновлення графіків при зміні параметрів
- Інтерактивні елементи управління
- Можливість збереження візуалізацій

### 3. Пояснювальний текст (Rich Text)

Використовуючи **Markdown** можна додавати:

```markdown
### Дані у формі масиву

Ця секція пояснює структуру даних.
```

**Функціонал Markdown:**
- Заголовки різних рівнів (використовуючи #, ##, ###)
- Форматований текст (жирний, курсив)
- Списки та таблиці
- Математичні формули
- Зображення та логотипи

## Порівняння з іншими IDE

### Jupyter Notebook vs PyCharm/Spyder

| Характеристика | Jupyter Notebook | PyCharm/Spyder |
|---------------|------------------|----------------|
| **Інтерактивність** | Висока - код виконується по клітинках | Низька - виконання всього файлу |
| **Візуалізація** | Вбудована в інтерфейс | Окремі вікна |
| **Проміжні результати** | Автоматично зберігаються | Потрібен Variable Explorer |
| **Документація** | Markdown в тому самому файлі | Окремі коментарі |
| **Поділ коду** | Логічні блоки в клітинках | Один суцільний файл |

### Приклад роботи з проміжними станами

```python
# Створення даних
import numpy as np
x = np.array([1, 2, 3, 4, 5])
y = np.array([2, 4, 6, 8, 10])

# В Jupyter можна перевірити дані на будь-якому етапі
print("X:", x)
print("Y:", y)

# Створення візуалізації
import matplotlib.pyplot as plt
plt.scatter(x, y)
plt.show()
```

**У Jupyter:** Кожна змінна доступна та видна між клітинками
**У PyCharm:** Потрібно використовувати Variable Explorer для перегляду змінних

## Переваги Jupyter Notebook

### Для аналізу даних:
- **Крокове виконання** - можна виконувати код частинами
- **Експериментування** - легко тестувати різні підходи
- **Документування** - код та пояснення в одному місці
- **Візуальна привабливість** - професійно виглядають звіти

### Для навчання:
- **Інтерактивність** - миттєвий зворотний зв'язок
- **Поступовість** - вивчення концепцій крок за кроком
- **Візуальне навчання** - графіки та діаграми
- **Легке поділення** - можна показувати окремі частини

### Для співпраці:
- **GitHub інтеграція** - notebooks відображаються на GitHub
- **Поділення** - легко ділитися через різні платформи
- **Відтворюваність** - інші можуть запустити ваш код

## Технічні можливості

### Підтримка мов програмування:
- **40+ мов програмування**: Python, R, Julia, Ruby, Scala
- **Kernels** - різні backend для різних мов
- **Багатомовність** - можна використовувати кілька мов в одному notebook

### Розширені функції:
- **Magic Commands** - спеціальні команди для оптимізації
- **Widgets** - інтерактивні елементи управління
- **Extensions** - додаткові можливості через розширення

## Практичні застосування

### У дослідженнях:
- Наукові публікації з відтворюваним кодом
- Прототипування алгоритмів
- Аналіз експериментальних даних

### У бізнесі:
- Звіти для стейкхолдерів
- Exploratory Data Analysis (EDA)
- Презентація результатів аналізу

### В освіті:
- Інтерактивні курси програмування
- Демонстрації концепцій
- Домашні завдання з кодом

## Найкращі практики використання

### Структурування notebook:
1. **Заголовок та опис** проєкту
2. **Імпорт бібліотек** на початку
3. **Завантаження даних** в окремих клітинках
4. **Аналіз по етапах** з поясненнями
5. **Висновки** в кінці

### Поради для ефективності:
- Використовуйте описові назви змінних
- Додавайте Markdown пояснення для складної логіки
- Регулярно зберігайте роботу (Ctrl+S)
- Перезапускайте kernel при необхідності
- Очищайте output перед поділенням

## Початок роботи

### Створення нового notebook:
1. Відкрийте Jupyter через Anaconda Navigator
2. Натисніть **"New"** → **"Python 3"**
3. Почніть з імпорту необхідних бібліотек
4. Додайте заголовок через Markdown

### Корисні комбінації клавіш:
- **Ctrl + Enter** - виконати клітинку
- **Shift + Enter** - виконати та перейти до наступної
- **Alt + Enter** - виконати та створити нову клітинку
- **A** - додати клітинку вгорі
- **B** - додати клітинку внизу
- **M** - змінити на Markdown
- **Y** - змінити на Code

Jupyter Notebook є ідеальним інструментом для інтерактивного аналізу даних, що поєднує код, візуалізацію та документацію в єдиному, зручному для використання середовищі.


---------------------------------------------------------------------------------------















# Розділ лекції: Основи аналітики даних

## 1.1 Що таке аналітика даних?

Аналітика даних - це процес застосування аналітичних технік до даних для отримання змістовних висновків. Ці висновки також називають **інсайтами** або **висновками**, які підтримують краще прийняття рішень та допомагають організаціям і компаніям.

### Три ключові компоненти аналітики даних:

1. **Дані** - вихідна інформація
2. **Аналітичні техніки** - методи обробки 
3. **Висновки** - інсайти для прийняття рішень

## 1.2 Що таке дані?

**Дані** - це сира інформація, яку можна збирати з різних джерел залежно від проекту, над яким ви працюєте.

### Приклад збору даних в Amazon:

Якщо ви аналітик даних в Amazon і маєте завдання зібрати транзакційні дані користувачів, ви можете збирати такі атрибути:

- Що купив кожен клієнт
- Скільки витратив покупець
- Які пристрої використовують користувачі Amazon
- Локація покупців

### Джерела даних:
- Корпоративні бази даних
- Excel файли
- CSV файли
- Фреймворки для великих даних

## 1.3 Аналітичні техніки

Після отримання даних необхідно застосувати аналітичні техніки для отримання змістовних висновків. До таких технік належать:

- Різні статистичні методи
- Побудова графіків та діаграм
- Техніки дослідницького аналізу даних
- Очищення даних

## 1.4 Висновки та інсайти

### Приклад висновків з аналізу даних Amazon:

1. **iPhone є найпопулярнішим товаром** на Amazon
2. **Знижки останньої хвилини** допомагають збільшити продажі
3. **Рекомендовані товари**, які часто купують разом

## 1.5 Повсякденний приклад: приготування піци

Для кращого розуміння концепції розглянемо аналогію з приготуванням піци:

### Інгредієнти (сирі дані):
- Борошно
- Сир
- Овочі
- Соуси та топінги

### Процес приготування (аналітичні техніки):
- Приготування тіста
- Випікання піци

### Висновки (інсайти):
Після куштування піци ви можете зробити висновок: "наступного разу додам більше сиру"

## 1.6 Реальний приклад: авіакомпанія

### Сценарій: 
Ви - аналітик даних в авіакомпанії

### Дані:
- Міста відправлення та призначення
- Ціни квитків
- Дати подорожей
- Приблизно 10 мільйонів спостережень

### Завдання для аналізу:
1. **Сезонний попит на рейси** - коли люди подорожують найчастіше?
2. **Оцінка задоволеності пасажирів** - наскільки клієнти задоволені сервісом?
3. **Найекономічніші авіалінії** - які компанії пропонують найкращі ціни?

### Результат:
Перетворення сирих даних про польоти в розумні бізнес-рішення, які допомагають покращити роботу авіакомпанії.

## Висновок

Аналітика даних - це процес перетворення сирих даних у практичні інсайти, що допомагають організаціям приймати кращі рішення та покращувати свою діяльність. Основна суть полягає в тому, щоб використовувати дані для підтримки розумного прийняття рішень у бізнесі.

---------------------------------------------------------------------------

# Розділ лекції: Відмінності між аналітикою даних та наукою про дані

## 1.7 Основні відмінності між Data Analytics та Data Science

### Ключова різниця:
**Наука про дані = Аналітика даних + Прогнозне моделювання**

### Аналітика даних:
1. **Збір даних** - отримання сирої інформації
2. **Очищення даних** - підготовка до аналізу
3. **Аналіз даних** - дослідження закономірностей
4. **Отримання інсайтів** - висновки для прийняття рішень

### Наука про дані:
Все, що є в аналітиці даних **ПЛЮС**:
- **Прогнозне моделювання** - використання алгоритмів для передбачення майбутніх подій
- **Машинне навчання**
- **Глибоке навчання**
- **Обробка природної мови (NLP)**
- **Алгоритми часових рядів**

## 1.8 Повсякденний приклад: туристична компанія

### Сценарій: 
Аналітик в компанії MakeMyTrip або Goibibo збирає дані про ваші подорожі

### Аналітика даних може відповісти на питання:
- **Куди ви їздили** в останній поїздці?
- **Скільки витратили** на останніх 10 подорожах?
- **Що пройшло добре** під час подорожей?
- **Які напрямки були найприємнішими?**

### Наука про дані може передбачити:
- **Який буде найкращий напрямок** для вашого наступного відпустки?
- **Коли найкраще їхати** в цей напрямок?
- **Скільки коштуватиме** ця подорож?

### Різниця в підході:
- **Аналітика даних**: розуміння минулого (що і чому сталося)
- **Наука про дані**: передбачення майбутнього на основі аналізу минулих даних

## 1.9 Реальний приклад: YouTube

### Дані для аналізу:
- Назва відео
- ID відео
- Співвідношення лайків
- Рівень залученості
- Довжина заголовка
- Чи стало відео вірусним

*Уявімо, що у нас є 1 мільйон таких записів*

### Аналітика даних дозволяє визначити:
- **Яке відео має найвище співвідношення лайків?**
- **У якого відео найвищий рівень залученості?**

### Наука про дані може передбачити:
- **Чи стане нове відео, яке я завантажу, вірусним?**
- Використовуючи минулі дані та алгоритми машинного навчання для прогнозування

## 1.10 Підсумок відмінностей

| Аспект | Аналітика даних | Наука про дані |
|--------|----------------|----------------|
| **Фокус** | Аналіз минулих даних | Аналіз + прогнозування |
| **Питання** | Що і чому сталося? | Що станеться в майбутньому? |
| **Методи** | Статистичний аналіз, візуалізація | ML, AI алгоритми, прогнозні моделі |
| **Результат** | Інсайти та висновки | Передбачення та рекомендації |
| **Часова орієнтація** | Минуле та теперішнє | Майбутнє |

## Висновок

Аналітика даних зосереджена на розумінні того, що сталося в минулому та чому це сталося. Наука про дані використовує цю інформацію для створення моделей, які можуть передбачати майбутні події та тенденції, застосовуючи складні алгоритми машинного навчання та штучного інтелекту.

-------------------------------------------------------------------------------------------------------




flowchart TD
    A[Understanding Use-case] --> B[Run ETL Pipeline]
    B --> C[EDA]
    B --> D[Extract Data]
    B --> E[Transform Data into Acceptable Format]
    B --> F[Load Data]
    C --> G[Conclusion]
    C --> H[Data Viz.]
    G --> I[Dashboarding]
    H --> I
    
    %% ETL Process details
    E -.-> J[Remove Duplicate Rows<br/>Remove In-relevant Rows/Cols<br/>Fix Errors<br/>Fix Missing Values & Data Types<br/>Deal with Outliers]
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style G fill:#fff3e0
    style I fill:#fce4ec



--------------------------------------------------------------------------------------------------------------------------------------------------------

# Розділ лекції: Життєвий цикл проекту з аналітики даних

## 1.11 Життєвий цикл проекту з аналітики даних

Для вирішення будь-якого реального проекту з аналітики даних необхідно дотримуватися певного життєвого циклу:

### Основні етапи:
1. **Розуміння use case** (бізнес-розуміння)
2. **Запуск ETL pipeline**
3. **Виконання EDA** (дослідницького аналізу даних)
4. **Формулювання висновків** з EDA
5. **Створення дашбордів** для проекту

## 1.12 Приклад застосування

### Сценарій: Аналітик даних в Amazon

**Дані про продажі:**
- Продукт, купований користувачем
- Ім'я користувача
- Ціна, сплачена користувачем
- Країна покупки

**Приклад датасету:**
```
Продукт    | Користувач | Ціна (₹) | Країна
iPhone     | John       | 80,000   | US
MacBook    | Rahul      | 120,000  | India
Phone      | Saurabh    | 25,000   | India
```

*Уявімо, що у нас є 1 мільйон таких записів*

### Можливі інсайти після аналізу:
- **iPhone та MacBook** є найпопулярнішими товарами на Amazon
- **Люди з Азії та Європи** зазвичай купують преміум продукти
- **Підлітки** рідше купують дорогі товари

## 1.13 Етап 1: Розуміння use case (бізнес-розуміння)

### Ключові учасники проекту:

#### Клієнт
- **Хто це**: власник бізнесу з проблемою
- **Приклад**: власник роздрібного магазину
- **Проблема**: різке падіння продажів минулого місяця
- **Потреба**: вирішити проблему зниження продажів

#### Product Owner (Власник продукту)
- **Роль**: розуміє потреби клієнта
- **Функції**:
  - Перетворює потреби клієнта в цілі та пріоритети
  - Визначає завдання для аналітичної команди
  - Приклад рішень: "Спочатку проаналізуємо дані продажів, потім відгуки клієнтів"
- **Відповідальність**: надання правильного продукту або рішення клієнту

#### Business Analyst (Бізнес-аналітик)
- **Роль**: міст між бізнесом та технічними командами
- **Функції**:
  - Спілкується з клієнтом та власником продукту
  - Перетворює бізнес-потреби в технічні завдання
  - Готує завдання для data команди (аналітики, інженери, data scientist)

## 1.14 Процес збору вимог

### Етапи взаємодії:
1. **Клієнт** → розмова з **Product Owner** + **Business Analyst**
2. **Збір вимог** для проекту
3. **Передача завдань** команді аналітики даних

### Ключові питання аналітика даних:
- **Звідки збирати дані?**
  - Сторонні API?
  - Внутрішні системи компанії?
  - Бази даних компанії?
  - Веб-скрапінг з сайтів?
  - Big Data джерела?
  - Інші джерела?

## 1.15 Основна мета першого етапу

**Головне завдання**: визначити джерела даних для проекту

**Принцип**: "Без даних немає ні data science, ні data analytics"

**Результат**: чітке розуміння:
- Що потрібно проаналізувати
- Звідки взяти дані
- Які технічні завдання необхідно виконати

## Висновок

Перший етап життєвого циклу проекту з аналітики даних - це фундамент успішного проекту. Правильне розуміння бізнес-потреб та ідентифікація джерел даних критично важливі для подальшого успіху всього проекту.

-----------------------------------------------------------------------------------------------------------------------------------------------------

# Розділ лекції: ETL Pipeline в аналітиці даних

## 1.16 Що таке ETL Pipeline?

**ETL** - це другий етап життєвого циклу проекту з аналітики даних, який виконується командою аналітиків та big data інженерів.

### Розшифровка ETL:
- **E** - **Extract** (Витягування) - отримання сирих даних
- **T** - **Transform** (Трансформація) - очищення та підготовка даних
- **L** - **Load** (Завантаження) - збереження підготовлених даних

## 1.17 Етап 1: Extract (Витягування даних)

### Джерела даних:

#### Структуровані формати:
- **CSV** (Comma Separated Values) - дані, розділені комою
- **TSV** (Tab Separated Values) - дані, розділені табуляцією
- **JSON** - колекція пар "ключ-значення"

#### Бази даних:
**SQL бази даних** (табличні):
- MySQL, PostgreSQL, SQL Server
- Дані зберігаються у формі таблиць (рядки та стовпці)
- Доступ через SQL запити

**NoSQL бази даних** (документо-орієнтовані):
- MongoDB, Cassandra
- Дані зберігаються у формі документів

#### Big Data платформи:
- Hive, Hadoop, Spark
- Використовують HiveQL (подібна до SQL)

### Приклади форматів даних:

**CSV формат:**
```
Name,Age,City
John,25,New York
Mary,30,London
```

**TSV формат:**
```
Name    Age    City
John    25     New York
Mary    30     London
```

**JSON формат:**
```json
[
  {"Name": "John", "Age": 25, "City": "New York"},
  {"Name": "Mary", "Age": 30, "City": "London"}
]
```

## 1.18 Розподіл джерел даних

### Внутрішні джерела (60-80% часу):
- Корпоративні бази даних
- Внутрішні системи компанії
- Data warehouses та data lakes

### Зовнішні джерела (20-40% часу):
- API третіх сторін (комерційні, платні)
- Web scraping з веб-сайтів
- Відкриті дані

## 1.19 Етап 2: Transform (Трансформація даних)

**Найважливіший етап**: займає **2/3 часу всього проекту**

### Типові проблеми сирих даних:
- Помилки та опечатки
- Пропущені значення
- Дублікати рядків та стовпців
- Нерелевантні дані
- Неправильні типи даних
- Викиди (outliers)

### Синоніми трансформації:
- Data Cleaning (очищення даних)
- Data Pre-processing (попередня обробка)
- Data Wrangling (обробка даних)
- Data Preparation (підготовка даних)

**Результат**: чисті, підготовлені, featurized дані

## 1.20 Етап 3: Load (Завантаження даних)

Вибір способу зберігання залежить від обсягу даних:

### Великі дані (петабайти, терабайти):
- **Big Data платформи**: Hive, Hadoop, Spark clusters
- **Відповідальний**: Big Data Engineer

### Середні дані (10-100 ГБ):
- **Бази даних**: SQL або NoSQL
- Залежить від специфіки проекту

### Малі дані:
- **Файли**: CSV, TSV, JSON, Excel
- Простіше зберігання та доступ

## 1.21 Схема ETL процесу

```
Сирі дані → Data Cleaning → Чисті дані → Зберігання → Подальший аналіз
```

### Деталізація трансформації:
1. Виправлення помилок
2. Обробка пропущених значень
3. Видалення дублікатів
4. Фільтрація нерелевантних даних
5. Приведення до правильних типів даних
6. Обробка викидів

## 1.22 Роль Big Data Engineer

**Відповідальність**: ефективне зберігання підготовлених даних

**Вибір технології залежить від**:
- Обсягу даних
- Швидкості доступу
- Типу аналізу
- Бюджету проекту

## Висновок

ETL Pipeline - це критично важливий етап, який забезпечує якість даних для подальшого аналізу. Без якісного ETL неможливо отримати достовірні інсайти та зробити правильні бізнес-рішення. Найбільше уваги потребує етап трансформації, оскільки якість очищення даних безпосередньо впливає на результати всього проекту.

-----------------------------------------------------------------------------------------------------------------------

# Розділ лекції: Візуалізація даних та створення дашбордів

## 1.23 Від EDA до візуалізації

Після виконання дослідницького аналізу даних (EDA) ми отримуємо висновки та інсайти з даних. Однак великі обсяги текстової інформації важко сприймати та аналізувати.

### Проблема текстових даних:

**Приклад результатів EDA** (найпопулярніші продукти):
```
iPhone: 50,000 проданих одиниць
OnePlus: 35,000 проданих одиниць
MacBook: 60,000 проданих одиниць
Samsung: 42,000 проданих одиниць
```

Коли у нас мільйон записів, така текстова інформація стає незрозумілою для швидкого аналізу.

## 1.24 Рішення: Візуалізація даних

### Що таке візуалізація даних?
**Візуалізація даних** - це процес створення графіків та діаграм для наочного представлення результатів аналізу.

### Приклад: стовпчикова діаграма

```
Продажі продуктів
│
│     ┌───┐
│ ┌───┤   │
│ │   │   │   ┌───┐
│ │   │   │   │   │   ┌───┐
└─┴───┴───┴───┴───┴───┴───┴─
iPhone OnePlus MacBook Samsung
```

**Результат**: одним поглядом можна побачити, що MacBook є лідером продажів.

## 1.25 Інструменти для візуалізації в Python

### Основні бібліотеки:

#### 1. **Pandas**
- Вбудовані можливості візуалізації
- Швидкі і прості графіки

#### 2. **Matplotlib** 
- Базовий модуль для візуалізації
- Фундамент для інших бібліотек
- Статичні графіки

#### 3. **Seaborn**
- Красивіші статистичні графіки
- Побудований на Matplotlib

#### 4. **Plotly**
- Інтерактивні та динамічні графіки
- Можливість взаємодії з користувачем

## 1.26 Важливість інференцій

### Ключовий принцип:
**Якщо ви не можете зробити висновки з ваших графіків, то ваш аналіз марний.**

### Необхідність комунікації:
- **Ділитеся результатами** з колегами та командою
- **Пояснюйте технічні речі** простою мовою
- **Робіть висновки зрозумілими** для нетехнічної аудиторії

### Критерії успішної візуалізації:
1. Графік легко читається
2. Висновки очевидні з першого погляду
3. Дані підтверджують або спростовують гіпотези
4. Результати можна пояснити бізнесу

## 1.27 Дашборди

### Що таке дашборд?
**Дашборд** - це інтерактивна панель, що об'єднує всі аналітичні результати на одному екрані.

### Приклад: дашборд фондового ринку

**Функціональність:**
- Вибір компанії (Amazon, Microsoft, Apple, Google)
- Автоматичне оновлення всіх графіків
- Різні види аналізу в одному місці:
  - Динаміка цін
  - Обсяги торгівлі  
  - Технічні індикатори
  - Порівняльний аналіз

### Переваги дашбордів:

#### Для аналітиків:
- Централізація всіх результатів
- Швидкий доступ до різних метрик
- Можливість інтерактивного дослідження

#### Для бізнесу:
- Миттєвий доступ до ключових показників
- Не потрібні технічні знання
- Можливість самостійного аналізу

#### Для команди:
- Спільне використання результатів
- Однакове розуміння даних
- Ефективне прийняття рішень

## 1.28 Процес створення дашбордів

### Етапи розробки:
1. **Визначення ключових метрик** - що важливо показати?
2. **Вибір типів візуалізації** - які графіки найкраще відображають дані?
3. **Створення інтерактивних елементів** - фільтри, селектори, кнопки
4. **Тестування зручності** - чи зрозуміло користувачеві?
5. **Розгортання та підтримка** - забезпечення актуальності даних

### Технічна реалізація:
- **Python**: Dash, Streamlit, Flask
- **BI-інструменти**: Power BI, Tableau, Looker
- **Веб-технології**: D3.js, React + Charts

## 1.29 Поради щодо ефективної візуалізації

### Принципи хорошої візуалізації:
1. **Простота** - не перевантажуйте графік
2. **Релевантність** - показуйте тільки важливі дані
3. **Читабельність** - підписи, легенди, кольори
4. **Консистентність** - єдиний стиль у всьому дашборді

### Поширені помилки:
- Забагато інформації на одному графіку
- Неправильний вибір типу діаграми
- Відсутність контексту або пояснень
- Ігнорування потреб кінцевих користувачів

## Висновок

Візуалізація даних та створення дашбордів - це завершальний етап аналітичного проекту, який перетворює технічні результати в зрозумілі бізнес-інсайти. Успішна візуалізація не тільки показує дані, а й розповідає історію, яка допомагає приймати обґрунтовані рішення.

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


# Розділ лекції: Аналіз текстових даних YouTube

## 2.1 Вступ до проекту аналізу даних YouTube

YouTube є другим за відвідуваністю сайтом у світі після Google. Щодня на платформі переглядається **1 мільярд годин** контенту, що генерує величезні обсяги даних для аналізу.

### Масштаб даних YouTube:
- Понад 2 мільярди активних користувачів щомісяця
- 500 годин відео завантажуються щохвилини
- Мільярди коментарів, лайків та дизлайків щодня

## 2.2 Типи аналізу даних YouTube

### 1. Аналіз настроїв (Sentiment Analysis)

**Мета**: визначити позитивні, негативні або нейтральні настрої в коментарях користувачів.

**Застосування**:
- Оцінка реакції аудиторії на відео
- Розуміння сприйняття бренду або продукту
- Виявлення потенційних проблем або трендів

**Приклад результату**:
```
Позитивні коментарі: 65%
Нейтральні коментарі: 25%  
Негативні коментарі: 10%
```

### 2. Аналіз використання емодзі

**Мета**: дослідження популярності та контексту використання емодзі в коментарях.

**Цікаві інсайти**:
- Які емодзі найчастіше використовуються
- Зв'язок між емодзі та тематикою відео
- Культурні відмінності у використанні емодзі
- Вплив емодзі на engagement

**Візуалізація**: хмара емодзі або топ-список найпопулярніших символів.

### 3. Аналіз співвідношення дизлайків та переглядів

**Мета**: вивчення кореляції між кількістю переглядів та дизлайків.

**Ключові питання**:
- Як зростання переглядів впливає на кількість дизлайків?
- Чи є оптимальне співвідношення для успішного відео?
- Які фактори впливають на це співвідношення?

**Візуалізація**: регресійний графік, що показує тренд зростання дизлайків відносно переглядів.

### 4. Аналіз трендових відео

**Мета**: виявлення закономірностей у популярному контенті.

**Дослідження включає**:
- Аналіз заголовків трендових відео
- Ключові слова та фрази
- Оптимальна довжина заголовків
- Часові патерни популярності

## 2.3 Методологія аналізу текстових даних

### Крок 1: Збір даних
- YouTube API для отримання метаданих
- Парсинг коментарів та описів
- Збір інформації про engagement метрики

### Крок 2: Попередня обробка тексту
- Очищення від HTML тегів та спеціальних символів
- Нормалізація тексту (lowercase, видалення зайвих пробілів)
- Токенізація та лематизація
- Видалення стоп-слів

### Крок 3: Аналіз та візуалізація
- Застосування NLP алгоритмів
- Створення візуалізацій
- Інтерпретація результатів

## 2.4 Інструменти для аналізу

### Python бібліотеки:
- **pandas** - обробка даних
- **numpy** - численні обчислення  
- **matplotlib/seaborn** - візуалізація
- **nltk/spacy** - обробка природної мови
- **textblob** - аналіз настроїв
- **wordcloud** - хмари слів

### API та інструменти:
- YouTube Data API v3
- Google Cloud Natural Language API
- Jupyter Notebooks для інтерактивного аналізу

## 2.5 Практичні застосування результатів

### Для контент-мейкерів:
- Оптимізація заголовків та описів відео
- Розуміння preferences аудиторії
- Покращення engagement метрик

### Для маркетологів:
- Аналіз ефективності рекламних кампаній
- Моніторинг репутації бренду
- Виявлення впливових трендів

### Для дослідників:
- Вивчення соціальних трендів
- Аналіз громадської думки
- Дослідження цифрової поведінки

## 2.6 Етичні міркування

### Важливі принципи:
- **Конфіденційність**: анонімізація персональних даних
- **Згода**: дотримання умов використання платформи
- **Об'єктивність**: уникнення упередженості в інтерпретації
- **Прозорість**: відкритість щодо методології аналізу

## Висновок

Аналіз текстових даних YouTube надає потужні можливості для розуміння цифрової поведінки мільйонів користувачів. Поєднання технік обробки природної мови з візуалізацією даних дозволяє отримувати цінні інсайти для бізнесу, досліджень та створення контенту. Ключем до успіху є правильна методологія збору даних, якісна попередня обробка та етичний підхід до аналізу.

---------------------------------------------------------------------------------------------------------

# Лекція: ETL Pipeline для аналізу YouTube даних

## Вступ

У попередній сесії ми розглянули постановку задачі та ознайомилися з даними. Тепер поглиблено вивчимо ETL pipeline (Extract, Transform, Load) - процес витягування, трансформації та завантаження даних.

## Життєвий цикл проєкту з аналізу даних

### Етапи проєкту:
1. **Розуміння задачі** - аналіз вимог та даних
2. **ETL Pipeline**:
   - **Extract (Витягування)** - збір сирих даних
   - **Transform (Трансформація)** - очищення та підготовка
   - **Load (Завантаження)** - збереження готових даних
3. **EDA** (Exploratory Data Analysis) - дослідницький аналіз
4. **Візуалізація** - створення графіків та діаграм
5. **Дашборд** - фінальна презентація результатів

## Необхідні Python бібліотеки

### Основні модулі для роботи з даними:

```python
import pandas as pd          # Робота з даними та їх маніпуляція
import numpy as np          # Чисельні обчислення та масиви
import seaborn as sns       # Швидка візуалізація даних
import matplotlib.pyplot as plt  # Базова візуалізація
import plotly              # Інтерактивні графіки
```

### Призначення бібліотек:

- **Pandas** - "бог" роботи з даними: читання, модифікація, маніпуляція
- **NumPy** - чисельні операції: середні значення, стандартне відхилення, персентилі, одновимірні масиви
- **Matplotlib** - базовий пакет для візуалізації
- **Seaborn** - швидкі та зручні графіки
- **Plotly** - інтерактивні та динамічні візуалізації

## Практична робота: Завантаження даних

### 1. Створення Jupyter Notebook

```python
# Створіть файл з назвою: YouTube_data_analysis.ipynb
```

### 2. Імпорт необхідних бібліотек

```python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
```

### 3. Читання CSV файлу

```python
# Базовий синтаксис читання CSV
comments = pd.read_csv('шлях_до_файлу/us_comments.csv')
```

### 4. Вирішення проблем з файловими шляхами

**Проблема**: Windows використовує зворотні слеші `\`, а Mac/Linux - прямі `/`

**Рішення**: Використовуйте raw string з префіксом `r`:

```python
comments = pd.read_csv(r'C:\Users\Desktop\us_comments.csv')
```

### 5. Обробка пошкоджених рядків

**Проблема**: Parser error через некоректні рядки

**Рішення**: Додайте параметр для пропуску проблемних рядків:

```python
comments = pd.read_csv(r'шлях_до_файлу/us_comments.csv', 
                      on_bad_lines='skip')
```

## Початковий аналіз даних

### Перегляд структури даних

```python
# Перші 5 рядків
comments.head(5)

# Перевірка типу даних
type(comments)  # pandas.core.frame.DataFrame
```

### Структури даних у Pandas:
- **Series** - одновимірна структура (аналог масиву)
- **DataFrame** - двовимірна структура (таблиця з рядками та стовпчиками)

## Очищення даних (Data Cleaning)

### 1. Виявлення пропущених значень

```python
# Перевірка на наявність пропусків
missing_check = comments.isnull()  # або comments.isna()

# Підрахунок пропусків по кожному стовпчику
missing_count = comments.isnull().sum()
```

**Результат**: Boolean DataFrame, де:
- `False` - значення присутнє
- `True` - значення відсутнє

### 2. Видалення пропущених значень

```python
# Видалення всіх рядків з пропусками
comments.dropna(inplace=True)

# Перевірка результату
comments.isnull().sum()  # Має показати 0 для всіх стовпчиків
```

**Параметр `inplace=True`** означає, що зміни застосовуються до самого DataFrame без створення копії.

## Результати першого етапу

Після виконання цих кроків ми маємо:
- ✅ Завантажені сирі дані з CSV файлу
- ✅ Очищені дані від пропущених значень
- ✅ Готовий DataFrame для подальшого аналізу

## Наступні кроки

У наступних сесіях ми будемо:
1. Проводити детальний дослідницький аналіз (EDA)
2. Створювати візуалізації
3. Будувати дашборд для презентації результатів

## Корисні поради

- Завжди перевіряйте дані після кожного етапу очищення
- Використовуйте `inplace=True` обережно - це незворотна операція
- Зберігайте резервні копії оригінальних даних
- Документуйте всі кроки очищення для відтворюваності результатів

---

*Примітка: Якщо у вас виникають питання, будь ласка, задавайте їх у розділі Q&A.*

-------------------------------------------------------------------------------------------------------------

# 16 Лекція: Аналіз хмари слів (WordCloud) для sentiment analysis

## Вступ

У попередній сесії ми виконали аналіз тональності (sentiment analysis) за допомогою векторного підходу та збережили значення полярності у відповідну колонку. Тепер перейдемо до аналізу хмари слів як частини дослідницького аналізу даних (EDA).

## Що таке хмара слів (WordCloud)?

**WordCloud** - це візуальне представлення текстових даних, де розмір слова відображає його важливість або частоту появи в тексті.

### Основні характеристики:
- Більші слова = вища частота/важливість
- Візуальний спосіб виявлення ключових тем
- Швидкий огляд основного змісту тексту

## Розуміння полярності

Полярність лежить у діапазоні від -1 до +1:

```
-1.0 ←→ -0.8 ←→ -0.5 ←→ 0 ←→ 0.5 ←→ 0.8 ←→ 1.0
│     Сильно    │   Нейтрально   │    Сильно     │
│   негативно   │                │  позитивно    │
```

### Категорії для аналізу:
- **Високо позитивні:** 0.8 ≤ полярність ≤ 1.0
- **Високо негативні:** -1.0 ≤ полярність ≤ -0.8

## Практична реалізація

### 1. Фільтрування позитивних коментарів

```python
# Створення фільтру для позитивних коментарів
filter_positive = (comments['polarity'] > 0.8) & (comments['polarity'] <= 1.0)

# Застосування фільтру
comments_positive = comments[filter_positive]

# Перевірка розміру
print(comments_positive.shape)
```

### 2. Фільтрування негативних коментарів

```python
# Створення фільтру для негативних коментарів  
filter_negative = (comments['polarity'] >= -1.0) & (comments['polarity'] <= -0.8)

# Застосування фільтру
comments_negative = comments[filter_negative]

# Перевірка розміру
print(comments_negative.shape)
```

### 3. Встановлення бібліотеки WordCloud

```bash
pip install wordcloud
```

### 4. Імпорт необхідних модулів

```python
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
```

## Підготовка даних для WordCloud

### Перетворення Series у String

WordCloud потребує рядкових даних, але pandas надає Series:

```python
# Перевірка типу даних
print(type(comments_positive['comment_text']))  # pandas.core.series.Series

# Конвертація Series у string за допомогою join
total_positive_comments = ' '.join(comments_positive['comment_text'])

print(type(total_positive_comments))  # <class 'str'>
```

## Обробка стоп-слів

**Стоп-слова** - це часто вживані слова без значущого змісту (and, the, is, are, to, etc.)

### Приклад стоп-слів у тексті:
```
"So and the are to I and the is..."
```

### Видалення стоп-слів:
```python
# Створення набору унікальних стоп-слів
stop_words = set(STOPWORDS)
print(stop_words)  # {'a', 'an', 'and', 'are', 'as', 'at', ...}
```

## Створення WordCloud для позитивних коментарів

```python
# Ініціалізація та генерація WordCloud
wordcloud_positive = WordCloud(stopwords=stop_words).generate(total_positive_comments)

# Відображення результату
plt.imshow(wordcloud_positive, interpolation='bilinear')
plt.axis('off')  # Вимкнення осей
plt.show()
```

### Аналіз позитивної хмари слів

Ключові слова у позитивних коментарях:
- **"love", "best", "beautiful", "amazing", "great", "awesome"** - показують сильне захоплення та схвалення
- **"please", "want", "hope"** - демонструють високий рівень залученості користувачів та бажання більшого контенту

**Висновок:** YouTube користувачі виражають сильну прихильність та ентузіазм щодо контенту.

## Створення WordCloud для негативних коментарів

```python
# Підготовка даних для негативних коментарів
total_negative_comments = ' '.join(comments_negative['comment_text'])

# Створення WordCloud
wordcloud_negative = WordCloud(stopwords=stop_words).generate(total_negative_comments)

# Відображення
plt.imshow(wordcloud_negative, interpolation='bilinear')
plt.axis('off')
plt.show()
```

### Аналіз негативної хмари слів

Ключові слова у негативних коментарях:
- **"kill", "hate", "bad", "stupid"** - агресивна та образлива лексика

**Висновок:** Частина YouTube аудиторії використовує агресивну мову, що може свідчити про гнів або незадоволення.

## Технічні деталі

### Структура коду:
1. **Фільтрування даних** за значенням полярності
2. **Конвертація** Series у string
3. **Видалення стоп-слів** для очищення
4. **Генерація WordCloud** з відповідними параметрами
5. **Візуалізація** результатів

### Параметри WordCloud:
- `stopwords` - набір слів для видалення
- `generate()` - метод створення хмари з тексту
- `imshow()` - відображення як зображення

## Практичні висновки

WordCloud аналіз дозволяє:
- Швидко виявити домінуючі теми в коментарях
- Зрозуміти емоційний тон аудиторії
- Ідентифікувати ключові слова для різних категорій тональності
- Візуально представити результати sentiment analysis

## Рекомендації

1. **Завжди використовуйте стоп-слова** для очищення даних
2. **Аналізуйте різні категорії** полярності окремо
3. **Поєднуйте** з іншими методами аналізу тексту
4. **Документуйте висновки** на основі візуальних результатів

Цей підхід забезпечує комплексне розуміння тональності та тематики коментарів у ваших даних.

-------------------------------------------------------------------------------------------------------------------

# 17 Лекція: Аналіз емодзі в коментарях YouTube

## Вступ

У попередній сесії ми виконали аналіз хмари слів та зробили висновки на основі візуалізації. Тепер перейдемо до аналізу емодзі як частини дослідницького аналізу даних (EDA).

## Мета аналізу емодзі

Створити топ-10 найчастіше використовуваних емодзі в коментарях YouTube та проаналізувати емоційний стан аудиторії на основі візуалізації їх частоти.

**Очікуваний результат:** Стовпчикова діаграма з емодзі та їх кількістю використання.

## Встановлення та імпорт бібліотек

### 1. Встановлення пакету emoji

```bash
pip install emoji==2.14.1
```

### 2. Імпорт необхідних модулів

```python
import emoji
import plotly.graph_objects as go
from plotly.offline import iplot
from collections import Counter

# Перевірка версії
print(emoji.__version__)  # 2.14.1
```

## Розуміння структури даних

### Тестування функції emoji_list()

```python
# Приклад тексту з емодзі
sample_text = "Це відео просто чудове! 😍 Дуже круто! 👍 Дякую! ❤️"

# Отримання інформації про емодзі
emoji_info = emoji.emoji_list(sample_text)
print(emoji_info)
```

**Результат:**
```python
[
    {'match': '😍', 'start': 25, 'end': 27},
    {'match': '👍', 'start': 39, 'end': 41},
    {'match': '❤️', 'start': 50, 'end': 52}
]
```

### Структура відповіді:
- **`match`** - сам емодзі
- **`start`** - початкова позиція в тексті
- **`end`** - кінцева позиція в тексті

## Витягування емодзі з одного коментаря

```python
# Витягування тільки емодзі з результату
emojis_from_text = [item['match'] for item in emoji_info]
print(emojis_from_text)  # ['😍', '👍', '❤️']
```

## Обробка всіх коментарів

### Повний алгоритм витягування емодзі

```python
# Ініціалізація списку для всіх емодзі
all_emojis = []

# Обробка всіх коментарів
for comment in comments['comment_text']:
    # Отримання інформації про емодзі в коментарі
    emoji_info = emoji.emoji_list(comment)
    
    # Витягування емодзі з інформації
    emojis_found = [item['match'] for item in emoji_info]
    
    # Додавання до загального списку
    all_emojis.extend(emojis_found)

# Перевірка результату
print(f"Перші 10 емодзі: {all_emojis[:10]}")
print(f"Загальна кількість емодзі: {len(all_emojis)}")
```

## Підрахунок частоти емодзі

### Використання Counter для статистики

```python
# Підрахунок частоти кожного емодзі
emoji_counter = Counter(all_emojis)

# Отримання топ-10 найчастіших емодзі
top_10_emojis = emoji_counter.most_common(10)
print(top_10_emojis)
```

**Приклад результату:**
```python
[('😂', 1523), ('❤️', 1204), ('👍', 987), ('😍', 756), ('🔥', 612), 
 ('👏', 489), ('😊', 432), ('💯', 387), ('🙏', 341), ('😘', 298)]
```

## Підготовка даних для візуалізації

### Розділення на окремі списки

```python
# Розділення на емодзі та їх кількість
emojis = [emoji_count[0] for emoji_count in top_10_emojis]
counts = [emoji_count[1] for emoji_count in top_10_emojis]

print("Емодзі:", emojis)
print("Кількість:", counts)
```

## Створення візуалізації з Plotly

### 1. Встановлення Plotly

```bash
pip install plotly
```

### 2. Створення інтерактивного стовпчикового графіку

```python
# Створення стовпчикової діаграми
fig = go.Bar(
    x=emojis,
    y=counts,
    name="Частота емодзі"
)

# Відображення графіку
iplot([fig])
```

### Особливості Plotly візуалізації:
- **Інтерактивність** - при наведенні показує точні значення
- **Якісне відображення емодзі** на осі X
- **Hover інформація** з деталями для кожного емодзі

## Аналіз та інтерпретація результатів

### Топ-5 емодзі та їх значення:

1. **😂 (Сміх до сліз)** - Користувачі знаходять контент кумедним
2. **❤️ (Червоне серце)** - Вираження любові та прихильності
3. **👍 (Вказівний палець вгору)** - Схвалення та підтримка
4. **😍 (Закохані очі)** - Захоплення контентом
5. **🔥 (Вогонь)** - Контент "палає", дуже крутий

### Загальні висновки:

**Позитивні емоції домінують:**
- Більшість емодзі виражають позитивні емоції
- Користувачі демонструють задоволення контентом
- Високий рівень залученості аудиторії

**Емоційний профіль аудиторії:**
- Веселість та гумор (😂)
- Любов та прихильність (❤️, 😍)
- Підтримка та схвалення (👍, 👏)
- Ентузіазм (🔥, 💯)

## Технічні деталі реалізації

### Структура коду:
1. **Витягування емодзі** з кожного коментаря
2. **Агрегація** всіх емодзі в один список
3. **Підрахунок частоти** за допомогою Counter
4. **Візуалізація** топ-10 результатів

### Ключові функції:
- `emoji.emoji_list()` - виявлення емодзі в тексті
- `Counter()` - підрахунок частоти
- `most_common()` - отримання найчастіших елементів
- `go.Bar()` - створення стовпчикової діаграми

## Практичні рекомендації

### Для поглибленого аналізу:
1. **Сегментація за часом** - аналіз емодзі по періодах
2. **Кореляція з тональністю** - зв'язок емодзі та sentiment
3. **Аналіз контексту** - емодзі в залежності від тематики відео
4. **Порівняння каналів** - різниці в використанні емодзі

### Обмеження методу:
- Емодзі можуть мати різні інтерпретації в контексті
- Сарказм може спотворювати значення
- Культурні відмінності у використанні емодзі

Аналіз емодзі надає цінну інформацію про емоційний стан аудиторії та може доповнювати традиційні методи аналізу тональності тексту.


---------------------------------------------------------------------------------------------------------------------------------------------


# 18 Лекція: Збір та об'єднання даних YouTube з різних країн

## Вступ

У попередній сесії ми виконали аналіз емодзі для виявлення найчастіше використовуваних емодзі користувачами YouTube. Тепер перейдемо до наступного етапу - збору та об'єднання даних з різних країн для створення комплексного набору даних.

## Життєвий цикл роботи з даними

Повертаємося до основного циклу аналітичного проєкту:

```
Raw Data → Data Cleaning → Featurized Data → Data Analysis
   ↑             ↑              ↑              ↑
Збір даних   Очищення   Підготовка ознак   Аналіз
```

**Поточний етап:** Збір даних (Data Collection)

## Структура наших даних

У нас є дані YouTube коментарів з різних країн:
- Індія
- США  
- Китай
- Росія
- Інші країни

**Формати файлів:**
- **JSON** - дані у форматі ключ-значення
- **CSV** - дані, розділені комами

## Робота з операційною системою

### Імпорт OS модуля

```python
import os
```

**Призначення OS пакету:** Взаємодія з операційною системою для:
- Створення файлів
- Модифікації файлів
- Доступу до файлів за певним шляхом
- Виконання системних операцій

### Отримання списку файлів

```python
# Вказуємо шлях до папки з даними
file_path = r"C:\path\to\your\data\folder"

# Отримуємо список всіх файлів у директорії
files = os.listdir(file_path)
print(files)
```

## Фільтрування CSV файлів

### Відбір тільки CSV файлів

```python
# Фільтруємо тільки CSV файли за допомогою list comprehension
files_csv = [file for file in files if '.csv' in file]
print(files_csv)
```

**Результат:** Список файлів типу:
- `india_comments.csv`
- `usa_comments.csv`  
- `china_comments.csv`
- `russia_comments.csv`

## Обробка попереджень

### Налаштування фільтру попереджень

```python
import warnings
from warnings import filterwarnings

# Ігнорувати всі попередження під час збору даних
filterwarnings('ignore')
```

**Мета:** Уникнення зупинки процесу через несуттєві попередження.

## Алгоритм об'єднання даних

### Псевдокод:

1. Створити порожній DataFrame
2. Для кожного CSV файлу:
   - Прочитати файл у DataFrame
   - Об'єднати з основним DataFrame
3. Оновити основний DataFrame

### Реалізація на Python

```python
import pandas as pd

# 1. Створення порожнього DataFrame
full_df = pd.DataFrame()

# 2. Шлях до файлів
file_path = r"C:\path\to\your\data\folder"

# 3. Обробка кожного CSV файлу
for file in files_csv:
    # Читання поточного файлу
    current_df = pd.read_csv(
        file_path + '\\' + file,
        encoding='ISO-8859-1',
        on_bad_lines='skip'
    )
    
    # Об'єднання з основним DataFrame
    full_df = pd.concat([current_df, full_df], ignore_index=True)
```

## Важливі параметри

### Кодування (Encoding)

```python
encoding='ISO-8859-1'
```

**Чому важливо:**
- Дані з різних країн можуть мати різні кодування
- Регіональні дані вимагають специфічного кодування
- ISO-8859-1 - один з найкращих варіантів для міжнародних даних

### Обробка помилкових рядків

```python
on_bad_lines='skip'
```

**Функція:** Пропускає рядки з некоректними даними замість зупинки процесу.

### Ігнорування індексів

```python
ignore_index=True
```

**Призначення:** Створює новий послідовний індекс для об'єднаного DataFrame.

## Повний код збору даних

```python
import os
import pandas as pd
import warnings
from warnings import filterwarnings

# Налаштування
filterwarnings('ignore')
file_path = r"C:\path\to\your\data\folder"

# Отримання списку CSV файлів
files = os.listdir(file_path)
files_csv = [file for file in files if '.csv' in file]

# Ініціалізація порожнього DataFrame
full_df = pd.DataFrame()

# Об'єднання всіх файлів
for file in files_csv:
    current_df = pd.read_csv(
        file_path + '\\' + file,
        encoding='ISO-8859-1',
        on_bad_lines='skip'
    )
    full_df = pd.concat([current_df, full_df], ignore_index=True)

# Перевірка результату
print(f"Розмір об'єднаного набору даних: {full_df.shape}")
print(f"Кількість рядків: {full_df.shape[0]}")
print(f"Кількість стовпчиків: {full_df.shape[1]}")
```

## Перевірка результатів

### Аналіз розмірності

```python
# Отримання інформації про розміри
print(full_df.shape)
# Результат: (total_rows, total_columns)
```

### Попередній перегляд

```python
# Перегляд перших кількох рядків
print(full_df.head())

# Інформація про стовпчики
print(full_df.info())
```

## Технічні особливості

### List Comprehension

```python
files_csv = [file for file in files if '.csv' in file]
```

**Переваги:**
- Компактний код
- Швидше виконання
- Читабельність

### Pandas concat()

```python
pd.concat([current_df, full_df], ignore_index=True)
```

**Функції:**
- Вертикальне об'єднання DataFrame
- Збереження структури даних
- Можливість ігнорування індексів

## Підготовка до наступного етапу

Після збору даних у нас є:
- Великий об'єднаний DataFrame
- Дані з різних країн в одному місці
- Готовність до етапу очищення

## Наступні кроки

У наступній сесії будемо вивчати:
- Виявлення дублікатів у даних
- Видалення дублікованих записів
- Базові методи очищення даних
- Перевірка якості об'єднаних даних

## Практичні рекомендації

### Перевірка перед об'єднанням:
- Переконайтесь у правильності шляхів до файлів
- Перевірте кодування кожного файлу
- Зробіть резервну копію оригінальних даних

### Оптимізація продуктивності:
- Використовуйте правильне кодування з першого разу
- Налаштуйте обробку помилок
- Моніторьте використання пам'яті при роботі з великими файлами

Збір даних є фундаментальним етапом будь-якого аналітичного проєкту, і правильне об'єднання файлів забезпечує якісну основу для подальшого аналізу.

------------------------------------------------------------------------------------------

#  19 Лекція: Очищення даних та експорт у різні формати

## Вступ

У попередній сесії ми зібрали дані YouTube з різних країн в один об'єднаний DataFrame. Тепер перейдемо до очищення цих даних від дублікатів та експорту у різні формати файлів.

## Етапи обробки даних

Відповідно до життєвого циклу аналітичного проєкту:

```
Raw Data → Data Cleaning → Featurized Data → Data Analysis
           ↑ (поточний етап)
```

## Виявлення та видалення дублікатів

### 1. Виявлення дублікованих записів

```python
# Перевірка наявності дублікатів
duplicates_check = full_df.duplicated()
print(duplicates_check)
```

**Результат:** Boolean Series, де:
- `False` - запис унікальний
- `True` - запис є дублікатом

### 2. Підрахунок кількості дублікатів

```python
# Фільтрування тільки дублікованих записів
duplicate_records = full_df[full_df.duplicated()]
print(f"Кількість дублікатів: {duplicate_records.shape[0]}")
```

### 3. Параметр `keep` у функції duplicated()

```python
# Різні стратегії обробки дублікатів
duplicates_first = full_df.duplicated(keep='first')    # За замовчуванням
duplicates_last = full_df.duplicated(keep='last')      # Залишити останній
duplicates_false = full_df.duplicated(keep=False)      # Позначити всі як дублікати
```

#### Пояснення параметру `keep`:

**Приклад:** Є 3 однакові записи (1-й, 2-й, 3-й)

- `keep='first'` - позначить 2-й та 3-й як дублікати
- `keep='last'` - позначить 1-й та 2-й як дублікати  
- `keep=False` - позначить всі три як дублікати

### 4. Видалення дублікатів

```python
# Видалення дублікованих записів
full_df = full_df.drop_duplicates()

# Перевірка результату
print(f"Розмір після очищення: {full_df.shape}")
```

**Приклад результату:**
- До очищення: 375,942 записи
- Після очищення: ~345,000 записів (видалено ~30,000 дублікатів)

## Експорт даних у різні формати

### 1. Експорт у CSV формат

```python
# Експорт повного набору даних
full_df.to_csv(
    r"C:\path\to\export\youtube_full_data.csv",
    index=False  # Без індексів pandas
)

# Експорт вибірки (перші 1000 записів)
full_df[:1000].to_csv(
    r"C:\path\to\export\youtube_sample.csv",
    index=False
)
```

**Параметри:**
- `index=False` - не включати індекси DataFrame у файл
- `r""` - raw string для правильної обробки шляхів Windows

### 2. Експорт у JSON формат

```python
# Експорт у JSON (індекси автоматично не включаються)
full_df[:1000].to_json(
    r"C:\path\to\export\youtube_sample.json"
)
```

**Особливості JSON:**
- Формат ключ-значення
- Не потребує параметра `index=False`
- Зазвичай більший розмір файлу (~2MB для 1000 записів)

### 3. Експорт у базу даних SQLite

#### Крок 1: Створення підключення

```python
from sqlalchemy import create_engine

# Створення движка для SQLite бази даних
engine = create_engine(r'sqlite:///C:\path\to\database\youtube_sample.sqlite')
```

**Синтаксис URL підключення:**
- **SQLite:** `sqlite:///path/to/database.sqlite`
- **PostgreSQL:** `postgresql://user:password@host:port/database`
- **MySQL:** `mysql://user:password@host:port/database`

#### Крок 2: Експорт даних у таблицю

```python
# Експорт у базу даних
full_df[:1000].to_sql(
    name='users',           # Назва таблиці
    con=engine,            # Підключення до БД
    if_exists='append'     # Стратегія при існуванні таблиці
)
```

**Параметр `if_exists`:**
- `'append'` - додати до існуючої таблиці
- `'replace'` - замінити таблицю
- `'fail'` - помилка, якщо таблиця існує

## Оптимізація для слабких систем

### Робота з вибірками даних

```python
# Замість повного набору даних використовуйте вибірки
sample_size = 1000

# Перші N записів
first_sample = full_df.head(sample_size)

# Останні N записів  
last_sample = full_df.tail(sample_size)

# Випадкова вибірка
random_sample = full_df.sample(n=sample_size, random_state=42)
```

**Переваги вибірок:**
- Швидша обробка
- Менше споживання пам'яті
- Зручно для тестування коду

## Перевірка результатів експорту

### 1. Перевірка CSV файлу

```python
# Читання експортованого CSV
exported_csv = pd.read_csv(r"C:\path\to\export\youtube_sample.csv")
print(f"Розмір експортованого CSV: {exported_csv.shape}")
```

### 2. Перевірка JSON файлу

```python
# Читання експортованого JSON
exported_json = pd.read_json(r"C:\path\to\export\youtube_sample.json")
print(f"Розмір експортованого JSON: {exported_json.shape}")
```

### 3. Перевірка SQLite бази

```python
# Читання з бази даних
data_from_db = pd.read_sql('SELECT * FROM users', con=engine)
print(f"Записів у базі даних: {data_from_db.shape[0]}")
```

## Практичний приклад повного процесу

```python
import pandas as pd
import os
from sqlalchemy import create_engine

# 1. Завантаження даних (з попередньої сесії)
# full_df = ... (ваш об'єднаний DataFrame)

# 2. Очищення від дублікатів
print(f"До очищення: {full_df.shape}")
full_df = full_df.drop_duplicates()
print(f"Після очищення: {full_df.shape}")

# 3. Створення вибірки для експорту
sample_df = full_df.head(1000)

# 4. Експорт у різні формати
export_path = r"C:\exported_data"

# CSV
sample_df.to_csv(f"{export_path}\\youtube_data.csv", index=False)

# JSON
sample_df.to_json(f"{export_path}\\youtube_data.json")

# SQLite
engine = create_engine(f'sqlite:///{export_path}\\youtube_data.sqlite')
sample_df.to_sql('youtube_comments', con=engine, if_exists='replace')

print("Експорт завершено успішно!")
```

## Важливі зауваження

### Безпека даних:
- **Завжди створюйте резервні копії** оригінальних даних
- **Не виконуйте операції очищення** без попередньої перевірки
- **Тестуйте на малих вибірках** перед обробкою повних наборів

### Продуктивність:
- Використовуйте `raw string (r"")` для шляхів файлів
- **Не запускайте експорт у БД кілька разів** з параметром 'append'
- Моніторьте використання пам'яті при роботі з великими даними

### Формати файлів:
- **CSV** - універсальний, швидкий, компактний
- **JSON** - зручний для веб-додатків, більший розмір
- **SQLite** - для реляційних запитів, складних фільтрів

## Наступні кроки

У наступній сесії ми вивчимо:
- Додаткові методи очищення даних
- Обробку пропущених значень
- Перевірку типів даних
- Валідацію якості даних

Правильне очищення та експорт даних є критично важливими етапами, які забезпечують якість подальшого аналізу.

----------------------------------------------------------------------------------------------------------------


#  20 Лекція: Аналіз категорій YouTube за кількістю вподобань

## Вступ

У попередній сесії ми вивчили експорт даних у різні формати (CSV, JSON, SQLite). Тепер перейдемо до дослідницького аналізу даних (EDA) для визначення, яка категорія має максимальну кількість вподобань.

## Постановка проблеми

**Завдання:** Визначити, яка категорія YouTube відео має найбільшу кількість лайків.

**Виклик:** У наших даних є тільки `category_id`, але немає `category_name`.

### Перевірка наявних даних

```python
# Перегляд структури даних
full_df.head(5)
```

**Проблема:** Маємо `category_id` (числові коди), але потребуємо зрозумілі назви категорій.

### Знаходження унікальних категорій

```python
# Отримання списку унікальних ID категорій
unique_categories = full_df['category_id'].unique()
print(unique_categories)
```

## Створення словника категорій з JSON файлів

### 1. Читання JSON файлу з інформацією про категорії

```python
# Читання JSON файлу з категоріями (приклад для США)
json_df = pd.read_json(r"C:\path\to\data\us_category_id.json")
print(json_df)
```

### 2. Аналіз структури JSON

```python
# Дослідження структури даних
print(json_df['items'])
```

**Структура:** Кожен рядок у колонці `items` містить словник з інформацією про категорію.

### 3. Доступ до конкретної категорії

```python
# Доступ до першої категорії
first_category = json_df['items'][0]
print(first_category)

# Результат: вкладений словник з інформацією про категорію
```

**Структура даних:**
```python
{
  "snippet": {
    "id": "1",
    "title": "Film & Animation"
  }
}
```

### 4. Створення словника відповідності

```python
# Ініціалізація порожнього словника
category_dictionary = {}

# Заповнення словника
for item in json_df['items'].values:
    # Отримання назви категорії
    title = item['snippet']['title']
    
    # Отримання ID категорії (конвертація в int)
    category_id = int(item['id'])
    
    # Додавання пари ключ-значення
    category_dictionary[category_id] = title

print(category_dictionary)
```

**Результат словника:**
```python
{
  1: 'Film & Animation',
  2: 'Autos & Vehicles',
  10: 'Music',
  15: 'Pets & Animals',
  17: 'Sports',
  19: 'Travel & Events',
  20: 'Gaming',
  22: 'People & Blogs',
  23: 'Comedy',
  24: 'Entertainment',
  25: 'News & Politics',
  26: 'Howto & Style',
  27: 'Education',
  28: 'Science & Technology'
}
```

## Додавання назв категорій до DataFrame

### Застосування map() функції

```python
# Створення нової колонки з назвами категорій
full_df['category_name'] = full_df['category_id'].map(category_dictionary)

# Перевірка результату
full_df.head(4)
```

## Розуміння персентилів

### Пояснення концепції

**25-й персентиль (Q1):** Значення, менше якого знаходяться 25% даних
**50-й персентиль (медіана):** Середнє значення в упорядкованому списку
**75-й персентиль (Q3):** Значення, менше якого знаходяться 75% даних

**Приклад:** Якщо 25-й персентиль = 32, то 25% значень менше 32, а 75% більше 32.

## Створення Box Plot для аналізу

### Що показує Box Plot?

```
    │
    ├─── Верхній викид (outlier)
    │
    ┌─┐ ← 75-й персентиль (Q3)
    │ │
    ├─┤ ← Медіана (50-й персентиль)
    │ │
    └─┘ ← 25-й персентиль (Q1)
    │
    ├─── Нижній викид (outlier)
    │
```

### Базовий Box Plot

```python
import seaborn as sns
import matplotlib.pyplot as plt

# Створення box plot
sns.boxplot(
    x='category_name', 
    y='likes', 
    data=full_df
)
plt.show()
```

### Покращення візуалізації

#### Проблема: Накладання міток на осі X

```python
# Збільшення розміру графіку
plt.figure(figsize=(12, 8))

# Створення box plot
sns.boxplot(
    x='category_name', 
    y='likes', 
    data=full_df
)

# Поворот міток на осі X
plt.xticks(rotation='vertical')

# Відображення графіку
plt.show()
```

## Інтерпретація результатів

### Аналіз Box Plot

**Високоефективні категорії:**
- **Music** - багато викидів у верхній частині
- **Comedy** - стабільно високі показники
- **Entertainment** - широкий діапазон значень

### Ключові спостереження:

1. **Викиди (Outliers):** Точки поза основним діапазоном показують відео з екстремально високою кількістю лайків

2. **Медіанні значення:** Показують типову продуктивність категорії

3. **Розподіл:** Висота "коробки" показує варіативність у межах категорії

## Повний код аналізу

```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# 1. Читання JSON з категоріями
json_df = pd.read_json(r"path\to\us_category_id.json")

# 2. Створення словника категорій
category_dictionary = {}
for item in json_df['items'].values:
    title = item['snippet']['title']
    category_id = int(item['id'])
    category_dictionary[category_id] = title

# 3. Додавання назв категорій
full_df['category_name'] = full_df['category_id'].map(category_dictionary)

# 4. Створення покращеного box plot
plt.figure(figsize=(12, 8))
sns.boxplot(x='category_name', y='likes', data=full_df)
plt.xticks(rotation='vertical')
plt.title('Розподіл лайків по категоріях YouTube')
plt.xlabel('Категорія')
plt.ylabel('Кількість лайків')
plt.tight_layout()
plt.show()
```

## Альтернативні метрики для аналізу

### Замість тільки максимуму, розглядайте:

1. **25-й персентиль** - нижня межа ефективності
2. **Медіана** - типова продуктивність
3. **75-й персентиль** - гарна продуктивність
4. **Максимум** - найкращі результати
5. **Мінімум** - найгірші результати

### Переваги Box Plot:
- Показує весь розподіл даних
- Виявляє викиди
- Порівнює кілька категорій одночасно
- Стійкий до екстремальних значень

## Бізнес-інсайти

### Висновки для клієнтів:

1. **Музичний контент** демонструє найбільший потенціал для вірусності
2. **Комедія та розваги** мають стабільно високу залученість
3. **Наявність викидів** у певних категоріях вказує на можливості для створення хітових відео
4. **Різниця в медіанах** допомагає встановити реалістичні очікування для різних типів контенту

Цей аналіз надає глибше розуміння продуктивності категорій порівняно з простим пошуком максимального значення.

--------------------------------------------------------------------------------------------------------------------

# 21 Лекція: Аналіз залученості YouTube аудиторії

## Вступ

У попередній сесії ми проаналізували, яка категорія має максимальну кількість вподобань, використовуючи box plot. Тепер перейдемо до аналізу залученості аудиторії через створення нових метрик ефективності.

## Визначення залученості аудиторії

**Мета:** Визначити, наскільки активно аудиторія взаємодіє з контентом.

### Ключові показники залученості:
1. **Like Rate** - частка людей, які поставили лайк
2. **Dislike Rate** - частка людей, які поставили дизлайк
3. **Comment Count Rate** - частка людей, які залишили коментар

## Створення метрик залученості

### Формула розрахунку показників

```python
# Розрахунок коефіцієнта лайків (у відсотках)
like_rate = (likes / views) * 100

# Розрахунок коефіцієнта дизлайків (у відсотках)  
dislike_rate = (dislikes / views) * 100

# Розрахунок коефіцієнта коментарів (у відсотках)
comment_rate = (comment_count / views) * 100
```

### Практична реалізація

```python
# Створення нової колонки "Like Rate"
full_df['like_rate'] = (full_df['likes'] / full_df['views']) * 100

# Створення нової колонки "Dislike Rate"
full_df['dislike_rate'] = (full_df['dislikes'] / full_df['views']) * 100

# Створення нової колонки "Comment Count Rate"
full_df['comment_count_rate'] = (full_df['comment_count'] / full_df['views']) * 100

# Перевірка нових колонок
print(full_df.columns)
```

## Візуальний аналіз залученості по категоріях

### Box Plot для Like Rate

```python
import matplotlib.pyplot as plt
import seaborn as sns

# Налаштування розміру графіку
plt.figure(figsize=(12, 8))

# Створення box plot для like rate
sns.boxplot(x='category_name', y='like_rate', data=full_df)

# Поворот міток на осі X
plt.xticks(rotation='vertical')

# Показ графіку
plt.show()
```

### Інтерпретація результатів Like Rate

**Високозалучені категорії:**
- Gaming
- Comedy  
- Music
- Entertainment
- News & Politics

**Характеристики:**
- Наявність викидів (outliers) вказує на відео з екстремально високою залученістю
- Медіанні значення показують типову продуктивність категорії
- Розподіл демонструє консистентність залученості

## Аналіз кореляції між показниками

### Scatter Plot з лінією регресії

```python
# Регресійний графік: Views vs Likes
sns.regplot(x='views', y='likes', data=full_df)
plt.show()
```

**Інтерпретація прямої лінії:**
- Пряма лінія вказує на лінійну залежність
- Збільшення переглядів призводить до пропорційного збільшення лайків
- Сильна позитивна кореляція

### Кореляційний аналіз

```python
# Вибір колонок для аналізу кореляції
correlation_columns = ['views', 'likes', 'dislikes']
correlation_data = full_df[correlation_columns]

# Розрахунок кореляції
correlation_matrix = correlation_data.corr()
print(correlation_matrix)
```

**Результат кореляційної матриці:**
```
         views    likes  dislikes
views    1.000    0.700     0.650
likes    0.700    1.000     0.580
dislikes 0.650    0.580     1.000
```

### Інтерпретація кореляції

**Коефіцієнт кореляції Views-Likes = 0.7:**
- Якщо перегляди збільшуються на 100 одиниць
- Лайки збільшуються приблизно на 70 одиниць
- Сильна позитивна залежність

## Тепловая карта кореляції

### Створення heatmap

```python
# Створення теплової карти кореляції
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('Кореляційна матриця метрик YouTube')
plt.show()
```

**Параметри heatmap:**
- `annot=True` - показувати числові значення кореляції
- `cmap='coolwarm'` - кольорова схема (синій-червоний)
- `center=0` - центрування кольорової шкали навколо нуля

### Читання теплової карти

**Кольорове кодування:**
- **Червоний** - сильна позитивна кореляція (близько до +1)
- **Синій** - сильна негативна кореляція (близько до -1)
- **Білий** - відсутність кореляції (близько до 0)

## Повний код аналізу залученості

```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Створення метрик залученості
full_df['like_rate'] = (full_df['likes'] / full_df['views']) * 100
full_df['dislike_rate'] = (full_df['dislikes'] / full_df['views']) * 100
full_df['comment_count_rate'] = (full_df['comment_count'] / full_df['views']) * 100

# 2. Box plot для like rate
plt.figure(figsize=(12, 8))
sns.boxplot(x='category_name', y='like_rate', data=full_df)
plt.xticks(rotation='vertical')
plt.title('Розподіл Like Rate по категоріях')
plt.show()

# 3. Регресійний аналіз
plt.figure(figsize=(10, 6))
sns.regplot(x='views', y='likes', data=full_df)
plt.title('Залежність між переглядами та лайками')
plt.show()

# 4. Кореляційний аналіз
correlation_data = full_df[['views', 'likes', 'dislikes']]
correlation_matrix = correlation_data.corr()

# 5. Теплова карта кореляції
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('Кореляційна матриця метрик YouTube')
plt.show()
```

## Бізнес-інсайти та рекомендації

### Висновки про залученість:

1. **Gaming та Comedy** показують найвищі показники like rate
2. **Лінійна залежність** між переглядами та лайками підтверджує важливість охоплення
3. **Кореляція 0.7** між переглядами та лайками вказує на передбачуваність ефективності

### Практичні рекомендації:

**Для контент-креаторів:**
- Фокус на категоріях з високим like rate (Gaming, Comedy)
- Стратегії збільшення переглядів автоматично підвищать лайки
- Моніторинг outliers для виявлення вірусного потенціалу

**Для маркетологів:**
- Використання метрик залученості для оцінки ROI
- Порівняння ефективності різних типів контенту
- Прогнозування результатів на основі кореляційних залежностей

## Обмеження аналізу

### Важливі зауваження:

1. **Кореляція ≠ Причинність** - зв'язок не означає, що одне спричиняє інше
2. **Часові фактори** - давність публікації впливає на накопичення метрик
3. **Алгоритмічні зміни** YouTube можуть впливати на показники
4. **Сезонність контенту** може спотворювати загальні тенденції

Аналіз залученості надає цінні інсайти для розуміння поведінки аудиторії та оптимізації контент-стратегії.

--------------------------------------------------------------------------------------------------------------

# 22 Лекція: Аналіз YouTube каналів з найбільшою кількістю трендових відео

## Вступ

У попередній сесії ми вивчили створення регресійних графіків та теплових карт кореляції, а також метрики для оцінки залученості YouTube аудиторії. Тепер проаналізуємо, які канали мають найбільшу кількість трендових відео.

## Постановка завдання

**Мета:** Визначити топ-5 YouTube каналів з найбільшою кількістю трендових відео та візуалізувати результати у вигляді стовпчикової діаграми.

## Структура даних

### Перегляд наявних даних

```python
# Перегляд структури даних
full_df.head(6)
```

**Ключова колонка:** `channel_title` - назва каналу, для якого потрібно підрахувати кількість відео.

## Метод 1: Використання value_counts()

### Базовий підрахунок частоти

```python
# Підрахунок кількості відео для кожного каналу
channel_frequency = full_df['channel_title'].value_counts()
print(channel_frequency)
```

**Результат:** Series з назвами каналів як індексами та кількістю відео як значеннями.

**Приклад виводу:**
```
WWE                           45
Late Night with Seth Meyers   38
The Tonight Show Starring...  32
Saturday Night Live          29
...
```

## Метод 2: Використання GroupBy

### Альтернативний підхід через групування

```python
# Групування по назвам каналів та підрахунок розміру груп
channel_groupby = full_df.groupby('channel_title').size()
print(channel_groupby)
```

### Сортування результатів

```python
# Сортування у спадаючому порядку
channel_sorted = full_df.groupby('channel_title').size().sort_values(ascending=False)
print(channel_sorted)
```

**Різниця методів:**
- `value_counts()` автоматично сортує за спаданням
- `groupby().size()` потребує додаткового сортування

## Створення DataFrame для візуалізації

### Перетворення Series у DataFrame

```python
# Скидання індексу для створення DataFrame
cdf = full_df.groupby('channel_title').size().sort_values(ascending=False).reset_index()
print(cdf)
```

**Проблема:** Колонка з кількістю має назву "0"

### Перейменування колонок

```python
# Перейменування колонки з кількістю
cdf = cdf.rename(columns={0: 'total_videos'})
print(cdf)
```

**Результат:** Чистий DataFrame з колонками `channel_title` та `total_videos`

## Створення інтерактивної візуалізації

### Імпорт Plotly Express

```python
import plotly.express as px
```

### Створення стовпчикової діаграми

```python
# Візуалізація топ-20 каналів
fig = px.bar(
    data_frame=cdf[:20],          # Перші 20 записів
    x='channel_title',            # Назви каналів на осі X
    y='total_videos',             # Кількість відео на осі Y
    title='Топ-20 YouTube каналів з найбільшою кількістю трендових відео'
)

# Показ графіку
fig.show()
```

### Переваги Plotly візуалізації

**Інтерактивні функції:**
- Hover ефекти з детальною інформацією
- Масштабування (zoom in/out)
- Вибір областей
- Експорт у PNG формат
- Автоматичне масштабування осей
- Скидання масштабу

## Повний код аналізу

```python
import pandas as pd
import plotly.express as px

# 1. Підрахунок кількості відео по каналах
channel_counts = full_df.groupby('channel_title').size().sort_values(ascending=False)

# 2. Створення DataFrame для візуалізації
cdf = channel_counts.reset_index()
cdf = cdf.rename(columns={0: 'total_videos'})

# 3. Показ топ-10 результатів
print("Топ-10 каналів:")
print(cdf.head(10))

# 4. Створення інтерактивної візуалізації
fig = px.bar(
    data_frame=cdf[:20],
    x='channel_title',
    y='total_videos',
    title='Топ-20 YouTube каналів за кількістю трендових відео',
    labels={
        'channel_title': 'Назва каналу',
        'total_videos': 'Кількість трендових відео'
    }
)

# Налаштування графіку
fig.update_xaxes(tickangle=45)  # Поворот підписів на осі X
fig.update_layout(height=600)   # Висота графіку

fig.show()
```

## Аналіз результатів

### Приклади топ-каналів

**Entertainment канали:**
- WWE (спортивні розваги)
- Late Night with Seth Meyers (ток-шоу)
- Saturday Night Live (комедійне шоу)

**Інсайти:**
1. Розважальні канали домінують у трендах
2. Регулярний контент сприяє попаданню в тренди
3. Великі медіа-компанії мають переваги

## Порівняння методів підрахунку

### value_counts() vs groupby()

```python
# Метод 1: value_counts()
method1 = full_df['channel_title'].value_counts()

# Метод 2: groupby()  
method2 = full_df.groupby('channel_title').size().sort_values(ascending=False)

# Перевірка еквівалентності
print("Методи дають однаковий результат:", method1.equals(method2))
```

**Рекомендація:** Використовуйте `value_counts()` для простоти, `groupby()` для складніших агрегацій.

## Додаткові можливості аналізу

### Фільтрування по категоріях

```python
# Аналіз каналів конкретної категорії
music_channels = full_df[full_df['category_name'] == 'Music']
music_top = music_channels['channel_title'].value_counts().head(10)
```

### Аналіз по часовим періодам

```python
# Конвертація дати публікації
full_df['publish_time'] = pd.to_datetime(full_df['publish_time'])
full_df['year'] = full_df['publish_time'].dt.year

# Аналіз по роках
yearly_trends = full_df.groupby(['year', 'channel_title']).size().reset_index()
```

## Технічні особливості

### reset_index() функція

```python
# Без reset_index()
series_data = full_df.groupby('channel_title').size()
print(type(series_data))  # pandas.Series

# З reset_index()
df_data = series_data.reset_index()
print(type(df_data))      # pandas.DataFrame
```

### Налаштування Plotly

```python
# Додаткові параметри для кращої візуалізації
fig = px.bar(
    data_frame=cdf[:15],
    x='channel_title',
    y='total_videos',
    color='total_videos',           # Колір залежно від значення
    color_continuous_scale='viridis'  # Кольорова схема
)

fig.update_xaxes(tickangle=45)
fig.show()
```

## Бізнес-застосування

### Для контент-креаторів:
- Вивчення стратегій успішних каналів
- Виявлення ніш з високим потенціалом трендовості
- Аналіз конкурентів

### Для маркетологів:
- Вибір каналів для співпраці
- Розуміння трендових тем
- Планування рекламних кампаній

### Для YouTube як платформи:
- Оптимізація алгоритмів рекомендацій
- Виявлення популярних типів контенту
- Аналіз екосистеми платформи

Цей аналіз надає цінні інсайти про динаміку YouTube трендів та може слугувати основою для стратегічних рішень у сфері цифрового маркетингу.

------------------------------------------------------------------------------------------------------------------------------------

# 23 Лекція: Аналіз впливу пунктуації на ефективність YouTube відео

## Вступ

У попередній сесії ми визначили канали з найбільшою кількістю трендових відео за допомогою інтерактивних стовпчикових діаграм. Тепер дослідимо, чи впливає кількість пунктуації в заголовках відео на їх популярність.

## Постановка дослідницького питання

**Гіпотеза:** Кількість знаків пунктуації в заголовку може впливати на кількість переглядів, лайків, дизлайків та коментарів.

**Мета:** Встановити кореляцію між кількістю пунктуації та показниками ефективності відео.

## Аналіз структури заголовків

### Перегляд прикладу заголовка

```python
# Доступ до заголовка першого відео
sample_title = full_df['title'][0]
print(sample_title)
```

**Завдання:** Підрахувати кількість знаків пунктуації в кожному заголовку.

## Підрахунок пунктуації за допомогою модуля string

### Імпорт та огляд модуля string

```python
import string

# Перегляд всіх знаків пунктуації
print(string.punctuation)
# Результат: !"#$%&'()*+,-./:;<=>?@[\]^_`{|}~
```

### Ручний підрахунок для одного заголовка

```python
# Підрахунок пунктуації в конкретному тексті
sample_text = full_df['title'][0]

# Використання list comprehension
punctuation_marks = [char for char in sample_text if char in string.punctuation]
print(f"Знаки пунктуації: {punctuation_marks}")
print(f"Загальна кількість: {len(punctuation_marks)}")
```

## Створення функції для автоматизації

### Визначення функції підрахунку пунктуації

```python
def punctuation_count(text):
    """
    Функція для підрахунку кількості знаків пунктуації в тексті
    
    Args:
        text (str): Вхідний текст для аналізу
        
    Returns:
        int: Кількість знаків пунктуації
    """
    return len([char for char in text if char in string.punctuation])

# Тестування функції
test_title = "Amazing Video! Can you believe it? #Trending"
print(f"Кількість пунктуації: {punctuation_count(test_title)}")
```

## Застосування функції до всього набору даних

### Робота з вибіркою для оптимізації

```python
# Створення вибірки для швидшої обробки
sample_size = 10000
sample = full_df.head(sample_size).copy()

# Застосування функції до стовпчика заголовків
sample['count_punctuation'] = sample['title'].apply(punctuation_count)

# Перегляд результатів
print(sample['count_punctuation'].head())
```

**Важливо:** Обробка повного набору даних може зайняти 5-10 хвилин залежно від характеристик системи.

## Візуальний аналіз залежності

### Box Plot для переглядів

```python
import seaborn as sns
import matplotlib.pyplot as plt

# Створення box plot: пунктуація vs перегляди
plt.figure(figsize=(12, 8))
sns.boxplot(x='count_punctuation', y='views', data=sample)
plt.title('Залежність кількості переглядів від кількості пунктуації в заголовку')
plt.xlabel('Кількість знаків пунктуації')
plt.ylabel('Кількість переглядів')
plt.show()
```

### Інтерпретація результатів для переглядів

**Ключові спостереження:**
- При **2-3 знаках пунктуації** спостерігаються найвищі пікові значення переглядів
- Наявність викидів (outliers) вказує на відео з екстремально високими показниками
- Заголовки без пунктуації показують нижчу медіанну ефективність

### Box Plot для лайків

```python
# Аналіз впливу на лайки
plt.figure(figsize=(12, 8))
sns.boxplot(x='count_punctuation', y='likes', data=sample)
plt.title('Залежність кількості лайків від кількості пунктуації в заголовку')
plt.xlabel('Кількість знаків пунктуації')
plt.ylabel('Кількість лайків')
plt.show()
```

### Інтерпретація результатів для лайків

**Виявлені патерни:**
- **2-3 знаки пунктуації** також показують найкращі результати для лайків
- **5 знаків пунктуації** демонструє деякі високі значення
- Підтверджується загальна тенденція оптимальної кількості пунктуації

## Повний код аналізу

```python
import pandas as pd
import numpy as np
import string
import seaborn as sns
import matplotlib.pyplot as plt

# 1. Визначення функції підрахунку пунктуації
def punctuation_count(text):
    return len([char for char in text if char in string.punctuation])

# 2. Створення вибірки
sample = full_df.head(10000).copy()

# 3. Додавання нової колонки
sample['count_punctuation'] = sample['title'].apply(punctuation_count)

# 4. Статистичний огляд
print("Статистика кількості пунктуації:")
print(sample['count_punctuation'].describe())

# 5. Візуалізація для різних метрик
metrics = ['views', 'likes', 'dislikes', 'comment_count']

fig, axes = plt.subplots(2, 2, figsize=(15, 12))
fig.suptitle('Вплив пунктуації на різні метрики YouTube відео', fontsize=16)

for i, metric in enumerate(metrics):
    row = i // 2
    col = i % 2
    
    sns.boxplot(x='count_punctuation', y=metric, data=sample, ax=axes[row, col])
    axes[row, col].set_title(f'Пунктуація vs {metric.title()}')
    axes[row, col].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()
```

## Статистичний аналіз

### Кореляційний аналіз

```python
# Розрахунок кореляції між пунктуацією та метриками
correlation_data = sample[['count_punctuation', 'views', 'likes', 'dislikes', 'comment_count']]
correlation_matrix = correlation_data.corr()

print("Кореляція з кількістю пунктуації:")
print(correlation_matrix['count_punctuation'].sort_values(ascending=False))
```

### Група аналіз за кількістю пунктуації

```python
# Групування по кількості пунктуації та розрахунок середніх значень
punctuation_analysis = sample.groupby('count_punctuation').agg({
    'views': ['mean', 'median', 'max'],
    'likes': ['mean', 'median', 'max'],
    'dislikes': ['mean', 'median', 'max'],
    'comment_count': ['mean', 'median', 'max']
}).round(2)

print(punctuation_analysis)
```

## Бізнес-інсайти та рекомендації

### Ключові висновки

1. **Оптимальна кількість пунктуації:** 2-3 знаки показують найкращі результати
2. **Уникайте крайнощів:** Заголовки без пунктуації або з надмірною кількістю менш ефективні
3. **Універсальність ефекту:** Тенденція проявляється для всіх метрик (перегляди, лайки, коментарі)

### Практичні рекомендації для контент-креаторів

**Структура ефективного заголовка:**
- Використовуйте 2-3 знаки пунктуації
- Оптимальні знаки: знак оклику (!), знак питання (?), двокрапка (:)
- Приклади: "Неймовірний експеримент! Результат вас здивує?"

### Психологічне обґрунтування

**Чому пунктуація впливає на ефективність:**
- **Емоційне забарвлення:** Знаки оклику створюють відчуття захоплення
- **Інтрига:** Знаки питання викликають цікавість
- **Структурування:** Двокрапки та тире організовують інформацію
- **Читабельність:** Помірна кількість покращує сприйняття

## Обмеження дослідження

### Важливі застереження

1. **Кореляція ≠ Причинність:** Пунктуація може бути пов'язана з іншими факторами
2. **Культурні відмінності:** Ефект може варіюватися залежно від аудиторії
3. **Тематична специфіка:** Різні категорії контенту можуть показувати різні результати
4. **Часові зміни:** Тренди в оформленні заголовків змінюються з часом

### Напрямки для подальших досліджень

- Аналіз конкретних типів пунктуації
- Дослідження позиції пунктуації в заголовку
- Взаємодія пунктуації з довжиною заголовка
- Порівняння ефективності по категоріях контенту

Цей аналіз демонструє, як навіть прості лінгвістичні елементи можуть суттєво впливати на ефективність цифрового контенту.

-------------------------------------------------------------------------------------------------------------------------------


