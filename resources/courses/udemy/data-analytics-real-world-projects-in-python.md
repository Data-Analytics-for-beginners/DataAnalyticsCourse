

# Data Analytics Real-World Projects in Python

https://ua.udemy.com/course/data-analytics-projects-python/

# Розділ лекції: Основи аналітики даних

## 1.1 Що таке аналітика даних?

Аналітика даних - це процес застосування аналітичних технік до даних для отримання змістовних висновків. Ці висновки також називають **інсайтами** або **висновками**, які підтримують краще прийняття рішень та допомагають організаціям і компаніям.

### Три ключові компоненти аналітики даних:

1. **Дані** - вихідна інформація
2. **Аналітичні техніки** - методи обробки 
3. **Висновки** - інсайти для прийняття рішень

## 1.2 Що таке дані?

**Дані** - це сира інформація, яку можна збирати з різних джерел залежно від проекту, над яким ви працюєте.

### Приклад збору даних в Amazon:

Якщо ви аналітик даних в Amazon і маєте завдання зібрати транзакційні дані користувачів, ви можете збирати такі атрибути:

- Що купив кожен клієнт
- Скільки витратив покупець
- Які пристрої використовують користувачі Amazon
- Локація покупців

### Джерела даних:
- Корпоративні бази даних
- Excel файли
- CSV файли
- Фреймворки для великих даних

## 1.3 Аналітичні техніки

Після отримання даних необхідно застосувати аналітичні техніки для отримання змістовних висновків. До таких технік належать:

- Різні статистичні методи
- Побудова графіків та діаграм
- Техніки дослідницького аналізу даних
- Очищення даних

## 1.4 Висновки та інсайти

### Приклад висновків з аналізу даних Amazon:

1. **iPhone є найпопулярнішим товаром** на Amazon
2. **Знижки останньої хвилини** допомагають збільшити продажі
3. **Рекомендовані товари**, які часто купують разом

## 1.5 Повсякденний приклад: приготування піци

Для кращого розуміння концепції розглянемо аналогію з приготуванням піци:

### Інгредієнти (сирі дані):
- Борошно
- Сир
- Овочі
- Соуси та топінги

### Процес приготування (аналітичні техніки):
- Приготування тіста
- Випікання піци

### Висновки (інсайти):
Після куштування піци ви можете зробити висновок: "наступного разу додам більше сиру"

## 1.6 Реальний приклад: авіакомпанія

### Сценарій: 
Ви - аналітик даних в авіакомпанії

### Дані:
- Міста відправлення та призначення
- Ціни квитків
- Дати подорожей
- Приблизно 10 мільйонів спостережень

### Завдання для аналізу:
1. **Сезонний попит на рейси** - коли люди подорожують найчастіше?
2. **Оцінка задоволеності пасажирів** - наскільки клієнти задоволені сервісом?
3. **Найекономічніші авіалінії** - які компанії пропонують найкращі ціни?

### Результат:
Перетворення сирих даних про польоти в розумні бізнес-рішення, які допомагають покращити роботу авіакомпанії.

## Висновок

Аналітика даних - це процес перетворення сирих даних у практичні інсайти, що допомагають організаціям приймати кращі рішення та покращувати свою діяльність. Основна суть полягає в тому, щоб використовувати дані для підтримки розумного прийняття рішень у бізнесі.

---------------------------------------------------------------------------

# Розділ лекції: Відмінності між аналітикою даних та наукою про дані

## 1.7 Основні відмінності між Data Analytics та Data Science

### Ключова різниця:
**Наука про дані = Аналітика даних + Прогнозне моделювання**

### Аналітика даних:
1. **Збір даних** - отримання сирої інформації
2. **Очищення даних** - підготовка до аналізу
3. **Аналіз даних** - дослідження закономірностей
4. **Отримання інсайтів** - висновки для прийняття рішень

### Наука про дані:
Все, що є в аналітиці даних **ПЛЮС**:
- **Прогнозне моделювання** - використання алгоритмів для передбачення майбутніх подій
- **Машинне навчання**
- **Глибоке навчання**
- **Обробка природної мови (NLP)**
- **Алгоритми часових рядів**

## 1.8 Повсякденний приклад: туристична компанія

### Сценарій: 
Аналітик в компанії MakeMyTrip або Goibibo збирає дані про ваші подорожі

### Аналітика даних може відповісти на питання:
- **Куди ви їздили** в останній поїздці?
- **Скільки витратили** на останніх 10 подорожах?
- **Що пройшло добре** під час подорожей?
- **Які напрямки були найприємнішими?**

### Наука про дані може передбачити:
- **Який буде найкращий напрямок** для вашого наступного відпустки?
- **Коли найкраще їхати** в цей напрямок?
- **Скільки коштуватиме** ця подорож?

### Різниця в підході:
- **Аналітика даних**: розуміння минулого (що і чому сталося)
- **Наука про дані**: передбачення майбутнього на основі аналізу минулих даних

## 1.9 Реальний приклад: YouTube

### Дані для аналізу:
- Назва відео
- ID відео
- Співвідношення лайків
- Рівень залученості
- Довжина заголовка
- Чи стало відео вірусним

*Уявімо, що у нас є 1 мільйон таких записів*

### Аналітика даних дозволяє визначити:
- **Яке відео має найвище співвідношення лайків?**
- **У якого відео найвищий рівень залученості?**

### Наука про дані може передбачити:
- **Чи стане нове відео, яке я завантажу, вірусним?**
- Використовуючи минулі дані та алгоритми машинного навчання для прогнозування

## 1.10 Підсумок відмінностей

| Аспект | Аналітика даних | Наука про дані |
|--------|----------------|----------------|
| **Фокус** | Аналіз минулих даних | Аналіз + прогнозування |
| **Питання** | Що і чому сталося? | Що станеться в майбутньому? |
| **Методи** | Статистичний аналіз, візуалізація | ML, AI алгоритми, прогнозні моделі |
| **Результат** | Інсайти та висновки | Передбачення та рекомендації |
| **Часова орієнтація** | Минуле та теперішнє | Майбутнє |

## Висновок

Аналітика даних зосереджена на розумінні того, що сталося в минулому та чому це сталося. Наука про дані використовує цю інформацію для створення моделей, які можуть передбачати майбутні події та тенденції, застосовуючи складні алгоритми машинного навчання та штучного інтелекту.

-------------------------------------------------------------------------------------------------------




flowchart TD
    A[Understanding Use-case] --> B[Run ETL Pipeline]
    B --> C[EDA]
    B --> D[Extract Data]
    B --> E[Transform Data into Acceptable Format]
    B --> F[Load Data]
    C --> G[Conclusion]
    C --> H[Data Viz.]
    G --> I[Dashboarding]
    H --> I
    
    %% ETL Process details
    E -.-> J[Remove Duplicate Rows<br/>Remove In-relevant Rows/Cols<br/>Fix Errors<br/>Fix Missing Values & Data Types<br/>Deal with Outliers]
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style G fill:#fff3e0
    style I fill:#fce4ec



--------------------------------------------------------------------------------------------------------------------------------------------------------

# Розділ лекції: Життєвий цикл проекту з аналітики даних

## 1.11 Життєвий цикл проекту з аналітики даних

Для вирішення будь-якого реального проекту з аналітики даних необхідно дотримуватися певного життєвого циклу:

### Основні етапи:
1. **Розуміння use case** (бізнес-розуміння)
2. **Запуск ETL pipeline**
3. **Виконання EDA** (дослідницького аналізу даних)
4. **Формулювання висновків** з EDA
5. **Створення дашбордів** для проекту

## 1.12 Приклад застосування

### Сценарій: Аналітик даних в Amazon

**Дані про продажі:**
- Продукт, купований користувачем
- Ім'я користувача
- Ціна, сплачена користувачем
- Країна покупки

**Приклад датасету:**
```
Продукт    | Користувач | Ціна (₹) | Країна
iPhone     | John       | 80,000   | US
MacBook    | Rahul      | 120,000  | India
Phone      | Saurabh    | 25,000   | India
```

*Уявімо, що у нас є 1 мільйон таких записів*

### Можливі інсайти після аналізу:
- **iPhone та MacBook** є найпопулярнішими товарами на Amazon
- **Люди з Азії та Європи** зазвичай купують преміум продукти
- **Підлітки** рідше купують дорогі товари

## 1.13 Етап 1: Розуміння use case (бізнес-розуміння)

### Ключові учасники проекту:

#### Клієнт
- **Хто це**: власник бізнесу з проблемою
- **Приклад**: власник роздрібного магазину
- **Проблема**: різке падіння продажів минулого місяця
- **Потреба**: вирішити проблему зниження продажів

#### Product Owner (Власник продукту)
- **Роль**: розуміє потреби клієнта
- **Функції**:
  - Перетворює потреби клієнта в цілі та пріоритети
  - Визначає завдання для аналітичної команди
  - Приклад рішень: "Спочатку проаналізуємо дані продажів, потім відгуки клієнтів"
- **Відповідальність**: надання правильного продукту або рішення клієнту

#### Business Analyst (Бізнес-аналітик)
- **Роль**: міст між бізнесом та технічними командами
- **Функції**:
  - Спілкується з клієнтом та власником продукту
  - Перетворює бізнес-потреби в технічні завдання
  - Готує завдання для data команди (аналітики, інженери, data scientist)

## 1.14 Процес збору вимог

### Етапи взаємодії:
1. **Клієнт** → розмова з **Product Owner** + **Business Analyst**
2. **Збір вимог** для проекту
3. **Передача завдань** команді аналітики даних

### Ключові питання аналітика даних:
- **Звідки збирати дані?**
  - Сторонні API?
  - Внутрішні системи компанії?
  - Бази даних компанії?
  - Веб-скрапінг з сайтів?
  - Big Data джерела?
  - Інші джерела?

## 1.15 Основна мета першого етапу

**Головне завдання**: визначити джерела даних для проекту

**Принцип**: "Без даних немає ні data science, ні data analytics"

**Результат**: чітке розуміння:
- Що потрібно проаналізувати
- Звідки взяти дані
- Які технічні завдання необхідно виконати

## Висновок

Перший етап життєвого циклу проекту з аналітики даних - це фундамент успішного проекту. Правильне розуміння бізнес-потреб та ідентифікація джерел даних критично важливі для подальшого успіху всього проекту.

-----------------------------------------------------------------------------------------------------------------------------------------------------

# Розділ лекції: ETL Pipeline в аналітиці даних

## 1.16 Що таке ETL Pipeline?

**ETL** - це другий етап життєвого циклу проекту з аналітики даних, який виконується командою аналітиків та big data інженерів.

### Розшифровка ETL:
- **E** - **Extract** (Витягування) - отримання сирих даних
- **T** - **Transform** (Трансформація) - очищення та підготовка даних
- **L** - **Load** (Завантаження) - збереження підготовлених даних

## 1.17 Етап 1: Extract (Витягування даних)

### Джерела даних:

#### Структуровані формати:
- **CSV** (Comma Separated Values) - дані, розділені комою
- **TSV** (Tab Separated Values) - дані, розділені табуляцією
- **JSON** - колекція пар "ключ-значення"

#### Бази даних:
**SQL бази даних** (табличні):
- MySQL, PostgreSQL, SQL Server
- Дані зберігаються у формі таблиць (рядки та стовпці)
- Доступ через SQL запити

**NoSQL бази даних** (документо-орієнтовані):
- MongoDB, Cassandra
- Дані зберігаються у формі документів

#### Big Data платформи:
- Hive, Hadoop, Spark
- Використовують HiveQL (подібна до SQL)

### Приклади форматів даних:

**CSV формат:**
```
Name,Age,City
John,25,New York
Mary,30,London
```

**TSV формат:**
```
Name    Age    City
John    25     New York
Mary    30     London
```

**JSON формат:**
```json
[
  {"Name": "John", "Age": 25, "City": "New York"},
  {"Name": "Mary", "Age": 30, "City": "London"}
]
```

## 1.18 Розподіл джерел даних

### Внутрішні джерела (60-80% часу):
- Корпоративні бази даних
- Внутрішні системи компанії
- Data warehouses та data lakes

### Зовнішні джерела (20-40% часу):
- API третіх сторін (комерційні, платні)
- Web scraping з веб-сайтів
- Відкриті дані

## 1.19 Етап 2: Transform (Трансформація даних)

**Найважливіший етап**: займає **2/3 часу всього проекту**

### Типові проблеми сирих даних:
- Помилки та опечатки
- Пропущені значення
- Дублікати рядків та стовпців
- Нерелевантні дані
- Неправильні типи даних
- Викиди (outliers)

### Синоніми трансформації:
- Data Cleaning (очищення даних)
- Data Pre-processing (попередня обробка)
- Data Wrangling (обробка даних)
- Data Preparation (підготовка даних)

**Результат**: чисті, підготовлені, featurized дані

## 1.20 Етап 3: Load (Завантаження даних)

Вибір способу зберігання залежить від обсягу даних:

### Великі дані (петабайти, терабайти):
- **Big Data платформи**: Hive, Hadoop, Spark clusters
- **Відповідальний**: Big Data Engineer

### Середні дані (10-100 ГБ):
- **Бази даних**: SQL або NoSQL
- Залежить від специфіки проекту

### Малі дані:
- **Файли**: CSV, TSV, JSON, Excel
- Простіше зберігання та доступ

## 1.21 Схема ETL процесу

```
Сирі дані → Data Cleaning → Чисті дані → Зберігання → Подальший аналіз
```

### Деталізація трансформації:
1. Виправлення помилок
2. Обробка пропущених значень
3. Видалення дублікатів
4. Фільтрація нерелевантних даних
5. Приведення до правильних типів даних
6. Обробка викидів

## 1.22 Роль Big Data Engineer

**Відповідальність**: ефективне зберігання підготовлених даних

**Вибір технології залежить від**:
- Обсягу даних
- Швидкості доступу
- Типу аналізу
- Бюджету проекту

## Висновок

ETL Pipeline - це критично важливий етап, який забезпечує якість даних для подальшого аналізу. Без якісного ETL неможливо отримати достовірні інсайти та зробити правильні бізнес-рішення. Найбільше уваги потребує етап трансформації, оскільки якість очищення даних безпосередньо впливає на результати всього проекту.

-----------------------------------------------------------------------------------------------------------------------

# Розділ лекції: Візуалізація даних та створення дашбордів

## 1.23 Від EDA до візуалізації

Після виконання дослідницького аналізу даних (EDA) ми отримуємо висновки та інсайти з даних. Однак великі обсяги текстової інформації важко сприймати та аналізувати.

### Проблема текстових даних:

**Приклад результатів EDA** (найпопулярніші продукти):
```
iPhone: 50,000 проданих одиниць
OnePlus: 35,000 проданих одиниць
MacBook: 60,000 проданих одиниць
Samsung: 42,000 проданих одиниць
```

Коли у нас мільйон записів, така текстова інформація стає незрозумілою для швидкого аналізу.

## 1.24 Рішення: Візуалізація даних

### Що таке візуалізація даних?
**Візуалізація даних** - це процес створення графіків та діаграм для наочного представлення результатів аналізу.

### Приклад: стовпчикова діаграма

```
Продажі продуктів
│
│     ┌───┐
│ ┌───┤   │
│ │   │   │   ┌───┐
│ │   │   │   │   │   ┌───┐
└─┴───┴───┴───┴───┴───┴───┴─
iPhone OnePlus MacBook Samsung
```

**Результат**: одним поглядом можна побачити, що MacBook є лідером продажів.

## 1.25 Інструменти для візуалізації в Python

### Основні бібліотеки:

#### 1. **Pandas**
- Вбудовані можливості візуалізації
- Швидкі і прості графіки

#### 2. **Matplotlib** 
- Базовий модуль для візуалізації
- Фундамент для інших бібліотек
- Статичні графіки

#### 3. **Seaborn**
- Красивіші статистичні графіки
- Побудований на Matplotlib

#### 4. **Plotly**
- Інтерактивні та динамічні графіки
- Можливість взаємодії з користувачем

## 1.26 Важливість інференцій

### Ключовий принцип:
**Якщо ви не можете зробити висновки з ваших графіків, то ваш аналіз марний.**

### Необхідність комунікації:
- **Ділитеся результатами** з колегами та командою
- **Пояснюйте технічні речі** простою мовою
- **Робіть висновки зрозумілими** для нетехнічної аудиторії

### Критерії успішної візуалізації:
1. Графік легко читається
2. Висновки очевидні з першого погляду
3. Дані підтверджують або спростовують гіпотези
4. Результати можна пояснити бізнесу

## 1.27 Дашборди

### Що таке дашборд?
**Дашборд** - це інтерактивна панель, що об'єднує всі аналітичні результати на одному екрані.

### Приклад: дашборд фондового ринку

**Функціональність:**
- Вибір компанії (Amazon, Microsoft, Apple, Google)
- Автоматичне оновлення всіх графіків
- Різні види аналізу в одному місці:
  - Динаміка цін
  - Обсяги торгівлі  
  - Технічні індикатори
  - Порівняльний аналіз

### Переваги дашбордів:

#### Для аналітиків:
- Централізація всіх результатів
- Швидкий доступ до різних метрик
- Можливість інтерактивного дослідження

#### Для бізнесу:
- Миттєвий доступ до ключових показників
- Не потрібні технічні знання
- Можливість самостійного аналізу

#### Для команди:
- Спільне використання результатів
- Однакове розуміння даних
- Ефективне прийняття рішень

## 1.28 Процес створення дашбордів

### Етапи розробки:
1. **Визначення ключових метрик** - що важливо показати?
2. **Вибір типів візуалізації** - які графіки найкраще відображають дані?
3. **Створення інтерактивних елементів** - фільтри, селектори, кнопки
4. **Тестування зручності** - чи зрозуміло користувачеві?
5. **Розгортання та підтримка** - забезпечення актуальності даних

### Технічна реалізація:
- **Python**: Dash, Streamlit, Flask
- **BI-інструменти**: Power BI, Tableau, Looker
- **Веб-технології**: D3.js, React + Charts

## 1.29 Поради щодо ефективної візуалізації

### Принципи хорошої візуалізації:
1. **Простота** - не перевантажуйте графік
2. **Релевантність** - показуйте тільки важливі дані
3. **Читабельність** - підписи, легенди, кольори
4. **Консистентність** - єдиний стиль у всьому дашборді

### Поширені помилки:
- Забагато інформації на одному графіку
- Неправильний вибір типу діаграми
- Відсутність контексту або пояснень
- Ігнорування потреб кінцевих користувачів

## Висновок

Візуалізація даних та створення дашбордів - це завершальний етап аналітичного проекту, який перетворює технічні результати в зрозумілі бізнес-інсайти. Успішна візуалізація не тільки показує дані, а й розповідає історію, яка допомагає приймати обґрунтовані рішення.

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


# Розділ лекції: Аналіз текстових даних YouTube

## 2.1 Вступ до проекту аналізу даних YouTube

YouTube є другим за відвідуваністю сайтом у світі після Google. Щодня на платформі переглядається **1 мільярд годин** контенту, що генерує величезні обсяги даних для аналізу.

### Масштаб даних YouTube:
- Понад 2 мільярди активних користувачів щомісяця
- 500 годин відео завантажуються щохвилини
- Мільярди коментарів, лайків та дизлайків щодня

## 2.2 Типи аналізу даних YouTube

### 1. Аналіз настроїв (Sentiment Analysis)

**Мета**: визначити позитивні, негативні або нейтральні настрої в коментарях користувачів.

**Застосування**:
- Оцінка реакції аудиторії на відео
- Розуміння сприйняття бренду або продукту
- Виявлення потенційних проблем або трендів

**Приклад результату**:
```
Позитивні коментарі: 65%
Нейтральні коментарі: 25%  
Негативні коментарі: 10%
```

### 2. Аналіз використання емодзі

**Мета**: дослідження популярності та контексту використання емодзі в коментарях.

**Цікаві інсайти**:
- Які емодзі найчастіше використовуються
- Зв'язок між емодзі та тематикою відео
- Культурні відмінності у використанні емодзі
- Вплив емодзі на engagement

**Візуалізація**: хмара емодзі або топ-список найпопулярніших символів.

### 3. Аналіз співвідношення дизлайків та переглядів

**Мета**: вивчення кореляції між кількістю переглядів та дизлайків.

**Ключові питання**:
- Як зростання переглядів впливає на кількість дизлайків?
- Чи є оптимальне співвідношення для успішного відео?
- Які фактори впливають на це співвідношення?

**Візуалізація**: регресійний графік, що показує тренд зростання дизлайків відносно переглядів.

### 4. Аналіз трендових відео

**Мета**: виявлення закономірностей у популярному контенті.

**Дослідження включає**:
- Аналіз заголовків трендових відео
- Ключові слова та фрази
- Оптимальна довжина заголовків
- Часові патерни популярності

## 2.3 Методологія аналізу текстових даних

### Крок 1: Збір даних
- YouTube API для отримання метаданих
- Парсинг коментарів та описів
- Збір інформації про engagement метрики

### Крок 2: Попередня обробка тексту
- Очищення від HTML тегів та спеціальних символів
- Нормалізація тексту (lowercase, видалення зайвих пробілів)
- Токенізація та лематизація
- Видалення стоп-слів

### Крок 3: Аналіз та візуалізація
- Застосування NLP алгоритмів
- Створення візуалізацій
- Інтерпретація результатів

## 2.4 Інструменти для аналізу

### Python бібліотеки:
- **pandas** - обробка даних
- **numpy** - численні обчислення  
- **matplotlib/seaborn** - візуалізація
- **nltk/spacy** - обробка природної мови
- **textblob** - аналіз настроїв
- **wordcloud** - хмари слів

### API та інструменти:
- YouTube Data API v3
- Google Cloud Natural Language API
- Jupyter Notebooks для інтерактивного аналізу

## 2.5 Практичні застосування результатів

### Для контент-мейкерів:
- Оптимізація заголовків та описів відео
- Розуміння preferences аудиторії
- Покращення engagement метрик

### Для маркетологів:
- Аналіз ефективності рекламних кампаній
- Моніторинг репутації бренду
- Виявлення впливових трендів

### Для дослідників:
- Вивчення соціальних трендів
- Аналіз громадської думки
- Дослідження цифрової поведінки

## 2.6 Етичні міркування

### Важливі принципи:
- **Конфіденційність**: анонімізація персональних даних
- **Згода**: дотримання умов використання платформи
- **Об'єктивність**: уникнення упередженості в інтерпретації
- **Прозорість**: відкритість щодо методології аналізу

## Висновок

Аналіз текстових даних YouTube надає потужні можливості для розуміння цифрової поведінки мільйонів користувачів. Поєднання технік обробки природної мови з візуалізацією даних дозволяє отримувати цінні інсайти для бізнесу, досліджень та створення контенту. Ключем до успіху є правильна методологія збору даних, якісна попередня обробка та етичний підхід до аналізу.

---------------------------------------------------------------------------------------------------------

# Лекція: ETL Pipeline для аналізу YouTube даних

## Вступ

У попередній сесії ми розглянули постановку задачі та ознайомилися з даними. Тепер поглиблено вивчимо ETL pipeline (Extract, Transform, Load) - процес витягування, трансформації та завантаження даних.

## Життєвий цикл проєкту з аналізу даних

### Етапи проєкту:
1. **Розуміння задачі** - аналіз вимог та даних
2. **ETL Pipeline**:
   - **Extract (Витягування)** - збір сирих даних
   - **Transform (Трансформація)** - очищення та підготовка
   - **Load (Завантаження)** - збереження готових даних
3. **EDA** (Exploratory Data Analysis) - дослідницький аналіз
4. **Візуалізація** - створення графіків та діаграм
5. **Дашборд** - фінальна презентація результатів

## Необхідні Python бібліотеки

### Основні модулі для роботи з даними:

```python
import pandas as pd          # Робота з даними та їх маніпуляція
import numpy as np          # Чисельні обчислення та масиви
import seaborn as sns       # Швидка візуалізація даних
import matplotlib.pyplot as plt  # Базова візуалізація
import plotly              # Інтерактивні графіки
```

### Призначення бібліотек:

- **Pandas** - "бог" роботи з даними: читання, модифікація, маніпуляція
- **NumPy** - чисельні операції: середні значення, стандартне відхилення, персентилі, одновимірні масиви
- **Matplotlib** - базовий пакет для візуалізації
- **Seaborn** - швидкі та зручні графіки
- **Plotly** - інтерактивні та динамічні візуалізації

## Практична робота: Завантаження даних

### 1. Створення Jupyter Notebook

```python
# Створіть файл з назвою: YouTube_data_analysis.ipynb
```

### 2. Імпорт необхідних бібліотек

```python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
```

### 3. Читання CSV файлу

```python
# Базовий синтаксис читання CSV
comments = pd.read_csv('шлях_до_файлу/us_comments.csv')
```

### 4. Вирішення проблем з файловими шляхами

**Проблема**: Windows використовує зворотні слеші `\`, а Mac/Linux - прямі `/`

**Рішення**: Використовуйте raw string з префіксом `r`:

```python
comments = pd.read_csv(r'C:\Users\Desktop\us_comments.csv')
```

### 5. Обробка пошкоджених рядків

**Проблема**: Parser error через некоректні рядки

**Рішення**: Додайте параметр для пропуску проблемних рядків:

```python
comments = pd.read_csv(r'шлях_до_файлу/us_comments.csv', 
                      on_bad_lines='skip')
```

## Початковий аналіз даних

### Перегляд структури даних

```python
# Перші 5 рядків
comments.head(5)

# Перевірка типу даних
type(comments)  # pandas.core.frame.DataFrame
```

### Структури даних у Pandas:
- **Series** - одновимірна структура (аналог масиву)
- **DataFrame** - двовимірна структура (таблиця з рядками та стовпчиками)

## Очищення даних (Data Cleaning)

### 1. Виявлення пропущених значень

```python
# Перевірка на наявність пропусків
missing_check = comments.isnull()  # або comments.isna()

# Підрахунок пропусків по кожному стовпчику
missing_count = comments.isnull().sum()
```

**Результат**: Boolean DataFrame, де:
- `False` - значення присутнє
- `True` - значення відсутнє

### 2. Видалення пропущених значень

```python
# Видалення всіх рядків з пропусками
comments.dropna(inplace=True)

# Перевірка результату
comments.isnull().sum()  # Має показати 0 для всіх стовпчиків
```

**Параметр `inplace=True`** означає, що зміни застосовуються до самого DataFrame без створення копії.

## Результати першого етапу

Після виконання цих кроків ми маємо:
- ✅ Завантажені сирі дані з CSV файлу
- ✅ Очищені дані від пропущених значень
- ✅ Готовий DataFrame для подальшого аналізу

## Наступні кроки

У наступних сесіях ми будемо:
1. Проводити детальний дослідницький аналіз (EDA)
2. Створювати візуалізації
3. Будувати дашборд для презентації результатів

## Корисні поради

- Завжди перевіряйте дані після кожного етапу очищення
- Використовуйте `inplace=True` обережно - це незворотна операція
- Зберігайте резервні копії оригінальних даних
- Документуйте всі кроки очищення для відтворюваності результатів

---

*Примітка: Якщо у вас виникають питання, будь ласка, задавайте їх у розділі Q&A.*

-------------------------------------------------------------------------------------------------------------

# 16 Лекція: Аналіз хмари слів (WordCloud) для sentiment analysis

## Вступ

У попередній сесії ми виконали аналіз тональності (sentiment analysis) за допомогою векторного підходу та збережили значення полярності у відповідну колонку. Тепер перейдемо до аналізу хмари слів як частини дослідницького аналізу даних (EDA).

## Що таке хмара слів (WordCloud)?

**WordCloud** - це візуальне представлення текстових даних, де розмір слова відображає його важливість або частоту появи в тексті.

### Основні характеристики:
- Більші слова = вища частота/важливість
- Візуальний спосіб виявлення ключових тем
- Швидкий огляд основного змісту тексту

## Розуміння полярності

Полярність лежить у діапазоні від -1 до +1:

```
-1.0 ←→ -0.8 ←→ -0.5 ←→ 0 ←→ 0.5 ←→ 0.8 ←→ 1.0
│     Сильно    │   Нейтрально   │    Сильно     │
│   негативно   │                │  позитивно    │
```

### Категорії для аналізу:
- **Високо позитивні:** 0.8 ≤ полярність ≤ 1.0
- **Високо негативні:** -1.0 ≤ полярність ≤ -0.8

## Практична реалізація

### 1. Фільтрування позитивних коментарів

```python
# Створення фільтру для позитивних коментарів
filter_positive = (comments['polarity'] > 0.8) & (comments['polarity'] <= 1.0)

# Застосування фільтру
comments_positive = comments[filter_positive]

# Перевірка розміру
print(comments_positive.shape)
```

### 2. Фільтрування негативних коментарів

```python
# Створення фільтру для негативних коментарів  
filter_negative = (comments['polarity'] >= -1.0) & (comments['polarity'] <= -0.8)

# Застосування фільтру
comments_negative = comments[filter_negative]

# Перевірка розміру
print(comments_negative.shape)
```

### 3. Встановлення бібліотеки WordCloud

```bash
pip install wordcloud
```

### 4. Імпорт необхідних модулів

```python
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
```

## Підготовка даних для WordCloud

### Перетворення Series у String

WordCloud потребує рядкових даних, але pandas надає Series:

```python
# Перевірка типу даних
print(type(comments_positive['comment_text']))  # pandas.core.series.Series

# Конвертація Series у string за допомогою join
total_positive_comments = ' '.join(comments_positive['comment_text'])

print(type(total_positive_comments))  # <class 'str'>
```

## Обробка стоп-слів

**Стоп-слова** - це часто вживані слова без значущого змісту (and, the, is, are, to, etc.)

### Приклад стоп-слів у тексті:
```
"So and the are to I and the is..."
```

### Видалення стоп-слів:
```python
# Створення набору унікальних стоп-слів
stop_words = set(STOPWORDS)
print(stop_words)  # {'a', 'an', 'and', 'are', 'as', 'at', ...}
```

## Створення WordCloud для позитивних коментарів

```python
# Ініціалізація та генерація WordCloud
wordcloud_positive = WordCloud(stopwords=stop_words).generate(total_positive_comments)

# Відображення результату
plt.imshow(wordcloud_positive, interpolation='bilinear')
plt.axis('off')  # Вимкнення осей
plt.show()
```

### Аналіз позитивної хмари слів

Ключові слова у позитивних коментарях:
- **"love", "best", "beautiful", "amazing", "great", "awesome"** - показують сильне захоплення та схвалення
- **"please", "want", "hope"** - демонструють високий рівень залученості користувачів та бажання більшого контенту

**Висновок:** YouTube користувачі виражають сильну прихильність та ентузіазм щодо контенту.

## Створення WordCloud для негативних коментарів

```python
# Підготовка даних для негативних коментарів
total_negative_comments = ' '.join(comments_negative['comment_text'])

# Створення WordCloud
wordcloud_negative = WordCloud(stopwords=stop_words).generate(total_negative_comments)

# Відображення
plt.imshow(wordcloud_negative, interpolation='bilinear')
plt.axis('off')
plt.show()
```

### Аналіз негативної хмари слів

Ключові слова у негативних коментарях:
- **"kill", "hate", "bad", "stupid"** - агресивна та образлива лексика

**Висновок:** Частина YouTube аудиторії використовує агресивну мову, що може свідчити про гнів або незадоволення.

## Технічні деталі

### Структура коду:
1. **Фільтрування даних** за значенням полярності
2. **Конвертація** Series у string
3. **Видалення стоп-слів** для очищення
4. **Генерація WordCloud** з відповідними параметрами
5. **Візуалізація** результатів

### Параметри WordCloud:
- `stopwords` - набір слів для видалення
- `generate()` - метод створення хмари з тексту
- `imshow()` - відображення як зображення

## Практичні висновки

WordCloud аналіз дозволяє:
- Швидко виявити домінуючі теми в коментарях
- Зрозуміти емоційний тон аудиторії
- Ідентифікувати ключові слова для різних категорій тональності
- Візуально представити результати sentiment analysis

## Рекомендації

1. **Завжди використовуйте стоп-слова** для очищення даних
2. **Аналізуйте різні категорії** полярності окремо
3. **Поєднуйте** з іншими методами аналізу тексту
4. **Документуйте висновки** на основі візуальних результатів

Цей підхід забезпечує комплексне розуміння тональності та тематики коментарів у ваших даних.

-------------------------------------------------------------------------------------------------------------------

# 17 Лекція: Аналіз емодзі в коментарях YouTube

## Вступ

У попередній сесії ми виконали аналіз хмари слів та зробили висновки на основі візуалізації. Тепер перейдемо до аналізу емодзі як частини дослідницького аналізу даних (EDA).

## Мета аналізу емодзі

Створити топ-10 найчастіше використовуваних емодзі в коментарях YouTube та проаналізувати емоційний стан аудиторії на основі візуалізації їх частоти.

**Очікуваний результат:** Стовпчикова діаграма з емодзі та їх кількістю використання.

## Встановлення та імпорт бібліотек

### 1. Встановлення пакету emoji

```bash
pip install emoji==2.14.1
```

### 2. Імпорт необхідних модулів

```python
import emoji
import plotly.graph_objects as go
from plotly.offline import iplot
from collections import Counter

# Перевірка версії
print(emoji.__version__)  # 2.14.1
```

## Розуміння структури даних

### Тестування функції emoji_list()

```python
# Приклад тексту з емодзі
sample_text = "Це відео просто чудове! 😍 Дуже круто! 👍 Дякую! ❤️"

# Отримання інформації про емодзі
emoji_info = emoji.emoji_list(sample_text)
print(emoji_info)
```

**Результат:**
```python
[
    {'match': '😍', 'start': 25, 'end': 27},
    {'match': '👍', 'start': 39, 'end': 41},
    {'match': '❤️', 'start': 50, 'end': 52}
]
```

### Структура відповіді:
- **`match`** - сам емодзі
- **`start`** - початкова позиція в тексті
- **`end`** - кінцева позиція в тексті

## Витягування емодзі з одного коментаря

```python
# Витягування тільки емодзі з результату
emojis_from_text = [item['match'] for item in emoji_info]
print(emojis_from_text)  # ['😍', '👍', '❤️']
```

## Обробка всіх коментарів

### Повний алгоритм витягування емодзі

```python
# Ініціалізація списку для всіх емодзі
all_emojis = []

# Обробка всіх коментарів
for comment in comments['comment_text']:
    # Отримання інформації про емодзі в коментарі
    emoji_info = emoji.emoji_list(comment)
    
    # Витягування емодзі з інформації
    emojis_found = [item['match'] for item in emoji_info]
    
    # Додавання до загального списку
    all_emojis.extend(emojis_found)

# Перевірка результату
print(f"Перші 10 емодзі: {all_emojis[:10]}")
print(f"Загальна кількість емодзі: {len(all_emojis)}")
```

## Підрахунок частоти емодзі

### Використання Counter для статистики

```python
# Підрахунок частоти кожного емодзі
emoji_counter = Counter(all_emojis)

# Отримання топ-10 найчастіших емодзі
top_10_emojis = emoji_counter.most_common(10)
print(top_10_emojis)
```

**Приклад результату:**
```python
[('😂', 1523), ('❤️', 1204), ('👍', 987), ('😍', 756), ('🔥', 612), 
 ('👏', 489), ('😊', 432), ('💯', 387), ('🙏', 341), ('😘', 298)]
```

## Підготовка даних для візуалізації

### Розділення на окремі списки

```python
# Розділення на емодзі та їх кількість
emojis = [emoji_count[0] for emoji_count in top_10_emojis]
counts = [emoji_count[1] for emoji_count in top_10_emojis]

print("Емодзі:", emojis)
print("Кількість:", counts)
```

## Створення візуалізації з Plotly

### 1. Встановлення Plotly

```bash
pip install plotly
```

### 2. Створення інтерактивного стовпчикового графіку

```python
# Створення стовпчикової діаграми
fig = go.Bar(
    x=emojis,
    y=counts,
    name="Частота емодзі"
)

# Відображення графіку
iplot([fig])
```

### Особливості Plotly візуалізації:
- **Інтерактивність** - при наведенні показує точні значення
- **Якісне відображення емодзі** на осі X
- **Hover інформація** з деталями для кожного емодзі

## Аналіз та інтерпретація результатів

### Топ-5 емодзі та їх значення:

1. **😂 (Сміх до сліз)** - Користувачі знаходять контент кумедним
2. **❤️ (Червоне серце)** - Вираження любові та прихильності
3. **👍 (Вказівний палець вгору)** - Схвалення та підтримка
4. **😍 (Закохані очі)** - Захоплення контентом
5. **🔥 (Вогонь)** - Контент "палає", дуже крутий

### Загальні висновки:

**Позитивні емоції домінують:**
- Більшість емодзі виражають позитивні емоції
- Користувачі демонструють задоволення контентом
- Високий рівень залученості аудиторії

**Емоційний профіль аудиторії:**
- Веселість та гумор (😂)
- Любов та прихильність (❤️, 😍)
- Підтримка та схвалення (👍, 👏)
- Ентузіазм (🔥, 💯)

## Технічні деталі реалізації

### Структура коду:
1. **Витягування емодзі** з кожного коментаря
2. **Агрегація** всіх емодзі в один список
3. **Підрахунок частоти** за допомогою Counter
4. **Візуалізація** топ-10 результатів

### Ключові функції:
- `emoji.emoji_list()` - виявлення емодзі в тексті
- `Counter()` - підрахунок частоти
- `most_common()` - отримання найчастіших елементів
- `go.Bar()` - створення стовпчикової діаграми

## Практичні рекомендації

### Для поглибленого аналізу:
1. **Сегментація за часом** - аналіз емодзі по періодах
2. **Кореляція з тональністю** - зв'язок емодзі та sentiment
3. **Аналіз контексту** - емодзі в залежності від тематики відео
4. **Порівняння каналів** - різниці в використанні емодзі

### Обмеження методу:
- Емодзі можуть мати різні інтерпретації в контексті
- Сарказм може спотворювати значення
- Культурні відмінності у використанні емодзі

Аналіз емодзі надає цінну інформацію про емоційний стан аудиторії та може доповнювати традиційні методи аналізу тональності тексту.


---------------------------------------------------------------------------------------------------------------------------------------------


# 18 Лекція: Збір та об'єднання даних YouTube з різних країн

## Вступ

У попередній сесії ми виконали аналіз емодзі для виявлення найчастіше використовуваних емодзі користувачами YouTube. Тепер перейдемо до наступного етапу - збору та об'єднання даних з різних країн для створення комплексного набору даних.

## Життєвий цикл роботи з даними

Повертаємося до основного циклу аналітичного проєкту:

```
Raw Data → Data Cleaning → Featurized Data → Data Analysis
   ↑             ↑              ↑              ↑
Збір даних   Очищення   Підготовка ознак   Аналіз
```

**Поточний етап:** Збір даних (Data Collection)

## Структура наших даних

У нас є дані YouTube коментарів з різних країн:
- Індія
- США  
- Китай
- Росія
- Інші країни

**Формати файлів:**
- **JSON** - дані у форматі ключ-значення
- **CSV** - дані, розділені комами

## Робота з операційною системою

### Імпорт OS модуля

```python
import os
```

**Призначення OS пакету:** Взаємодія з операційною системою для:
- Створення файлів
- Модифікації файлів
- Доступу до файлів за певним шляхом
- Виконання системних операцій

### Отримання списку файлів

```python
# Вказуємо шлях до папки з даними
file_path = r"C:\path\to\your\data\folder"

# Отримуємо список всіх файлів у директорії
files = os.listdir(file_path)
print(files)
```

## Фільтрування CSV файлів

### Відбір тільки CSV файлів

```python
# Фільтруємо тільки CSV файли за допомогою list comprehension
files_csv = [file for file in files if '.csv' in file]
print(files_csv)
```

**Результат:** Список файлів типу:
- `india_comments.csv`
- `usa_comments.csv`  
- `china_comments.csv`
- `russia_comments.csv`

## Обробка попереджень

### Налаштування фільтру попереджень

```python
import warnings
from warnings import filterwarnings

# Ігнорувати всі попередження під час збору даних
filterwarnings('ignore')
```

**Мета:** Уникнення зупинки процесу через несуттєві попередження.

## Алгоритм об'єднання даних

### Псевдокод:

1. Створити порожній DataFrame
2. Для кожного CSV файлу:
   - Прочитати файл у DataFrame
   - Об'єднати з основним DataFrame
3. Оновити основний DataFrame

### Реалізація на Python

```python
import pandas as pd

# 1. Створення порожнього DataFrame
full_df = pd.DataFrame()

# 2. Шлях до файлів
file_path = r"C:\path\to\your\data\folder"

# 3. Обробка кожного CSV файлу
for file in files_csv:
    # Читання поточного файлу
    current_df = pd.read_csv(
        file_path + '\\' + file,
        encoding='ISO-8859-1',
        on_bad_lines='skip'
    )
    
    # Об'єднання з основним DataFrame
    full_df = pd.concat([current_df, full_df], ignore_index=True)
```

## Важливі параметри

### Кодування (Encoding)

```python
encoding='ISO-8859-1'
```

**Чому важливо:**
- Дані з різних країн можуть мати різні кодування
- Регіональні дані вимагають специфічного кодування
- ISO-8859-1 - один з найкращих варіантів для міжнародних даних

### Обробка помилкових рядків

```python
on_bad_lines='skip'
```

**Функція:** Пропускає рядки з некоректними даними замість зупинки процесу.

### Ігнорування індексів

```python
ignore_index=True
```

**Призначення:** Створює новий послідовний індекс для об'єднаного DataFrame.

## Повний код збору даних

```python
import os
import pandas as pd
import warnings
from warnings import filterwarnings

# Налаштування
filterwarnings('ignore')
file_path = r"C:\path\to\your\data\folder"

# Отримання списку CSV файлів
files = os.listdir(file_path)
files_csv = [file for file in files if '.csv' in file]

# Ініціалізація порожнього DataFrame
full_df = pd.DataFrame()

# Об'єднання всіх файлів
for file in files_csv:
    current_df = pd.read_csv(
        file_path + '\\' + file,
        encoding='ISO-8859-1',
        on_bad_lines='skip'
    )
    full_df = pd.concat([current_df, full_df], ignore_index=True)

# Перевірка результату
print(f"Розмір об'єднаного набору даних: {full_df.shape}")
print(f"Кількість рядків: {full_df.shape[0]}")
print(f"Кількість стовпчиків: {full_df.shape[1]}")
```

## Перевірка результатів

### Аналіз розмірності

```python
# Отримання інформації про розміри
print(full_df.shape)
# Результат: (total_rows, total_columns)
```

### Попередній перегляд

```python
# Перегляд перших кількох рядків
print(full_df.head())

# Інформація про стовпчики
print(full_df.info())
```

## Технічні особливості

### List Comprehension

```python
files_csv = [file for file in files if '.csv' in file]
```

**Переваги:**
- Компактний код
- Швидше виконання
- Читабельність

### Pandas concat()

```python
pd.concat([current_df, full_df], ignore_index=True)
```

**Функції:**
- Вертикальне об'єднання DataFrame
- Збереження структури даних
- Можливість ігнорування індексів

## Підготовка до наступного етапу

Після збору даних у нас є:
- Великий об'єднаний DataFrame
- Дані з різних країн в одному місці
- Готовність до етапу очищення

## Наступні кроки

У наступній сесії будемо вивчати:
- Виявлення дублікатів у даних
- Видалення дублікованих записів
- Базові методи очищення даних
- Перевірка якості об'єднаних даних

## Практичні рекомендації

### Перевірка перед об'єднанням:
- Переконайтесь у правильності шляхів до файлів
- Перевірте кодування кожного файлу
- Зробіть резервну копію оригінальних даних

### Оптимізація продуктивності:
- Використовуйте правильне кодування з першого разу
- Налаштуйте обробку помилок
- Моніторьте використання пам'яті при роботі з великими файлами

Збір даних є фундаментальним етапом будь-якого аналітичного проєкту, і правильне об'єднання файлів забезпечує якісну основу для подальшого аналізу.

------------------------------------------------------------------------------------------

#  19 Лекція: Очищення даних та експорт у різні формати

## Вступ

У попередній сесії ми зібрали дані YouTube з різних країн в один об'єднаний DataFrame. Тепер перейдемо до очищення цих даних від дублікатів та експорту у різні формати файлів.

## Етапи обробки даних

Відповідно до життєвого циклу аналітичного проєкту:

```
Raw Data → Data Cleaning → Featurized Data → Data Analysis
           ↑ (поточний етап)
```

## Виявлення та видалення дублікатів

### 1. Виявлення дублікованих записів

```python
# Перевірка наявності дублікатів
duplicates_check = full_df.duplicated()
print(duplicates_check)
```

**Результат:** Boolean Series, де:
- `False` - запис унікальний
- `True` - запис є дублікатом

### 2. Підрахунок кількості дублікатів

```python
# Фільтрування тільки дублікованих записів
duplicate_records = full_df[full_df.duplicated()]
print(f"Кількість дублікатів: {duplicate_records.shape[0]}")
```

### 3. Параметр `keep` у функції duplicated()

```python
# Різні стратегії обробки дублікатів
duplicates_first = full_df.duplicated(keep='first')    # За замовчуванням
duplicates_last = full_df.duplicated(keep='last')      # Залишити останній
duplicates_false = full_df.duplicated(keep=False)      # Позначити всі як дублікати
```

#### Пояснення параметру `keep`:

**Приклад:** Є 3 однакові записи (1-й, 2-й, 3-й)

- `keep='first'` - позначить 2-й та 3-й як дублікати
- `keep='last'` - позначить 1-й та 2-й як дублікати  
- `keep=False` - позначить всі три як дублікати

### 4. Видалення дублікатів

```python
# Видалення дублікованих записів
full_df = full_df.drop_duplicates()

# Перевірка результату
print(f"Розмір після очищення: {full_df.shape}")
```

**Приклад результату:**
- До очищення: 375,942 записи
- Після очищення: ~345,000 записів (видалено ~30,000 дублікатів)

## Експорт даних у різні формати

### 1. Експорт у CSV формат

```python
# Експорт повного набору даних
full_df.to_csv(
    r"C:\path\to\export\youtube_full_data.csv",
    index=False  # Без індексів pandas
)

# Експорт вибірки (перші 1000 записів)
full_df[:1000].to_csv(
    r"C:\path\to\export\youtube_sample.csv",
    index=False
)
```

**Параметри:**
- `index=False` - не включати індекси DataFrame у файл
- `r""` - raw string для правильної обробки шляхів Windows

### 2. Експорт у JSON формат

```python
# Експорт у JSON (індекси автоматично не включаються)
full_df[:1000].to_json(
    r"C:\path\to\export\youtube_sample.json"
)
```

**Особливості JSON:**
- Формат ключ-значення
- Не потребує параметра `index=False`
- Зазвичай більший розмір файлу (~2MB для 1000 записів)

### 3. Експорт у базу даних SQLite

#### Крок 1: Створення підключення

```python
from sqlalchemy import create_engine

# Створення движка для SQLite бази даних
engine = create_engine(r'sqlite:///C:\path\to\database\youtube_sample.sqlite')
```

**Синтаксис URL підключення:**
- **SQLite:** `sqlite:///path/to/database.sqlite`
- **PostgreSQL:** `postgresql://user:password@host:port/database`
- **MySQL:** `mysql://user:password@host:port/database`

#### Крок 2: Експорт даних у таблицю

```python
# Експорт у базу даних
full_df[:1000].to_sql(
    name='users',           # Назва таблиці
    con=engine,            # Підключення до БД
    if_exists='append'     # Стратегія при існуванні таблиці
)
```

**Параметр `if_exists`:**
- `'append'` - додати до існуючої таблиці
- `'replace'` - замінити таблицю
- `'fail'` - помилка, якщо таблиця існує

## Оптимізація для слабких систем

### Робота з вибірками даних

```python
# Замість повного набору даних використовуйте вибірки
sample_size = 1000

# Перші N записів
first_sample = full_df.head(sample_size)

# Останні N записів  
last_sample = full_df.tail(sample_size)

# Випадкова вибірка
random_sample = full_df.sample(n=sample_size, random_state=42)
```

**Переваги вибірок:**
- Швидша обробка
- Менше споживання пам'яті
- Зручно для тестування коду

## Перевірка результатів експорту

### 1. Перевірка CSV файлу

```python
# Читання експортованого CSV
exported_csv = pd.read_csv(r"C:\path\to\export\youtube_sample.csv")
print(f"Розмір експортованого CSV: {exported_csv.shape}")
```

### 2. Перевірка JSON файлу

```python
# Читання експортованого JSON
exported_json = pd.read_json(r"C:\path\to\export\youtube_sample.json")
print(f"Розмір експортованого JSON: {exported_json.shape}")
```

### 3. Перевірка SQLite бази

```python
# Читання з бази даних
data_from_db = pd.read_sql('SELECT * FROM users', con=engine)
print(f"Записів у базі даних: {data_from_db.shape[0]}")
```

## Практичний приклад повного процесу

```python
import pandas as pd
import os
from sqlalchemy import create_engine

# 1. Завантаження даних (з попередньої сесії)
# full_df = ... (ваш об'єднаний DataFrame)

# 2. Очищення від дублікатів
print(f"До очищення: {full_df.shape}")
full_df = full_df.drop_duplicates()
print(f"Після очищення: {full_df.shape}")

# 3. Створення вибірки для експорту
sample_df = full_df.head(1000)

# 4. Експорт у різні формати
export_path = r"C:\exported_data"

# CSV
sample_df.to_csv(f"{export_path}\\youtube_data.csv", index=False)

# JSON
sample_df.to_json(f"{export_path}\\youtube_data.json")

# SQLite
engine = create_engine(f'sqlite:///{export_path}\\youtube_data.sqlite')
sample_df.to_sql('youtube_comments', con=engine, if_exists='replace')

print("Експорт завершено успішно!")
```

## Важливі зауваження

### Безпека даних:
- **Завжди створюйте резервні копії** оригінальних даних
- **Не виконуйте операції очищення** без попередньої перевірки
- **Тестуйте на малих вибірках** перед обробкою повних наборів

### Продуктивність:
- Використовуйте `raw string (r"")` для шляхів файлів
- **Не запускайте експорт у БД кілька разів** з параметром 'append'
- Моніторьте використання пам'яті при роботі з великими даними

### Формати файлів:
- **CSV** - універсальний, швидкий, компактний
- **JSON** - зручний для веб-додатків, більший розмір
- **SQLite** - для реляційних запитів, складних фільтрів

## Наступні кроки

У наступній сесії ми вивчимо:
- Додаткові методи очищення даних
- Обробку пропущених значень
- Перевірку типів даних
- Валідацію якості даних

Правильне очищення та експорт даних є критично важливими етапами, які забезпечують якість подальшого аналізу.

----------------------------------------------------------------------------------------------------------------


