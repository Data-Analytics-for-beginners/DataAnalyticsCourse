





# Що таке real-time аналітика даних?

Реальночасова (real-time) аналітика даних — це підхід до аналізу інформації, який дозволяє обробляти та аналізувати дані миттєво або майже миттєво після їх створення чи збору. Замість накопичення даних для періодичної пакетної обробки, real-time аналітика працює з потоковими даними в міру їх надходження.

Ключові особливості real-time аналітики:

1. **Мінімальна затримка** — аналіз відбувається з затримкою в мілісекунди або секунди
2. **Безперервна обробка** — система постійно приймає та аналізує нові дані
3. **Негайне реагування** — можливість приймати рішення та діяти одразу після отримання інформації
4. **Потокова архітектура** — використання спеціальних технологій для роботи з потоками даних

Real-time аналітика особливо цінна в ситуаціях, де швидкість прийняття рішень критично важлива і може суттєво впливати на результати бізнесу, безпеку чи інші важливі показники.


# Реальні приклади real-time аналітики даних

### Фінансовий сектор
- **Виявлення шахрайства з кредитними картками** — банки аналізують кожну транзакцію в момент її здійснення, порівнюючи з типовими патернами витрат клієнта і блокуючи підозрілі операції
- **Алгоритмічний трейдинг** — автоматизовані системи аналізують тисячі ринкових сигналів за долі секунди для прийняття торгових рішень

### Роздрібна торгівля та e-commerce
- **Персоналізовані рекомендації** — Amazon, Netflix та інші сервіси аналізують поведінку користувача під час сеансу для відображення релевантних товарів
- **Динамічне ціноутворення** — авіакомпанії та готелі змінюють ціни в реальному часі, враховуючи попит, конкуренцію та інші фактори

### Виробництво
- **Предиктивне обслуговування** — заводи General Electric використовують сенсори для моніторингу обладнання в реальному часі та передбачення поломок до їх виникнення
- **Контроль якості** — автоматизовані лінії використовують комп'ютерний зір для миттєвого виявлення дефектів

### Телекомунікації
- **Оптимізація мережі** — оператори мобільного зв'язку аналізують навантаження на мережу для динамічного перерозподілу ресурсів
- **Виявлення аномалій** — моніторинг мережевого трафіку для виявлення кібератак або збоїв

### Транспорт і логістика
- **Навігаційні системи** — Google Maps та інші сервіси аналізують дані про трафік в реальному часі для оптимізації маршрутів
- **Управління автопарком** — логістичні компанії відстежують місцезнаходження та стан транспортних засобів для оптимізації доставки

### Охорона здоров'я
- **Моніторинг пацієнтів** — системи в лікарнях аналізують життєві показники пацієнтів для негайного сповіщення медперсоналу про критичні зміни
- **Епідеміологічний нагляд** — аналіз даних для відстеження спалахів захворювань (використовувалось під час пандемії COVID-19)

### Соціальні мережі
- **Аналіз трендів** — Twitter та інші платформи виявляють популярні теми та хештеги в режимі реального часу
- **Модерація контенту** — автоматичні системи аналізують контент одразу після публікації для виявлення забороненого матеріалу

Ці приклади демонструють, як real-time аналітика змінює різні галузі, даючи можливість миттєво реагувати на зміни та приймати більш обґрунтовані рішення без затримок.


# Технологічний стек аналітики даних реального часу

## Шар збору та транспортування даних
- **Apache Kafka** — високопродуктивна розподілена система обміну повідомленнями
- **Amazon Kinesis** — хмарний сервіс для обробки потокових даних
- **Google Pub/Sub** — глобально масштабована система публікації/підписки
- **Apache Pulsar** — система обміну повідомленнями з багаторівневою архітектурою
- **RabbitMQ** — брокер повідомлень для надійної передачі даних

## Шар обробки потоків
- **Apache Spark Streaming** — розширення Spark для обробки міні-батчами
- **Apache Flink** — справжня потокова обробка з гарантіями "exactly-once"
- **Apache Storm** — розподілена обчислювальна система реального часу
- **Kafka Streams** — бібліотека для створення додатків потокової обробки
- **AWS Lambda** — подієва обробка без серверної інфраструктури
- **Azure Stream Analytics** — керований сервіс аналізу потоків у хмарі

## Системи зберігання та доступу
- **ClickHouse** — колоночна СУБД для аналітичних запитів
- **TimescaleDB** — часова база даних на основі PostgreSQL
- **Redis** — високопродуктивна база даних в пам'яті 
- **Cassandra** — розподілена NoSQL база для роботи з великими обсягами даних
- **Druid** — аналітична база даних реального часу для великих наборів даних
- **InfluxDB** — часова база даних оптимізована для часових рядів

## Відображення та візуалізація
- **Grafana** — платформа для моніторингу та аналітики з підтримкою тривог
- **Kibana** — інтерфейс для візуалізації даних з Elasticsearch
- **Apache Superset** — сучасна платформа для дослідження даних
- **Tableau** — потужний інструмент бізнес-аналітики з динамічними панелями
- **Power BI** — інтерактивна візуалізація даних з підтримкою режиму реального часу
- **Looker** — BI-платформа з підтримкою LookML для моделювання даних

## Комплексні інфраструктурні рішення
- **Confluent Platform** — повна платформа для потокової обробки на базі Kafka
- **Elastic Stack** — зв'язка Elasticsearch, Logstash і Kibana для пошуку та аналізу
- **DataStax Enterprise** — інтегрована платформа даних на базі Cassandra
- **Google Dataflow** — керований сервіс для єдиної обробки пакетних і потокових даних
- **Snowflake Snowpipe** — неперервне завантаження даних у хмарне сховище

## Додаткові технології
- **Apache NiFi** — автоматизація потоків даних між системами
- **Prometheus** — моніторинг та аналітика систем з оповіщенням
- **gRPC** — високопродуктивний фреймворк віддалених викликів процедур
- **Apache Airflow** — платформа для оркестрації робочих потоків

Вибір конкретних компонентів стеку залежить від специфічних потреб бізнесу, очікуваних обсягів даних, вимог до латентності та наявної інфраструктури.



# Confluent Cloud

Confluent Cloud — це повністю керована хмарна платформа для Apache Kafka, розроблена компанією Confluent (заснованою творцями Apache Kafka). Ця платформа дозволяє організаціям розгортати, керувати та масштабувати потокову обробку даних без необхідності самостійного управління інфраструктурою Kafka.

## Основні особливості Confluent Cloud:

1. **Повністю керований сервіс** — забезпечує налаштування, конфігурацію, підтримку та моніторинг інфраструктури Kafka, дозволяючи командам зосередитись на розробці додатків.

2. **Глобальне розгортання** — доступний у різних регіонах основних хмарних провайдерів (AWS, Google Cloud, Microsoft Azure).

3. **Масштабованість** — можливість легко масштабувати від невеликих пілотних проєктів до великих виробничих систем з обробкою петабайтів даних.

4. **Екосистема Kafka** — включає не тільки Apache Kafka, але й додаткові компоненти:
   - Schema Registry для управління схемами даних
   - ksqlDB для потокової обробки SQL-подібними запитами
   - Kafka Connect для інтеграції з різними джерелами та приймачами даних
   - Механізми моніторингу та управління

5. **Рівні обслуговування** — пропонує різні варіанти (базовий, стандартний, виділений) з різними рівнями продуктивності, масштабованості та SLA.

6. **Безпека** — включає шифрування даних, аутентифікацію, авторизацію та інші функції безпеки.

Confluent Cloud особливо корисний для організацій, які хочуть використовувати Kafka для обробки потокових даних, але не бажають витрачати ресурси на налаштування та підтримку інфраструктури самостійно.

## Крок 1: Налаштування в Confluent Cloud

Зареєструйтеся в Confluent Cloud.
![](https://github.com/Data-Analytics-for-beginners/DataAnalyticsCourse/blob/main/IMAGES/lesson10_img0.png)

### Щоб додати середовище:
1. Відкрийте консоль Confluent Cloud і перейдіть на сторінку середовищ за адресою https://confluent.cloud/environments.
![](https://github.com/Data-Analytics-for-beginners/DataAnalyticsCourse/blob/main/IMAGES/lesson10_img3.png)

   
3. Натисніть 'Додати хмарне середовище'.
4. Введіть назву середовища: stocks_environment
![](https://github.com/Data-Analytics-for-beginners/DataAnalyticsCourse/blob/main/IMAGES/lesson10_img4.png)

6. Натисніть 'Створити'.
7. Пропустіть будь-які підказки чи опції для управління потоком.

### Потім створіть кластер у своєму середовищі:
1. Натисніть 'Додати кластер'.
![](https://github.com/Data-Analytics-for-beginners/DataAnalyticsCourse/blob/main/IMAGES/lesson10_img5.png)

3. На сторінці 'Створити кластер' для 'Базового' кластера виберіть 'Почати налаштування'.
![](https://github.com/Data-Analytics-for-beginners/DataAnalyticsCourse/blob/main/IMAGES/lesson10_img6.png)

5. Коли вам буде запропоновано вибрати провайдера та розташування, виберіть us-east-2 від AWS.
![](https://github.com/Data-Analytics-for-beginners/DataAnalyticsCourse/blob/main/IMAGES/lesson10_img7.png)

7. Виберіть 'Продовжити'.
8. Вкажіть назву кластера, перегляньте інформацію про конфігурацію та вартість, а потім виберіть 'Запустити кластер'.
9. Залежно від обраного хмарного провайдера та інших налаштувань, забезпечення кластера може зайняти кілька хвилин, але після цього з'явиться сторінка 'Огляд кластера'.

### Створіть ключ API Confluent Cloud і збережіть його:
**Примітка:** якщо у вас більше одного середовища, дотримуйтесь другого набору інструкцій нижче.

1. У меню 'Адміністрування' натисніть 'Ключі Cloud API' або перейдіть за посиланням https://confluent.cloud/settings/api-keys.
2. Натисніть 'Додати ключ'.

![](https://github.com/Data-Analytics-for-beginners/DataAnalyticsCourse/blob/main/IMAGES/lesson10_img11.png)

![](https://github.com/Data-Analytics-for-beginners/DataAnalyticsCourse/blob/main/IMAGES/lesson10_img12.png)


   
3. Виберіть створення ключа, пов'язаного з вашим обліковим записом.
4. Будуть створені та показані ключ та секрет API.
5. Натисніть 'Копіювати', щоб скопіювати ключ і секрет у безпечне місце.

**Важливо**

Секрет для ключа відображається лише на початку в діалоговому вікні створення ключа API і його неможливо переглянути чи отримати пізніше з веб-інтерфейсу. Зберігайте секрет і відповідний ключ у безпечному місці. Не діліться секретом вашого ключа API.

6. (Необов'язково, але рекомендовано) Введіть опис ключа API, що описує, як ви плануєте його використовувати, щоб відрізнити від інших ключів API.
7. Виберіть прапорець підтвердження, що ви зберегли ключ і секрет.
8. Натисніть 'Зберегти'. Ключ додається до таблиці ключів.





## Створення топіку в Kafka

Першим кроком необхідно створити топік Kafka, куди надходитимуть дані котирувань акцій:

1. Відкрийте меню навігації та натисніть "Топіки" (Topics), потім "Створити топік" (Create topic).
2. У полі "Назва топіку" (Topic name) введіть "SPY" (тікер індексного фонду S&P 500).
3. Змініть поле "Розділи" (Partitions) з 6 на 1.
4. Натисніть "Створити з параметрами за замовчуванням" (Create with defaults).

**Примітка**: Зазвичай розділи використовуються для розпаралелювання обробки, але для нашого простого прикладу достатньо одного розділу.

##  Налаштування Schema Registry

Schema Registry забезпечує сумісність форматів даних між виробниками та споживачами:

1. У середовищі, де ви хочете налаштувати Schema Registry, знайдіть "Облікові дані" (Credentials) на правій бічній панелі та натисніть "Ключі" (Keys).
2. Натисніть "Додати ключ" (Add key), щоб створити новий ключ API для Schema Registry.
3. Збережіть згенеровані API Key та API Secret.
4. Встановіть прапорець "Я зберіг свій API ключ і секрет та готовий продовжити" та натисніть "Продовжити".

##  Визначення JSON-схеми для топіку

Для забезпечення узгодженості даних важливо визначити схему:

1. У меню навігації натисніть "Топіки", потім виберіть створений топік "SPY".
2. Натисніть вкладку "Схема" (Schema).
3. Натисніть "Встановити схему" (Set a schema).
4. Виберіть тип схеми: JSON.
5. Видаліть шаблонну схему та введіть наступну JSON-схему:

```json
{
  "$id": "http://example.com/myURI.schema.json",
  "$schema": "http://json-schema.org/draft-07/schema#",
  "additionalProperties": false,
  "description": "Схема для даних котирувань акцій.",
  "properties": {
    "bid_timestamp": {
      "description": "Часова мітка котирування.",
      "type": "string"
    },
    "price": {
      "description": "Ціна акції.",
      "type": "number"
    },
    "symbol": {
      "description": "Тікер акції.",
      "type": "string"
    }
  },
  "title": "SampleRecord",
  "type": "object"
}
```

6. Натисніть "Створити" (Create).

**Важливо**: Запам'ятайте або запишіть URL-адреси сервера Bootstrap та Schema Registry, які знадобляться вам пізніше. URL Bootstrap сервера можна знайти в "Огляд кластера → Налаштування кластера", а URL Schema Registry — у "Середовища → stocks_environments".

## Створення обчислювального пулу Flink

Apache Flink — це потужний інструмент для обробки даних у реальному часі:

1. У меню навігації натисніть "Середовища" (Environments) та виберіть середовище, де ви хочете використовувати Flink SQL.
2. На сторінці деталей середовища натисніть "Flink".
3. Виберіть "Обчислювальні пули" (Compute pools).
4. Натисніть "Створити обчислювальний пул" (Create compute pool).
5. У випадаючому списку "Регіон" виберіть регіон, у якому розміщені дані для обробки. Натисніть "Продовжити".

   **Важливо**: Цей регіон має бути тим самим регіоном, у якому ви створили кластер (наприклад, AWS us-east-2).

6. У полі "Назва пулу" введіть "stocks-compute-pool".
7. У випадаючому списку "Максимальна кількість CFU" виберіть 10.
8. Натисніть "Продовжити", а на сторінці "Перевірити та створити" натисніть "Завершити".

Після створення пулу він з'явиться на сторінці Flink у стані "Підготовка" (Provisioning). Через кілька хвилин пул має перейти у стан "Працює" (Running).

##  Виконання запитів Flink SQL

Тепер ми можемо створити таблицю Flink та запустити запит для обробки даних у реальному часі:

1. У новому робочому просторі введіть наступний запит SQL:

```sql
CREATE TABLE tumble_interval_SPY
(`symbol` STRING, `window_start` STRING,`window_end` STRING,`price` DOUBLE, PRIMARY KEY (`symbol`) NOT ENFORCED)
DISTRIBUTED BY (symbol) INTO 1 BUCKETS 
WITH ('value.format' = 'json-registry');
```

2. Натисніть "Виконати" (Run).

Цей запит створює таблицю Flink з полями, що вказують початок і кінець часового вікна, а також ціну для кожного вікна.

3. Додайте новий запит, натиснувши символ "+":

```sql
INSERT INTO tumble_interval_SPY
SELECT symbol, DATE_FORMAT(window_start,'yyyy-MM-dd hh:mm:ss.SSS'), DATE_FORMAT(window_end,'yyyy-MM-dd hh:mm:ss.SSS'), AVG(price)
FROM TABLE(
        TUMBLE(TABLE SPY, DESCRIPTOR($rowtime), INTERVAL '5' SECONDS))
GROUP BY
    symbol,
    window_start,
    window_end;
```

4. Натисніть "Виконати" (Run).

Цей запит обчислює середнє значення ціни акцій SPY кожні 5 секунд і вставляє результат у таблицю `tumble_interval_SPY`. Техніка "tumbling window" дозволяє створювати непересічні вікна фіксованого розміру для групування даних потоку за часом.







# Посилання:
1) (real-time)²: Real-time data for real-time analytics with Kafka and ClickHouse, https://www.youtube.com/watch?v=bwK0WmADFEg
2) IoT demo with Grafana, Kafka, and ClickHouse, https://www.youtube.com/watch?v=AmEYNdU_340
3) How to use FlinkSQL with Kafka, Streamlit, and the Alpaca API, https://github.com/confluentinc/demo-scene/tree/master/flink-streamlit
4) How to Use Flink SQL, Streamlit, and Kafka: Part 1, https://www.confluent.io/blog/how-to-use-flinksql-streamlit-kafka-part-1/
5) How to Use Flink SQL, Streamlit, and Kafka: Part 2, https://www.confluent.io/blog/how-use-flinksql-streamlit-kafka-part-2/
6) Apache Kafka® and Apache Flink® Tutorials, https://developer.confluent.io/tutorials
7) Apache Kafka Fundamentals You Should Know, https://www.youtube.com/watch?v=-RDyEFvnTXI
8) DEVOPS ЗА 15 МИНУТ, https://www.youtube.com/watch?v=NAytfPVJR34
9) System Design: Why Is Docker Important?, https://www.youtube.com/watch?v=QEzbZKtLi-g
10) 


